{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MOSIP's documentation! Get the code The source http://github.com/mosip is available on Github. The mailing list for the project is located at google groups: Here lets link to a page and see how it shows up - To get started on contributing to MOSIP, read our Contributor Guide and our Code of Conduct . Pre-Registration APIs Registration Processor APIs ID Authentication APIs ABIS APIs Biometric APIs OTP Manager API","title":"Introduction"},{"location":"#welcome-to-mosips-documentation","text":"","title":"Welcome to MOSIP's documentation!"},{"location":"#get-the-code","text":"The source http://github.com/mosip is available on Github. The mailing list for the project is located at google groups: Here lets link to a page and see how it shows up - To get started on contributing to MOSIP, read our Contributor Guide and our Code of Conduct . Pre-Registration APIs Registration Processor APIs ID Authentication APIs ABIS APIs Biometric APIs OTP Manager API","title":"Get the code"},{"location":"ABIS-APIs/","text":"An ABIS system that integrates with MOSIP should support the following operations. All ABIS operations are via a message queue and asynchronous. Common parameters used for all ABIS operations Name Description Restrictions Default Value Example requestID ID that is associated with each request sent to ABIS ABIS should not use this ID in any other context outside the request 80bd41f8-31b2-46ac-ac9c-3534fc1b220e (UUID) referenceID Id of a single registration record. Registration record is maintained in MOSIP. This ID is the mapping between MOSIP and ABIS None 80bd41f8-31b2-46ac-ac9c-3534fc1b220e (UUID) referenceURL URL to the biometrics data stored in MOSIP. This URL will have read only access None biometricType Type of biometric data sent in the request FMR/FIR/IIR Return Value Response None None Integer failureReason Response None None Integer scaledScore Score of the match against the population None None Integer targetFPIR FPIR score as per the formula None None Integer maxResults maximum number of results returned for IDENTIFY operation 30 30 Integer Standard return codes 0 not used 1 Success 2 Failed Failure reasons Code Reason 1 Internal error - Unknown 2 Aborted 3 Unexpected error 4 Unable to serve the request 5 Missing reference id 7 Unable to fetch biometric details All the below operations send biometric data in CBEFF format. (Please refer to the link for sample cbeff data) INSERT (insert biometric data of an Individual) //Request { \"id\" : \"mosip.abis.insert\", \"ver\" : \"1.0\", \"requestId\" : \"01234567-89AB-CDEF-0123-456789ABCDEF\", \"timestamp\" : \"1539777717\", \"referenceId\" : \"01234567-89AB-CDEF-0123-456789ABCDEF\", \"referenceURL\" : \"https://mosip.io/biometric/45678\" } //Success response { \"id\" : \"mosip.abis.insert\", \"requestId\" : \"01234567-89AB-CDEF-0123-456789ABCDEF\", \"timestamp\" : \"1539777717\", \"returnValue\" : \"1\" } //Failure response { \"id\" : \"mosip.abis.insert\", \"requestId\" : \"01234567-89AB-CDEF-0123-456789ABCDEF\", \"timestamp\" : \"1539777717\", \"returnValue\" : \"2\", \"errorCode\" : \"1\" \"failureReason\" : \"\" } Behavior of Insert ABIS must get biometric data from referenceURL, process it and store it locally within the ABIS reference database referenceId must not be active prior to this operation i.e., it must not have been used before this operation De-duplication must not be performed in this operation MOSIP must provide CBEFF format biometric data to ABIS IDENTIFY //Request { \"id\" : \"mosip.abis.identify\", \"ver\" : \"1.0\", \"requestId\" : \"01234567-89AB-CDEF-0123-456789ABCDEF\", \"timestamp\" : \"1539777717\", \"referenceId\" : \"987654321-89AB-CDEF-0123-456789ABCDEF\", \"referenceUrl\" : \"https://mosip.io/registrationprocessor/v1/bio-dedupe/biometricfile/2cce7b7d-b58a-4466-a006-c79297281789\", \"maxResults\" : 10, \"targetFPIR\" : 30, \"gallery\" : { \"referenceIds\" : [ { \"referenceId\" : \"2cce7b7d-b58a-4466-a006-c79297281123\" }, { \"referenceId\" : \"2cce7b7d-b58a-4466-a006-c79297281456\" }, { \"referenceId\" : \"2cce7b7d-b58a-4466-a006-c79297281678\" }, { \"referenceId\" : \"2cce7b7d-b58a-4466-a006-c79297281098\" }, { \"referenceId\" : \"2cce7b7d-b58a-4466-a006-c79297281321\" } ] } } //Success response { \"id\" : \"mosip.abis.identify\", \"requestId\" : \"01234567-89AB-CDEF-0123-456789ABCDEF\", \"timestamp\" : \"1539777717\", \"returnValue\" : \"1\", \"candidateList\" : { \"count\" : \"\", \"candidates\" : [ { \"referenceId\" : \"\", \"internalScore\": \"\", \"scaledScore\" : \"\", \"analytics\": [ { \"key1\": \"value1\", \"key2\": \"value2\" } ], \"scores\": [ { \"biometricType\": \"FIR\", \"scaledScore\": \"\", \"internalScore\": \"\", \"analytics\": [ { \"key1\": \"value1\", \"key2\": \"value2\" } ] }, { \"biometricType\": \"IIR\", \"scaledScore\": \"\", \"internalScore\": \"\", \"analytics\": [ { \"key1\": \"value1\", \"key2\": \"value2\" } ] } ] } ] } } //Failure response { \"id\" : \"mosip.id.identify\", \"requestId\" : \"01234567-89AB-CDEF-0123-456789ABCDEF\", \"timestamp\" : \"1539777717\", \"returnValue\" : \"2\", \"errorCode\" : \"7\" \"failureReason\" : \"\" } Behavior of IDENTIFY IDENTIFY request MUST provide a 1:n comparison The input set for comparison can be provided by referenceID The collection against which the input set has to be matched is specified by a set of referenceID's. If referenceId is provided, atleast one referenceID must be provided in the input. The provided referenceID's must be present in the reference database. If IDENTIFY is against all the entries in ABIS, then gallery attribute MUST not be specified maxResults specify how many results can be returned. By default this will be 10 IDENTIFY should give all candidates which match targetFIPR or a better score than the targetFIPR This request should not match against referenceID that is not in the reference database If referenceID is not NULL, then, ABIS performs 1:n comparison against all the entries in ABIS using the referenceID If referenceID is NULL and referenceURL is provided, then, ABIS performs 1:n comparison against all the entries in ABIS using the referenceURL If referenceID and reference URL both are NULL, then, ABIS throws an error (error code 5) DELETE //Request { \"id\" : \"mosip.abis.delete\", \"ver\" : \"1.0\", \"requestId\" : \"01234567-89AB-CDEF-0123-456789ABCDEF\", \"timestamp\" : \"1539777717\", \"referenceId\" : \"\" } //Success response { \"id\" : \"mosip.abis.delete\", \"requestId\" : \"01234567-89AB-CDEF-0123-456789ABCDEF\", \"timestamp\" : \"1539777717\", \"returnValue\" : \"\" } //Failure response { \"id\" : \"mosip.abis.delete\", \"requestId\" : \"01234567-89AB-CDEF-0123-456789ABCDEF\", \"timestamp\" : \"1539777717\", \"returnValue\" : \"\", \"errorCode\" : \"1\" \"failureReason\" : \"\" } Behavior of DELETE Removes only the entry referred by the referenceId This operation can be used to remove duplicates found by Identify PING //Request { \"id\" : \"mosip.abis.ping\", \"ver\" : \"1.0\", \"requestId\" : \"01234567-89AB-CDEF-0123-456789ABCDEF\", \"timestamp\" : \"1539777717\" } //Success response { \"id\" : \"mosip.abis.ping\", \"requestId\" : \"01234567-89AB-CDEF-0123-456789ABCDEF\", \"timestamp\" : \"1539777717\", \"returnValue\" : \"\", \"failureReason\" : \"\" } Behavior of PING A PING request should respond with a response on the liveness of the ABIS system Pending Jobs //Request { \"id\" : \"mosip.abis.pendingJobs\", \"ver\" : \"1.0\", \"requestId\" : \"01234567-89AB-CDEF-0123-456789ABCDEF\", \"timestamp\" : \"1539777717\" } //Success response { \"id\" : \"mosip.abis.pendingJobs\", \"requestId\" : \"01234567-89AB-CDEF-0123-456789ABCDEF\", \"timestamp\" : \"1539777717\", \"jobscount\" : \"\", \"returnValue\" : \"\", \"failureReason\" : \"\" } Behavior of Pending Jobs ABIS responds with the count of requests that are still pending Reference Count //Request { \"id\" : \"mosip.abis.referenceCount\", \"ver\" : \"1.0\", \"requestId\" : \"01234567-89AB-CDEF-0123-456789ABCDEF\", \"timestamp\" : \"1539777717\" } //Success response { \"id\" : \"mosip.abis.referenceCount\", \"requestId\" : \"01234567-89AB-CDEF-0123-456789ABCDEF\", \"timestamp\" : \"1539777717\", \"count\" : \"\", \"returnValue\" : \"\", \"failureReason\" : \"\" } Behavior of Reference Count ABIS will send a count of records in the reference database","title":"ABIS APIs"},{"location":"ABIS-APIs/#common-parameters-used-for-all-abis-operations","text":"Name Description Restrictions Default Value Example requestID ID that is associated with each request sent to ABIS ABIS should not use this ID in any other context outside the request 80bd41f8-31b2-46ac-ac9c-3534fc1b220e (UUID) referenceID Id of a single registration record. Registration record is maintained in MOSIP. This ID is the mapping between MOSIP and ABIS None 80bd41f8-31b2-46ac-ac9c-3534fc1b220e (UUID) referenceURL URL to the biometrics data stored in MOSIP. This URL will have read only access None biometricType Type of biometric data sent in the request FMR/FIR/IIR Return Value Response None None Integer failureReason Response None None Integer scaledScore Score of the match against the population None None Integer targetFPIR FPIR score as per the formula None None Integer maxResults maximum number of results returned for IDENTIFY operation 30 30 Integer","title":"Common parameters used for all ABIS operations"},{"location":"ABIS-APIs/#standard-return-codes","text":"0 not used 1 Success 2 Failed","title":"Standard return codes"},{"location":"ABIS-APIs/#failure-reasons","text":"Code Reason 1 Internal error - Unknown 2 Aborted 3 Unexpected error 4 Unable to serve the request 5 Missing reference id 7 Unable to fetch biometric details All the below operations send biometric data in CBEFF format. (Please refer to the link for sample cbeff data)","title":"Failure reasons"},{"location":"ABIS-APIs/#insert-insert-biometric-data-of-an-individual","text":"//Request { \"id\" : \"mosip.abis.insert\", \"ver\" : \"1.0\", \"requestId\" : \"01234567-89AB-CDEF-0123-456789ABCDEF\", \"timestamp\" : \"1539777717\", \"referenceId\" : \"01234567-89AB-CDEF-0123-456789ABCDEF\", \"referenceURL\" : \"https://mosip.io/biometric/45678\" } //Success response { \"id\" : \"mosip.abis.insert\", \"requestId\" : \"01234567-89AB-CDEF-0123-456789ABCDEF\", \"timestamp\" : \"1539777717\", \"returnValue\" : \"1\" } //Failure response { \"id\" : \"mosip.abis.insert\", \"requestId\" : \"01234567-89AB-CDEF-0123-456789ABCDEF\", \"timestamp\" : \"1539777717\", \"returnValue\" : \"2\", \"errorCode\" : \"1\" \"failureReason\" : \"\" }","title":"INSERT (insert biometric data of an Individual)"},{"location":"ABIS-APIs/#behavior-of-insert","text":"ABIS must get biometric data from referenceURL, process it and store it locally within the ABIS reference database referenceId must not be active prior to this operation i.e., it must not have been used before this operation De-duplication must not be performed in this operation MOSIP must provide CBEFF format biometric data to ABIS","title":"Behavior of Insert"},{"location":"ABIS-APIs/#identify","text":"//Request { \"id\" : \"mosip.abis.identify\", \"ver\" : \"1.0\", \"requestId\" : \"01234567-89AB-CDEF-0123-456789ABCDEF\", \"timestamp\" : \"1539777717\", \"referenceId\" : \"987654321-89AB-CDEF-0123-456789ABCDEF\", \"referenceUrl\" : \"https://mosip.io/registrationprocessor/v1/bio-dedupe/biometricfile/2cce7b7d-b58a-4466-a006-c79297281789\", \"maxResults\" : 10, \"targetFPIR\" : 30, \"gallery\" : { \"referenceIds\" : [ { \"referenceId\" : \"2cce7b7d-b58a-4466-a006-c79297281123\" }, { \"referenceId\" : \"2cce7b7d-b58a-4466-a006-c79297281456\" }, { \"referenceId\" : \"2cce7b7d-b58a-4466-a006-c79297281678\" }, { \"referenceId\" : \"2cce7b7d-b58a-4466-a006-c79297281098\" }, { \"referenceId\" : \"2cce7b7d-b58a-4466-a006-c79297281321\" } ] } } //Success response { \"id\" : \"mosip.abis.identify\", \"requestId\" : \"01234567-89AB-CDEF-0123-456789ABCDEF\", \"timestamp\" : \"1539777717\", \"returnValue\" : \"1\", \"candidateList\" : { \"count\" : \"\", \"candidates\" : [ { \"referenceId\" : \"\", \"internalScore\": \"\", \"scaledScore\" : \"\", \"analytics\": [ { \"key1\": \"value1\", \"key2\": \"value2\" } ], \"scores\": [ { \"biometricType\": \"FIR\", \"scaledScore\": \"\", \"internalScore\": \"\", \"analytics\": [ { \"key1\": \"value1\", \"key2\": \"value2\" } ] }, { \"biometricType\": \"IIR\", \"scaledScore\": \"\", \"internalScore\": \"\", \"analytics\": [ { \"key1\": \"value1\", \"key2\": \"value2\" } ] } ] } ] } } //Failure response { \"id\" : \"mosip.id.identify\", \"requestId\" : \"01234567-89AB-CDEF-0123-456789ABCDEF\", \"timestamp\" : \"1539777717\", \"returnValue\" : \"2\", \"errorCode\" : \"7\" \"failureReason\" : \"\" }","title":"IDENTIFY"},{"location":"ABIS-APIs/#behavior-of-identify","text":"IDENTIFY request MUST provide a 1:n comparison The input set for comparison can be provided by referenceID The collection against which the input set has to be matched is specified by a set of referenceID's. If referenceId is provided, atleast one referenceID must be provided in the input. The provided referenceID's must be present in the reference database. If IDENTIFY is against all the entries in ABIS, then gallery attribute MUST not be specified maxResults specify how many results can be returned. By default this will be 10 IDENTIFY should give all candidates which match targetFIPR or a better score than the targetFIPR This request should not match against referenceID that is not in the reference database If referenceID is not NULL, then, ABIS performs 1:n comparison against all the entries in ABIS using the referenceID If referenceID is NULL and referenceURL is provided, then, ABIS performs 1:n comparison against all the entries in ABIS using the referenceURL If referenceID and reference URL both are NULL, then, ABIS throws an error (error code 5)","title":"Behavior of IDENTIFY"},{"location":"ABIS-APIs/#delete","text":"//Request { \"id\" : \"mosip.abis.delete\", \"ver\" : \"1.0\", \"requestId\" : \"01234567-89AB-CDEF-0123-456789ABCDEF\", \"timestamp\" : \"1539777717\", \"referenceId\" : \"\" } //Success response { \"id\" : \"mosip.abis.delete\", \"requestId\" : \"01234567-89AB-CDEF-0123-456789ABCDEF\", \"timestamp\" : \"1539777717\", \"returnValue\" : \"\" } //Failure response { \"id\" : \"mosip.abis.delete\", \"requestId\" : \"01234567-89AB-CDEF-0123-456789ABCDEF\", \"timestamp\" : \"1539777717\", \"returnValue\" : \"\", \"errorCode\" : \"1\" \"failureReason\" : \"\" }","title":"DELETE"},{"location":"ABIS-APIs/#behavior-of-delete","text":"Removes only the entry referred by the referenceId This operation can be used to remove duplicates found by Identify","title":"Behavior of DELETE"},{"location":"ABIS-APIs/#ping","text":"//Request { \"id\" : \"mosip.abis.ping\", \"ver\" : \"1.0\", \"requestId\" : \"01234567-89AB-CDEF-0123-456789ABCDEF\", \"timestamp\" : \"1539777717\" } //Success response { \"id\" : \"mosip.abis.ping\", \"requestId\" : \"01234567-89AB-CDEF-0123-456789ABCDEF\", \"timestamp\" : \"1539777717\", \"returnValue\" : \"\", \"failureReason\" : \"\" }","title":"PING"},{"location":"ABIS-APIs/#behavior-of-ping","text":"A PING request should respond with a response on the liveness of the ABIS system","title":"Behavior of PING"},{"location":"ABIS-APIs/#pending-jobs","text":"//Request { \"id\" : \"mosip.abis.pendingJobs\", \"ver\" : \"1.0\", \"requestId\" : \"01234567-89AB-CDEF-0123-456789ABCDEF\", \"timestamp\" : \"1539777717\" } //Success response { \"id\" : \"mosip.abis.pendingJobs\", \"requestId\" : \"01234567-89AB-CDEF-0123-456789ABCDEF\", \"timestamp\" : \"1539777717\", \"jobscount\" : \"\", \"returnValue\" : \"\", \"failureReason\" : \"\" }","title":"Pending Jobs"},{"location":"ABIS-APIs/#behavior-of-pending-jobs","text":"ABIS responds with the count of requests that are still pending","title":"Behavior of Pending Jobs"},{"location":"ABIS-APIs/#reference-count","text":"//Request { \"id\" : \"mosip.abis.referenceCount\", \"ver\" : \"1.0\", \"requestId\" : \"01234567-89AB-CDEF-0123-456789ABCDEF\", \"timestamp\" : \"1539777717\" } //Success response { \"id\" : \"mosip.abis.referenceCount\", \"requestId\" : \"01234567-89AB-CDEF-0123-456789ABCDEF\", \"timestamp\" : \"1539777717\", \"count\" : \"\", \"returnValue\" : \"\", \"failureReason\" : \"\" }","title":"Reference Count"},{"location":"ABIS-APIs/#behavior-of-reference-count","text":"ABIS will send a count of records in the reference database","title":"Behavior of Reference Count"},{"location":"APIs/","text":"","title":"APIs"},{"location":"Architecture/","text":"MOSIP Architecture The Modular Open Source Identity Platform (MOSIP) helps Governments and other user organizations implement a digital, foundational identity system in a cost effective way. Nations can use MOSIP freely to build their own identity systems. Being modular in its architecture, MOSIP provides flexibility to countries in how they implement and configure their systems, and helps avoid vendor lock-in. MOSIP provides the following basic features - acquire an individual's identity data process the identity data to establish uniqueness generate a Unique Identity Number (UIN) authenticate an individual's identity where required to provide access to services such as healthcare, education, social security etc. The key objectives of the platform are to - provide the basic framework to create a fully functional identity system provide the flexibility for a country to choose the features from the basic framework according to their requirements maintain the privacy, security and confidentiality of an individual's data provide a scalable and accessible solution to cater to a wide range of population (a few thousands to tens of millions) Architectural Principles MOSIP is built on the following architecture principles MOSIP must not use proprietary or commercial license frameworks. Where deemed essential, such components must be encapsulated to enable their replacement if necessary (to avoid vendor lock-in) MOSIP must use open standards to expose it\u2019s functionality (to avoid technology lock-in) Each MOSIP component must be independently scalable (scale out) to meet varying load requirements MOSIP must use commodity computing hardware & software to build the platform Data must be encrypted in-flight and at-rest. All requests must be authenticated and authorized. Privacy of Identity Data is an absolute must in MOSIP MOSIP must follow platform based approach so that all common features are abstracted as reusable components and frameworks into a common layer MOSIP must follow API first approach and expose the business functions as RESTful services MOSIP must follow the following manageability principles \u2013 Auditability & monitor ability of every event in the system, testability of every feature of the platform & easy upgrade ability of the platform MOSIP components must be loosely coupled so that they can be composed to build the identity solution as per the requirements of a country MOSIP must support i18n capability All modules of MOSIP should be resilient such that the solution as a whole is fault tolerant The key sub-systems of MOSIP should be designed for extensibility. For example, if an external system has to be integrated for fingerprint data, it should be easy to do so The key design aspects considered for MOSIP are Ecosystem approach MOSIP on its own will not be able to meet the end-to-end requirements of a country. Devices and ABIS providers are key to process an individual's data and prove uniqueness. MOSIP should be able to integrate with devices and ABIS that conform to the standards to achieve the stated goals. MOSIP should also be able to cater to a diverse set of institutions wanting to authenticate an Individual against the data stored in MOSIP. So, key parameters are * All public/external facing interfaces of MOSIP must be standards-based for interoperability Configurability MOSIP should be flexible for countries to configure the base platform according to their specific requirements. Some of the examples of configurability are Country should be able to choose the features required. For example, it must be possible for a country to turn off Finger Print capture Country should be able to configure the attributes of an ID Object * Country should be able to define the length of the UIN number Extensibility MOSIP should be flexible to extend functionality on top of the basic platform. Some of the examples of extensibility are A country should be able to introduce a new step in processing data Integrate MOSIP with other ID systems and include it as part of the MOSIP data processing flow Modularity All components in MOSIP should be modular and their features exposed via interfaces such that the implementation behind the interface can be changed without affecting other modules. Some examples of modularity are UIN generator algorithm provided by the platform can be replaced by a country with their own implementation The default demographic deduplication algorithm provided by MOSIP can be changed to a different one without impacting the process flow Logical Architecture Microservice based architecture for all platform services Services must be stateless to scale out horizontally Services must be idempotent Services must have well defined interfaces This will help achieve modularity, better maintainability and scalability. Staged Event Driven Architecture (SEDA) for processing Registration data Each processing step must be a stage Stages must be connected with event bus Process flow must be configured outside code This will help achieve extensibility and scale out stages independently. Thick client architecture for Registration client Client must run on a desktop/laptop in offline mode also Client must integrate with devices in a secure manner Data Architecture Open Source and Vendor Neutral To handle vendor neutrality and open source, the following consideration are followed while designing the data model and the database design. No business logic is applied at database level: Database will be used only to store and retrieve data. There is no business logic applied at database level other than Primary / Unique key, Not null and foreign keys. Foreign keys are applied within the same database, if a table is referenced in another database then no FK is applied. No specific database features to be used: Features that are common across databases which are compliant with open source standards are applied. All DDL, DML and DQL statements will follow ANSI standards Metadata approach to handle complex and flexible data structures Only following datatypes are being used Character varying Timestamp Date Integer Number Bytea/blob Boolean Multi-Language MOSIP platform is being built for multiple countries, there is a need to support multiple languages. So as per the requirements, MOSIP will support 3 languages as configured by the country level administrator. Multi language support is needed for the following datasets. Master Data ID data of an individual Transaction comments Labels used in UI Messages and notifications From database side, the data will support UTF-8 Unicode character set to store data entered in multiple languages. There will not be any in-built support to translate data at database level. Any translation or transliteration will be handled at API or UI layer. High Performance To support high performance, following database design features are to be considered Database sharding is applied on uin dataset. By default, base sharding algorithm will be applied in MOSIP system. SI can define the sharding algorithm based on the deployment setup All tables will have a primary key index on the primary key field. This will help in faster retrievals and joins All foreign keys will have indexes defined so that it will help in faster joins No referential integrity is applied on tables across databases Partitioning: Partitioning design to be discussed as PostgreSQL has certain limitation / different way of implementation that requires specific database features to be applied. To be discussed further to finalize the implementation of this feature.","title":"Architecture"},{"location":"Architecture/#mosip-architecture","text":"The Modular Open Source Identity Platform (MOSIP) helps Governments and other user organizations implement a digital, foundational identity system in a cost effective way. Nations can use MOSIP freely to build their own identity systems. Being modular in its architecture, MOSIP provides flexibility to countries in how they implement and configure their systems, and helps avoid vendor lock-in. MOSIP provides the following basic features - acquire an individual's identity data process the identity data to establish uniqueness generate a Unique Identity Number (UIN) authenticate an individual's identity where required to provide access to services such as healthcare, education, social security etc. The key objectives of the platform are to - provide the basic framework to create a fully functional identity system provide the flexibility for a country to choose the features from the basic framework according to their requirements maintain the privacy, security and confidentiality of an individual's data provide a scalable and accessible solution to cater to a wide range of population (a few thousands to tens of millions)","title":"MOSIP Architecture"},{"location":"Architecture/#architectural-principles","text":"MOSIP is built on the following architecture principles MOSIP must not use proprietary or commercial license frameworks. Where deemed essential, such components must be encapsulated to enable their replacement if necessary (to avoid vendor lock-in) MOSIP must use open standards to expose it\u2019s functionality (to avoid technology lock-in) Each MOSIP component must be independently scalable (scale out) to meet varying load requirements MOSIP must use commodity computing hardware & software to build the platform Data must be encrypted in-flight and at-rest. All requests must be authenticated and authorized. Privacy of Identity Data is an absolute must in MOSIP MOSIP must follow platform based approach so that all common features are abstracted as reusable components and frameworks into a common layer MOSIP must follow API first approach and expose the business functions as RESTful services MOSIP must follow the following manageability principles \u2013 Auditability & monitor ability of every event in the system, testability of every feature of the platform & easy upgrade ability of the platform MOSIP components must be loosely coupled so that they can be composed to build the identity solution as per the requirements of a country MOSIP must support i18n capability All modules of MOSIP should be resilient such that the solution as a whole is fault tolerant The key sub-systems of MOSIP should be designed for extensibility. For example, if an external system has to be integrated for fingerprint data, it should be easy to do so The key design aspects considered for MOSIP are","title":"Architectural Principles"},{"location":"Architecture/#ecosystem-approach","text":"MOSIP on its own will not be able to meet the end-to-end requirements of a country. Devices and ABIS providers are key to process an individual's data and prove uniqueness. MOSIP should be able to integrate with devices and ABIS that conform to the standards to achieve the stated goals. MOSIP should also be able to cater to a diverse set of institutions wanting to authenticate an Individual against the data stored in MOSIP. So, key parameters are * All public/external facing interfaces of MOSIP must be standards-based for interoperability","title":"Ecosystem approach"},{"location":"Architecture/#configurability","text":"MOSIP should be flexible for countries to configure the base platform according to their specific requirements. Some of the examples of configurability are Country should be able to choose the features required. For example, it must be possible for a country to turn off Finger Print capture Country should be able to configure the attributes of an ID Object * Country should be able to define the length of the UIN number","title":"Configurability"},{"location":"Architecture/#extensibility","text":"MOSIP should be flexible to extend functionality on top of the basic platform. Some of the examples of extensibility are A country should be able to introduce a new step in processing data Integrate MOSIP with other ID systems and include it as part of the MOSIP data processing flow","title":"Extensibility"},{"location":"Architecture/#modularity","text":"All components in MOSIP should be modular and their features exposed via interfaces such that the implementation behind the interface can be changed without affecting other modules. Some examples of modularity are UIN generator algorithm provided by the platform can be replaced by a country with their own implementation The default demographic deduplication algorithm provided by MOSIP can be changed to a different one without impacting the process flow","title":"Modularity"},{"location":"Architecture/#logical-architecture","text":"Microservice based architecture for all platform services Services must be stateless to scale out horizontally Services must be idempotent Services must have well defined interfaces This will help achieve modularity, better maintainability and scalability. Staged Event Driven Architecture (SEDA) for processing Registration data Each processing step must be a stage Stages must be connected with event bus Process flow must be configured outside code This will help achieve extensibility and scale out stages independently. Thick client architecture for Registration client Client must run on a desktop/laptop in offline mode also Client must integrate with devices in a secure manner","title":"Logical Architecture"},{"location":"Architecture/#data-architecture","text":"","title":"Data Architecture"},{"location":"Architecture/#open-source-and-vendor-neutral","text":"To handle vendor neutrality and open source, the following consideration are followed while designing the data model and the database design. No business logic is applied at database level: Database will be used only to store and retrieve data. There is no business logic applied at database level other than Primary / Unique key, Not null and foreign keys. Foreign keys are applied within the same database, if a table is referenced in another database then no FK is applied. No specific database features to be used: Features that are common across databases which are compliant with open source standards are applied. All DDL, DML and DQL statements will follow ANSI standards Metadata approach to handle complex and flexible data structures Only following datatypes are being used Character varying Timestamp Date Integer Number Bytea/blob Boolean","title":"Open Source and Vendor Neutral"},{"location":"Architecture/#multi-language","text":"MOSIP platform is being built for multiple countries, there is a need to support multiple languages. So as per the requirements, MOSIP will support 3 languages as configured by the country level administrator. Multi language support is needed for the following datasets. Master Data ID data of an individual Transaction comments Labels used in UI Messages and notifications From database side, the data will support UTF-8 Unicode character set to store data entered in multiple languages. There will not be any in-built support to translate data at database level. Any translation or transliteration will be handled at API or UI layer.","title":"Multi-Language"},{"location":"Architecture/#high-performance","text":"To support high performance, following database design features are to be considered Database sharding is applied on uin dataset. By default, base sharding algorithm will be applied in MOSIP system. SI can define the sharding algorithm based on the deployment setup All tables will have a primary key index on the primary key field. This will help in faster retrievals and joins All foreign keys will have indexes defined so that it will help in faster joins No referential integrity is applied on tables across databases Partitioning: Partitioning design to be discussed as PostgreSQL has certain limitation / different way of implementation that requires specific database features to be applied. To be discussed further to finalize the implementation of this feature.","title":"High Performance"},{"location":"Automated-Biometric-Identification-System-(ABIS)-Interface/","text":"Providing unique identity for an individual is one of key features of MOSIP platform. To do this MOSIP will Use multi modal biometric information of an individual Leverage Automated Biometric Identification System (ABIS) to de-duplicate Individual's biometric data Design for integrating with multiple ABIS providers to leverage expertise of different ABIS providers Not use ABIS for authentication (deduplication only) This document will provide the specification an ABIS provider must implement to meet MOSIP's requirements. Overview MOSIP will interact with ABIS only via message queues JSON format will be used for all control messages like INSERT, IDENTIFY etc... to interact with ABIS CBEFF XML format will be used to send biometrics data to ABIS (Please refer to MOSIP Biometric Data Specifications for cbeff data sample) Inside the CBEFF XML individual biometrics data must be in respective ISO format Biometric data sent to ABIS will follow the below standards. CBEFF: ISO/IEC 19785-3 (Please refer to MOSIP Biometric Data Specifications for details) Fingerprints Finger Print Minutiae Record (FMR) - ISO/IEC 19794-2 Finger Print Image Record (FIR) - ISO/IEC 19794-4 IRIS Iris Image Record (IIR) - ISO/IEC 19794-6 Face Face Image Data (FID) - ISO/IEC 19794-5 ABIS should support the following types of biometric images - Unsegmented fingerprint images (4-4-2), or - Unsegmented fingerprint images (4-1-4-1), or - Unsegmented fingerprint images (2-2-2-2-2), or - Individual fingerprint images (segmented) - IRIS images (Left, and Right) - Face photograph ABIS should support the following requests Management Registration Ping Insert GetPendingRequests Identify GetReferenceCounts Delete Configure Ping Key terminologies ABIS Automated Biometric Identification System whose primary responsibility is de-duplication of biometric data (finger print & Iris). Candidate list Each Identify request to ABIS can return zero or more records that match the provided biometric data. The returned records is referred to as the candidate list Reference Database The database maintained with the ABIS system which stores the biometrics data of an individual Gallery A subset of the population in the Reference Database used as a target to search for a specific entry. ABIS should have a provision to take an Identify or Verify request with a target gallery parameter ID Repository The database within MOSIP which contains the ID details (demographic & biometric) of an Individual. This database is not related to Reference Database. TargetFPIR FPIR is False Positive Identification Rate. This metric tells how many false positive identification an ABIS achieved. An ABIS should be able to send the FMR as defined below. round (-10 * log10 (achieved targetFPIR)) The acceptable values are Target FPIR score 1 in 1,000 30 1 in 10,000 40 1 in 100,000 50 API's to interact with an ABIS system ABIS APIs details the API's that an ABIS system must support","title":"Interface"},{"location":"Automated-Biometric-Identification-System-(ABIS)-Interface/#overview","text":"MOSIP will interact with ABIS only via message queues JSON format will be used for all control messages like INSERT, IDENTIFY etc... to interact with ABIS CBEFF XML format will be used to send biometrics data to ABIS (Please refer to MOSIP Biometric Data Specifications for cbeff data sample) Inside the CBEFF XML individual biometrics data must be in respective ISO format Biometric data sent to ABIS will follow the below standards. CBEFF: ISO/IEC 19785-3 (Please refer to MOSIP Biometric Data Specifications for details) Fingerprints Finger Print Minutiae Record (FMR) - ISO/IEC 19794-2 Finger Print Image Record (FIR) - ISO/IEC 19794-4 IRIS Iris Image Record (IIR) - ISO/IEC 19794-6 Face Face Image Data (FID) - ISO/IEC 19794-5 ABIS should support the following types of biometric images - Unsegmented fingerprint images (4-4-2), or - Unsegmented fingerprint images (4-1-4-1), or - Unsegmented fingerprint images (2-2-2-2-2), or - Individual fingerprint images (segmented) - IRIS images (Left, and Right) - Face photograph ABIS should support the following requests Management Registration Ping Insert GetPendingRequests Identify GetReferenceCounts Delete Configure Ping","title":"Overview"},{"location":"Automated-Biometric-Identification-System-(ABIS)-Interface/#key-terminologies","text":"","title":"Key terminologies"},{"location":"Automated-Biometric-Identification-System-(ABIS)-Interface/#abis","text":"Automated Biometric Identification System whose primary responsibility is de-duplication of biometric data (finger print & Iris).","title":"ABIS"},{"location":"Automated-Biometric-Identification-System-(ABIS)-Interface/#candidate-list","text":"Each Identify request to ABIS can return zero or more records that match the provided biometric data. The returned records is referred to as the candidate list","title":"Candidate list"},{"location":"Automated-Biometric-Identification-System-(ABIS)-Interface/#reference-database","text":"The database maintained with the ABIS system which stores the biometrics data of an individual","title":"Reference Database"},{"location":"Automated-Biometric-Identification-System-(ABIS)-Interface/#gallery","text":"A subset of the population in the Reference Database used as a target to search for a specific entry. ABIS should have a provision to take an Identify or Verify request with a target gallery parameter","title":"Gallery"},{"location":"Automated-Biometric-Identification-System-(ABIS)-Interface/#id-repository","text":"The database within MOSIP which contains the ID details (demographic & biometric) of an Individual. This database is not related to Reference Database.","title":"ID Repository"},{"location":"Automated-Biometric-Identification-System-(ABIS)-Interface/#targetfpir","text":"FPIR is False Positive Identification Rate. This metric tells how many false positive identification an ABIS achieved. An ABIS should be able to send the FMR as defined below. round (-10 * log10 (achieved targetFPIR)) The acceptable values are Target FPIR score 1 in 1,000 30 1 in 10,000 40 1 in 100,000 50","title":"TargetFPIR"},{"location":"Automated-Biometric-Identification-System-(ABIS)-Interface/#apis-to-interact-with-an-abis-system","text":"ABIS APIs details the API's that an ABIS system must support","title":"API's to interact with an ABIS system"},{"location":"Biometric-Functions-API-Specification/","text":"Draft 3 (August, 2019) MOSIP uses biometrics in the registration and the authentication processes. As part of this it requires specialized processing of the biometrics data for different types of biometrics. This document defines the interface for the Java Library providing the functional support for processing biometrics. Function Quality Check Use Cases When a biometric image is received by MOSIP in the registration client using a forced capture, this method is used to check the quality of the image. Server side validation of quality of biometric images uses this method When external biometric images are received to be put on record this method is used to determine the quality of the received biometric image Signature Input Parameters Biometric Image in \u201cBiometric Image Record\u201d format. This could be FIR, IIR etc. Control Flags is an optional list of name value pairs that can be used to configure the behavior of the library. Output Parameters QualityScore object with Quality score and Analytics. The quality score is on a scale of 100 - Higher is better. Errors/Exceptions Unsupported biometric type Unsupported image format Processing error Behavior Fingerprint The biometric image record is a Fingerprint Image Record. The FIR structure is explained in a later section The image is a jpeg2000 format lossless image The quality score will be using NFIQ2 for 500 dpi images and NFIQ for other densities The analytics data returned can have information on finger index, liveness, etc. Iris The biometric image record is a Iris Image Record. The IIR structure is explained in a later section The image is a jpeg2000 format lossless image The quality score will be on a scale of 100 and will factor focus, blur, eyelid position etc. The analytics data returned can have information on eye index, eyelid position, iris obscuration, gaze angle etc. Face The biometric image record is a Face Image Record. The FaceIR structure is explained in a later section The image is a jpeg2000 format lossless image The quality score will be on a scale of 100 and will factor ICAO standards The analytics data returned can have information on tilt, missing landmarks, lighting etc. Notes For new biometrics that come up the quality check could be different. The quality score could be a single or composite score in the future. Function Matcher Use Cases Used for matching a biometric received in an auth transaction with the biometrics on record Used for authentication of operators in offline mode Used for prevention of erroneous submission of operator biometrics in place of registrant\u2019s biometric on registration client Signature Input Parameters Sample Input Image Record (1) - This is a Biometric Image Record with metadata and image data. This is the freshly received input which needs to be matched. Match List of Image Records (n) - This is the set of biometrics on record that the input images needs to be matched against. The smaller this list the better the performance. Also there will be outer limits to the size of this list based on the library used. Control Flags is an optional list of name value pairs that can be used to configure the behavior of the library. Output Parameters List of MatchScore object with Match score and Analytics. The match score is on a scale of 100 - Higher is better. Errors / Exceptions Unsupported biometric type Unsupported image format Mismatch in biometric types (sample to record) Mixed biometric types (mix of types in the on records list) Processing error Behavior Fingerprint The biometric record will have a jpeg2000 format lossless image or minutiae in an ISO template (FMR). The on record biometrics will be jpeg200 format lossless images or biometric extracts tagged to a specific extractor. Best matches are provided for an image to image match. The sample and on records data are both images. The matcher uses its own extraction algorithm on the images to be compared. In case the sample is not an image but minutiae (FMR) then the match accuracy and efficacy might be lower as the extraction templates and algorithms might not be the same In case the on record extract is a format that the matcher does not understand the match will not happen. Also if the comparison is between two extracts (Minutiae) the False Rejection and False Acceptance cases might be high A typical 1 : n match is used when comparing one finger received to a set of fingers (upto 10) on record for that person or a limited set of people (upto 10 per person). The match score will return a score specific to the library as well as a confidence score on a scale of 100. Iris Input and on record images are jpeg2000 format lossless images The matcher uses its own algorithm to perform extraction, segmentation and identifying patterns on the images being compared. Typical comparisons are 1 : 2 for a person and 1 : n for multiple people. (deduplication scenario) The match score will return a score specific to the library as well as a confidence score on a scale of 100. Face Input and on record images are jpeg2000 format lossless images The matcher uses its own algorithm to perform extraction, segmentation and identifying landmarks on the images being compared. Typical comparisons are 1 : 1 for one person and 1 : n for multiple people (deduplication scenario) The match score will return a score specific to the library as well as a confidence score on a scale of 100. Notes As new biometrics come up the matching score will be as per the sample set and standards defined for that biometric. Function Composite Matcher Use Cases Used for matching multiple modes of biometrics of an individual Signature Input Parameters Sample List of Input Image Records (1 set) - This is a Biometric Image Record list with metadata and image data belonging to an individual. This is the freshly received input which needs to be matched. Match List of Image Records (1 set) - This is the set of biometrics on record that the input images needs to be matched against belonging to an individual. Control Flags is an optional list of name value pairs that can be used to configure the behavior of the library. Output Parameters A MatchScore object with Match score and Analytics. The match score is on a scale of 100 - Higher is better. Errors/Exceptions Unsupported biometric type Unsupported image format Mismatch in biometric types (sample to record) Processing error Behavior Multiple Fingerprints The biometric record will have a jpeg2000 format lossless image or minutiae in an ISO template (FMR). There will be multiple fingers in the input based on which the composite match has to be done. A 1 : 1 composite match is done when comparing one set of fingers received to a set of fingers on record for that person. The match score will return a score specific to the library as well as a confidence score on a scale of 100. Multiple Biometric Types Input and on record images are jpeg2000 format lossless images The data consists of mixed biometric type like Iris + Fingerprint etc. The data on record should have the biometric elements being passed in for a match to happen. The match is a 1 : 1 composite match. The match score will return a score specific to the library as well as a confidence score on a scale of 100. Notes As new biometrics come up the matching score will be as per the sample set and standards defined for that biometric. Function Extractor Use Cases Used to extract salient features and patterns of a biometric to use in fast comparison In case of fingerprints this is called Minutiae and a standard representation of minutiae is an ISO template for FMR Signature Input Parameters Biometric Image in \u201cBiometric Image Record\u201d format. This could be FIR, IIR, FaceIR etc. Control Flags is an optional list of name value pairs that can be used to configure the behavior of the library. Output Parameters Biometric Extract Record in the form of FMR or a proprietary structure. The extract also contains a quality score. Errors/Exceptions Unsupported biometric type Unsupported image format Processing error Behavior Fingerprint The extractor will extract either an ISO template FMR or extract a proprietary representation that can give better results. The extract record will be marked with the format and any additional metadata needed. Iris Currently there are no known IRIS extraction standards. Any template extracted can be consumed only by a corresponding matcher. Face Face analysis yields landmarks on face and these are typically stored in proprietary formats. Any template extracted can be used by a corresponding matcher. Notes For non fingerprint biometrics characteristics and patterns and landmarks might be identified and stored in a custom format. This format will be proprietary to each extractor and can be only consumed by its corresponding matcher. The extract will also contain meta information about the extractor and the version of the algorithm it uses and any other assumption it has made in the process of extraction that can be useful during matching. Function Segmenter Use Cases Used to split images into individual biometric segments when received from external sources Signature Input Parameters Biometric Image in \u201cBiometric Image Record\u201d format. This could be FIR, IIR, FaceIR etc. Control Flags is an optional list of name value pairs that can be used to configure the behavior of the library. Output Parameters List of Biometric Image Record that contains the individual biometrics Errors/Exceptions Unsupported biometric type Unsupported image format Processing error Behavior Fingerprint Input will contain unsegmented image such as left slap or right slap or two thumbs Output will be biometrics of each finger present in the input image Iris Input will contain unsegmented image such as both eyes Output will be biometrics of each eye present in the input image Face Not applicable at present Notes The segmenter will identify the individual fingers present. Image Record Formats Image Records will have three parts Header Information Image Block Security Block The header information will comply to the standard specified here: https://www.iso.org/standard/60458.html The image block itself will comply to formats specified here https://github.com/mosip/mosip/wiki/MOSIP-Biometric-Data-Specifications#data-standards-for-registration The Security Block will be as per CBEFF specification. It is optional. The cbeff format reference is here - http://docs.oasis-open.org/bioserv/BIAS/v2.0/csprd01/schemas/cbeff_ed2.xsd Score Object The quality score object will have two sections. One is the score section and the other is the analytics section. Score Parameters Graded score on a scale of 100. This is an unsigned float that represents a % and can have values between 0 and 100 including decimals like 82.45. Internal Score as defined on the scale of the implementing library. This is an unsigned long Analytics List of name value pairs that can be used to convey additional information. The values filled are specific to the implementing library. This could contain information about the aspects where quality is failing for e.g. ICAO compliance for tilt or lighting. In case of matches it could contain information like the NIST score, the algorithm used for matching and more. Appendix A - Java Specifications class Score { float scaledScore; //0 - 100, used for internal classification and efficacy analysis long internalScore; // used against threshold specified in config file KeyValuePair[] analyticsInfo; // detailed breakdown and other information } class CompositeScore { float scaledScore; //0 - 100, used for internal classification and efficacy analysis long internalScore; // used against threshold specified in config file Score[] individualScores; // List of score for individual matches. Array size matches the input sample array size. KeyValuePair[] analyticsInfo; // detailed breakdown and other information } class BIR { BIRVersion version; BIRVersion cbeffVersion; BIRInfo birInfo; BDBInfo bdbInfo; byte[] bdb; byte[] sb; SBInfo sbInfo; List<JAXBElement<String>> element; } class BIRInfo { String creator; String index; // UUID with pattern [a-fA-F0-9]{8}\\-([a-fA-F0-9]{4}\\-){3}[a-fA-F0-9]{12} byte[] payload; Boolean integrity; DateTime creationDate; DateTime notValidBefore; DateTime notValidAfter; } class BDBInfo { byte[] challengeResponse; String index; // UUID with pattern [a-fA-F0-9]{8}\\-([a-fA-F0-9]{4}\\-){3}[a-fA-F0-9]{12} Boolean encryption; DateTime creationDate; DateTime notValidBefore; DateTime notValidAfter; BiometricType[] type; BiometricSubType subtype; ProcessedLevelType level; RegistryInfo product; RegistryInfo captureDevice; RegistryInfo featureExtractionAlgorithm; RegistryInfo comparisonAlgorithm; RegistryInfo compressionAlgorithm; PurposeType purpose; Integer quality; } class RegistryInfo { String organization; String type; } class SBInfo { RegistryIDType format; } class BIRVersion { int major; int minor; } enum SingleType { Scent, DNA, Ear, Face, Finger, Foot, HandGeometry, Vein, Iris, Retina, Voice, Gait, Keystroke, LipMovement, SignatureSign } enum SingleAnySubtypeType { Left, Right, Thumb, IndexFinger, MiddleFinger, RingFinger, LittleFinger } enum ProcessedLevelType { Raw, Intermediate, Processed } enum PurposeType { Verify, Identify, Enroll, EnrollVerify, EnrollIdentify, Audit } interface IBioApi { Score[] match(BIR sample, BIR[] gallery, KeyValuePair[] flags); CompositeScore compositeMatch(BIR[] sampleList, BIR[] recordList, KeyValuePair[] flags); QualityScore checkQuality(BIR sample, KeyValuePair[] flags); BIR extractTemplate(BIR sample, KeyValuePair[] flags); BIR[] segment(BIR sample, KeyValuePair[] flags); } The above code snippets can be referred to from here - CBEFF-util and Bio API","title":"Biometric SDKs"},{"location":"Code-of-Conduct/","text":"Our Pledge In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards Examples of behaviour that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behaviour by participants include: The use of sexualised language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities Project maintainers are responsible for clarifying the standards of acceptable behaviour and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behaviour. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviours that they deem inappropriate, threatening, offensive, or harmful. Scope This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement Instances of abusive, harassing, or otherwise unacceptable behaviour may be reported by contacting the project team at conduct@mosip.io. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Code of Conduct"},{"location":"Code-of-Conduct/#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"Code-of-Conduct/#our-standards","text":"Examples of behaviour that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behaviour by participants include: The use of sexualised language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"Code-of-Conduct/#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behaviour and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behaviour. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviours that they deem inappropriate, threatening, offensive, or harmful.","title":"Our Responsibilities"},{"location":"Code-of-Conduct/#scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"Code-of-Conduct/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behaviour may be reported by contacting the project team at conduct@mosip.io. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"Enforcement"},{"location":"Code-of-Conduct/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Attribution"},{"location":"Contributor-Guide/","text":"Welcome Welcome to MOSIP, and thank you for thinking of contributing! Before you get started, Please make sure to read and observe our Code of Conduct . Check out our repositories and Platform Documentation Join us in our chatroom and developer mailing list Your First Contribution Help is always welcome with MOSIP! Documentation can be simplified, code can be clarified, and test coverage can be improved. If you want to get started on contributing, there are multiple ways to do so - a) Contribute to code (fix bugs, implement new features, enhance features) b) Contribute to improve specifications. c) Suggest new features and feature enhancements. d) Test MOSIP and log bugs. e) Help us fix documentation errors. f) Help spread the word on MOSIP among interested developers. There are multiple repositories within the MOSIP organization. To contribute to (a) Code, take a look at the Open Issues that are not assigned to anyone and put a comment saying \u201cI would like to take this up\u201d along with your approach and design preferably with pseudo code. The issue shall be assigned to you after review. Alternatively, please join our developer mailing list and send an email there. The core contributors team will interact with you via Github Issue comments. You may work in the forked version of the repository and send pull requests via Github. For (b) - (e), i.e. new bugs, specs, features or documentation improvements, please log Github Issues under the appropriate repository, or join and write in at our developer mailing list Getting in touch If you have questions about the development process Feel free to jump into our Gitter channel or Join the mosip-dev@groups.io mailing list. * For general queries, try info@mosip.io Community Events We actively organise online and in-person meetups and discussions. Keep an eye on this page for the latest.","title":"Contributor Guide"},{"location":"Contributor-Guide/#welcome","text":"Welcome to MOSIP, and thank you for thinking of contributing! Before you get started, Please make sure to read and observe our Code of Conduct . Check out our repositories and Platform Documentation Join us in our chatroom and developer mailing list","title":"Welcome"},{"location":"Contributor-Guide/#your-first-contribution","text":"Help is always welcome with MOSIP! Documentation can be simplified, code can be clarified, and test coverage can be improved. If you want to get started on contributing, there are multiple ways to do so - a) Contribute to code (fix bugs, implement new features, enhance features) b) Contribute to improve specifications. c) Suggest new features and feature enhancements. d) Test MOSIP and log bugs. e) Help us fix documentation errors. f) Help spread the word on MOSIP among interested developers. There are multiple repositories within the MOSIP organization. To contribute to (a) Code, take a look at the Open Issues that are not assigned to anyone and put a comment saying \u201cI would like to take this up\u201d along with your approach and design preferably with pseudo code. The issue shall be assigned to you after review. Alternatively, please join our developer mailing list and send an email there. The core contributors team will interact with you via Github Issue comments. You may work in the forked version of the repository and send pull requests via Github. For (b) - (e), i.e. new bugs, specs, features or documentation improvements, please log Github Issues under the appropriate repository, or join and write in at our developer mailing list","title":"Your First Contribution"},{"location":"Contributor-Guide/#getting-in-touch","text":"If you have questions about the development process Feel free to jump into our Gitter channel or Join the mosip-dev@groups.io mailing list. * For general queries, try info@mosip.io","title":"Getting in touch"},{"location":"Contributor-Guide/#community-events","text":"We actively organise online and in-person meetups and discussions. Keep an eye on this page for the latest.","title":"Community Events"},{"location":"FAQs/","text":"","title":"FAQs"},{"location":"Features/","text":"","title":"Features"},{"location":"Getting-Started/","text":"Getting Started 1. Getting the Source Code 2. Setup and Configure Jenkins 3. Setup and Configure Jfrog Artifactory Version 6.5.2 4. Setup and Configure SonarQube version 7.3 5. Setup and Configure Docker Registry 6. Installing External Dependencies 7. Configuring MOSIP 8. MOSIP Deployment 1. Getting the Source Code [\u2191] Knowledge of Linux and Azure with Kubernetes are required to follow the instructions. MOSIP source code can be obtained via creating a fork of mosip-platform Github repository from the URL . To know more about how to fork code from Github follow this guide . Once Forked, start the process of setting up your CI/CD tools to build and run MOSIP. NOTE MOSIP configuration has been seperated from the source code. For running the source code, you will be needing a fork of mosip-config repository from this URL . All the configuration files will be stored under config-templates folder under this repository. 2. Setup and Configure Jenkins [\u2191] In this step, we will setup jenkins and configure it. Configuration contains steps like creating credentials, creating pipelines using xml files present in MOSIP source code, connecting Jenkins to recently forked repository and creating webhooks. Lets look at these steps one by one - A. Installing Jenkins version 2.150.1 Jenkins installation is standard(see How to install Jenkins ), but to use MOSIP supported build pipelines you have to install Jenkins in an Redhat 7.6 environment. The prerequisite for installing Jenkins is you should have java already installed and path for JAVA_HOME is also set. Also the following plugins have to be installed list of plugins - Github Plugin Artifactory Plugin Credentials Plugin Docker Pipeline Plugin Email Extension Plugin Pipeline Plugin Publish Over SSH Plugin SonarQube Scanner for Jenkins Plugin SSH Agent Plugin Pipeline Utility Steps Plugin M2 Release Plugin SSH Credentials Plugin * Office 365 Plugin Once the plugin installation is complete, run this command in Jenkins Script Console - System.setProperty(\"hudson.model.DirectoryBrowserSupport.CSP\", \"\") This above command modifies Content Security Policy in Jenkins to enable loading of style and javascript for HTML Reports. B. Setting Up Github for/in Jenkins Setting up Github for/in Jenkins involves putting the Jenkins Webhook url in Github Repo so that Github can inform Jenkins for push events(look at Webhooks and Github hook ). After hooks are in place, setup Github credentials inside Jenkins, so that on webhook event our pipeline can checkout the code from Github. To set up Github Credentials, follow these steps - I. Goto Jenkins II. Goto Credentials -> System III. Goto Global credentials IV. Click on Add Credentials V. Now use following details Kind=Username with password Scope=Global (Jenkins, nodes, items, all child items, etc) Username=<Your Github Username> Password=<Your Github Password> ID=Some Unique Identifier to refer to this credentials (to autogenerate this, leave this blank) Description=<It is optional> VI. Now since our Jenkinsfile usage this github credentials, update the credentials id in the Jenkinsfile. C. Create Pipelines Next step after Jenkins installation is to configure/create Jenkins Jobs. These Jenkins Jobs are written as Jenkins Pipelines and respective Jenkinsfile in URL . MOSIP currently has 5 Jenkins jobs that take care of CI/CD process for Development Environment. They are - master-branch-build-all-modules Jenkinsfile for master-branch-build-all-modules can be found in URL , named MasterJenkinsfile This Job is used to build MOSIP as a single unit. This Job also acts like a nightly process to check the build status of MOSIP code in Master Branch. To create this Job you need to create a new Item in Jenkins as a Pipeline Project. Here is the configuration for Pipeline you might have to explicitly change to use MOSIP provided Jenkinsfile- As it can be seen from the above image this pipeline uses Jenkinsfile present in master branch of mosip-platform repository. You need to provide the Github credentials that this pipeline will take to connect and download this Jenkinsfile at the time of the build. Let us now look into this Jenkinsfile. Jenkinsfile for this pipeline is written in Groovy Language using the scripted style of writing code. Then we have module specific Jenkinsfile for individual Modules. These Modules are: Kernel Registration Registration-Processor Pre-Registration ID-Repository ID-Authentication Each Module's CI/CD Jenkins script can be found in URL . This Jenkins script will be named Jenkinsfile and is responsible to build and deploy the entire Module to Dev environment For promoting these modules to QA, there is a pipeline named PromoteToQAJenkinsFile which is located in root directory of mosip source code. This pipeline tags the entire code, runs build process, and once everything is successful, it deploys the entire code to QA environment. In each Jenkinsfile you will see some variables starting with params. These variables are passed as parameters into the Jenkins jobs. You have to setup these parameters variables in your jenkins to use these Jenkinsfiles. These Variables include: NOTE -> To set up parameters for a jenkins job, go inside the jenkins job-> Click Configure->Click on \"This project is parameterized\" and provide the parameter name and value. BRANCH_NAME REGISTRY_URL REGISTRY_NAME REGISTRY_CREDENTIALS GIT_URL GIT_CREDENTIALS BUIILD_OPTION NOTE We are building docker images in each of the JenkinsFile, so docker should be installed and accessible for Jenkins user. Please install Docker version 18.09.3 in your Jenkins instance. 3. Setup and Configure Jfrog Artifactory Version 6.5.2 [\u2191] For installing and setting up Jfrog, following steps here need to be followed. Once the setup is complete, please add the following remote repositories to your Jfrog configuration and point them to libs-release virtual repository: Maven Central Jcentre * Openimaj To configure Maven to resolve artifacts through Artifactory you need to modify the settings.xml of Jenkins machine's m2_home to point to JFrog. To generate these settings, go to Artifact Repository Browser of the Artifacts module, select Set Me Up. In the Set Me Up dialog, set Maven in the Tool field and click \"Generate Maven Settings\". For more information on artifactory configuration refer here NOTE JFrog Artifactory setup by MOSIP is open to public for read only access. So if any of the modules are dependent on previous modules, that you don't have built, you need to connect to our JFrog server to pull those dependencies. For doing that, in the settings.xml file that you generated above, replace url of ID with repository snapshot and release to our Jfrog URLs which will be : 1. <url>http://<abcd>.mosip.io/artifactory/libs-snapshot</url> for libs-snapshot 2. <url>http://<abcd>.mosip.io/artifactory/libs-release</url> for libs-release Once you are done with pulling the dependencies you need, you can replace it back to your Jfrog URLs. Also if you are planning to import all versions of the Mosip modules in Jfrog to your VM or Jfrog, make sure you have enough space in your Jfrog VM where you will be importing these dependencies. 4. Setup and Configure SonarQube version 7.3 [\u2191] SonarQube server can be setup by following single instructions given here . For configuring SonarQube with Jenkins, steps given here can be followed. Steps to install SonarQube on Ubuntu 16.04 Perform a system update * sudo apt-get update * sudo apt-get -y upgrade Install JDK * sudo add-apt-repository ppa:webupd8team/java * sudo apt-get update * sudo apt install oracle-java8-installer We can now check the version of Java by typing: * java -version Install and configure PostgreSQL * sudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt/ `lsb_release -cs`-pgdg main\" >> /etc/apt/sources.list.d/pgdg.list' * wget -q https://www.postgresql.org/media/keys/ACCC4CF8.asc -O - | sudo apt-key add - * sudo apt-get -y install postgresql postgresql-contrib Start PostgreSQL server and enable it to start automatically at boot time by running: * sudo systemctl start postgresql * sudo systemctl enable postgresql Change the password for the default PostgreSQL user. * sudo passwd postgres Switch to the postgres user. * su - postgres Create a new user by typing: * createuser sonar Switch to the PostgreSQL shell. * psql Set a password for the newly created user for SonarQube database. * ALTER USER sonar WITH ENCRYPTED password 'StrongPassword'; Create a new database for PostgreSQL database by running: * CREATE DATABASE sonar OWNER sonar; Exit from the psql shell: * \\q Switch back to the sudo user by running the exit command. Download and configure SonarQube * wget https://sonarsource.bintray.com/Distribution/sonarqube/sonarqube-6.4.zip Install unzip by running: * apt-get -y install unzip Unzip the archive using the following command. * sudo unzip sonarqube-6.4.zip -d /opt Rename the directory: * sudo mv /opt/sonarqube-6.4 /opt/sonarqube * sudo nano /opt/sonarqube/conf/sonar.properties Find the following lines. #sonar.jdbc.username= #sonar.jdbc.password= Uncomment and provide the PostgreSQL username and password of the database that we have created earlier. It should look like: sonar.jdbc.username=sonar sonar.jdbc.password=StrongPassword Next, find: sonar.jdbc.url=jdbc:postgresql://localhost/sonar Uncomment the line, save the file and exit from the editor. Configure Systemd service SonarQube can be started directly using the startup script provided in the installer package. As a matter of convenience, you should setup a Systemd unit file for SonarQube. * nano /etc/systemd/system/sonar.service Populate the file with: [Unit] Description=SonarQube service After=syslog.target network.target [Service] Type=forking ExecStart=/opt/sonarqube/bin/linux-x86-64/sonar.sh start ExecStop=/opt/sonarqube/bin/linux-x86-64/sonar.sh stop User=root Group=root Restart=always [Install] WantedBy=multi-user.target Start the application by running: * sudo systemctl start sonar Enable the SonarQube service to automatically start at boot time. * sudo systemctl enable sonar To check if the service is running, run: * sudo systemctl status sonar 5. Setup and Configure Docker Registry [\u2191] In this step we will setup and configure a private docker registry, which will be basic authenticated, SSL secured. In our setup we are using azure blobs as storage for our docker images. More options for configuring registry can be found here We are deploying Docker registry as Containerized services. For setting up the registry, Docker and Docker Compose need to be installed. We have setted up the registry in a machine with Redhat 7.6 installed. Once installation is done, the yaml files which we will be using to setup the registry can be found in this link We are using Registry image : registry:2.5.1, registry with any other version can be deployed from here . For routing purpose, we are using HAproxy image dockercloud/haproxy:1.6.2, other options such as ngnix etc. can also be used for the same purpose. We have the following docker-compose files, under this link 1. registry-docker-compose.yml: For basic registry and haproxy setup. 2. registry-docker-compose-basic-authentication.yml: For securing the docker registry through base authentication. For basic authentication, you have to setup a htpasswd file and add a simple user to it. For generating this htpaswd file: * Create Htpasswd_dir directory mkdir -p ~/htpasswd_dir * Create htpasswd file with your username and password docker run --rm --entrypoint htpasswd registry:2 -Bbn <username> \"<password>\" > ~/htpasswd_dir/htpasswd * In the registry-docker-compose-basic-authentication.yml file, replace and with specific values. registry-docker-compose-azure-storage.yml: This file is used for configuring azure blob storage. We are assuming that Azure blob has already been configured by you. Replace REGISTRY_STORAGE_AZURE_ACCOUNTNAME, REGISTRY_STORAGE_AZURE_ACCOUNTKEY, REGISTRY_STORAGE_AZURE_CONTAINER with appropriate values configured by you while setting up azure blob storage. registry-docker-compose-tls-enabled.yml: We are using Let's Encrypt , CA signed SSL certificates. Documentation of Let's Encrypt can be referred here Once Certificates have been generated, replace the property and property in registry-docker-compose-tls-enabled.yml with appropriate values. After completing all the above changes, use docker-compose tool to bring up the container using the following command: docker-compose -f registry-docker-compose.yml -f registry-docker-compose-basic-authentication.yml -f registry-docker-compose-azure-storage.yml -f registry-docker-compose-tls-enabled.yml up -d Once the registry is up and running, variables registryUrl , registryName , registryCredentials can be configured accordingly in Jenkinsfile. For configuring registry Credentials in Jenkins, Username/Password credentials need to be added in Jenkins Global Credentials and credential ID needs to be provided in registryCredentials variable in all the Jenkinsfiles. 6. Installing External Dependencies [\u2191] 6.1 Install and use PostgreSql Version 10.2 on RHEL 7.5 Often simply Postgres, is an object-relational database management system (ORDBMS) with an emphasis on extensibility and standards compliance. It can handle workloads ranging from small single-machine applications to large Internet-facing applications (or for data warehousing) with many concurrent users Postgresql Prerequisites On a Linux or Mac system, you must have superuser privileges to perform a PostgreSQL installation. To perform an installation on a Windows system, you must have administrator privileges. Steps to install Postgresql in RHEL-7.5 Download and install PostgreSQL. $ sudo yum install https://download.postgresql.org/pub/repos/yum/10/redhat/rhel-7-x86_64/pgdg-redhat10-10-2.noarch.rpm $ sudo yum-config-manager --disable pgdg95 checking the postgresql packages $ sudo yum update $ sudo yum list postgresql* ##### Installation command $ sudo yum install postgresql10 postgresql10-server $sudo /usr/pgsql-10/bin/postgresql-10-setup initdb $sudo systemctl enable postgresql-10 ##### Postgresql service stop/start/restart command $ sudo systemctl start postgresql-10 $ sudo systemctl status postgresql-10 $ sudo systemctl stop postgresql-10 To changing default port 5432 to 9001 and connection + buffer size we need to edit the postgresql.conf file from below path PostgreSQL is running on default port 5432. you decide to change the default port, please ensure that your new port number does not conflict with any services running on that port. ##### Steps to change the default port : ###### Open the file and modify the below changes $ sudo vi /var/lib/pgsql/10/data/postgresql.conf listen_addresses = '*' (changed to * instead of local host ) port = 9001 ( uncomment port=5432 and change the port number ###### Open the port 9001 from the VM $ sudo firewall-cmd --zone=public --add-port=9001/tcp --permanent $ sudo firewall-cmd --reload To increase the buffer size and number of postgreSql connection same fine modify the below changes also $ sudo vi /var/lib/pgsql/10/data/postgresql.conf unix_socket_directories = '/var/run/postgresql, /tmp' max_connections = 1000 shared_buffers = 2GB $ sudo systemctl start postgresql-10 To change the default password Login to postgrsql $ sudo su postgres bash-4.2$ psql -p 9001 postgres=# \\password postgres Enter new password: Enter it again: postgres=# \\q sudo systemctl restart postgresql-10 It will ask new password to login to postgresql # example for sourcing the sql file form command line $ psql --username=postgres --host=<server ip> --port=9001 --dbname=postgres Open the file $ sudo vim /var/lib/pgsql/10/data/pg_hba.conf Default lines are present in pg_hab.conf file TYPE DATABASE USER ADDRESS METHOD local all all peer host all all 127.0.0.1/32 ident host all all ::1/128 ident local replication all peer host replication all 127.0.0.1/32 ident host replication all ::1/128 ident Modify with below changes in file /var/lib/pgsql/10/data/pg_hba.conf local all all md5 host all all 127.0.0.1/32 ident host all all 0.0.0.0/0 md5 host all all ::1/128 ident local replication all peer host replication all 127.0.0.1/32 ident host replication all ::1/128 ident sudo systemctl restart postgresql-10 sudo systemctl status postgresql-10 Reference link: https://www.tecmint.com/install-postgresql-on-centos-rhel-fedora 6.2 Install and use Nginx Version-1.15.8 on RHEL 7.5 We are using nginx for webserver andalso proxy server for MOSIP project Create the file named /etc/yum.repos.d/nginx.repo using a text editor such as vim command $ sudo vi /etc/yum.repos.d/nginx.repo Append following for RHEL 7.5 [nginx] name=nginx repo baseurl=http://nginx.org/packages/mainline/rhel/7/$basearch/ gpgcheck=0 enabled=1 After updating repo, please run following commands to install and enable nginx - $ sudo yum update $ sudo yum install nginx $ sudo systemctl enable nginx To start, stop, restart or get status of nginx use the following commands - $ sudo systemctl start nginx $ sudo systemctl stop nginx $ sudo systemctl restart nginx $ sudo systemctl status nginx To edit files use a text editor such as vi $ sudo vi /etc/nginx/nginx.conf Example to configure the nginx for dev environment - ``` user madmin; worker_processes 2; error_log /var/log/nginx/error.log warn; pid /var/run/nginx.pid; events { worker_connections 1024; } http { include /etc/nginx/mime.types; default_type application/octet-stream; log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; access_log /var/log/nginx/access.log main; client_max_body_size 10m; sendfile on; tcp_nopush on; proxy_max_temp_file_size 0; sendfile_max_chunk 10m; keepalive_timeout 65; gzip on; gzip_disable \"msie6\"; gzip_vary on; gzip_proxied any; gzip_comp_level 6; gzip_buffers 16 8k; gzip_http_version 1.1; gzip_min_length 256; gzip_types text/plain text/css application/json application/x-javascript text/xml application/xml application/javascript application/octet-stream application/xml+rss text/javascript application/vnd.ms-fontobject application/x-font-ttf font/opentype image/svg+xml image/x-icon image/png image/jpg; #include /etc/nginx/conf.d/*.conf; HTTP configuration server { listen 80 default_server; listen [::]:80 default_server ipv6only=on; server_name <your-domain-name>; location / { root /usr/share/nginx/html; index index.html index.htm; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; proxy_set_header Host $host; proxy_connect_timeout 3600s; proxy_send_timeout 3600s; proxy_read_timeout 3600s; } return 301 https://$host$request_uri; } HTTPS configuration for your domain server { client_max_body_size 20M; listen *:443 ssl http2; listen [::]:443 ssl http2; server_name dev.mosip.io; ssl on; ssl_certificate <your-letsencrypt-fullchainpem-path>; ssl_certificate_key <your-letsencrypt-privatekey-pem-path>; location /v1/keymanager/ { proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; #proxy_set_header Cookie $http_cookie; proxy_pass http://<your-keymanager-vm-ip>:<port>/v1/keymanager/; } location /registrationprocessor/v1/packetreceiver/ { proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_pass http://<your-dmz-vm-ip>:<port>/registrationprocessor/v1/packetreceiver/; } location /registrationprocessor/v1/registrationstatus/ { proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_pass http://<your-dmz-vm-ip>:<port>/registrationprocessor/v1/registrationstatus/; } location / { proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; #proxy_set_header X-Forwarded-Host $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_connect_timeout 3600s; proxy_send_timeout 3600s; proxy_read_timeout 3600s; proxy_pass https://<your-dev-k8-cluster-endpoint>; //kubernetes end point #proxy_intercept_errors on; #error_page 301 302 307 = @handle_redirects; } } } ``` Use below command to open the port 80/443 from RHEL 7.5 VM $ sudo firewall-cmd --zone=public --add-port=80/tcp --permanent $ sudo firewall-cmd --zone=public --add-port=443/tcp --permanent $ sudo firewall-cmd --reload Generate SSL/TLS for HTTPS - RHEL 7 version, these are the following commands you have to run to generate certificates for nginx server. wget https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm It will install EPEL script for RHEL 7 OS. It is same as PPA for Ubuntu, it will install some extra packages for enterprise linux edition. You can read more about it https://fedoraproject.org/wiki/EPEL. This command will run the EPEL install scripts and enable the EPEL packages for RHEL7 sudo yum install epel-release-latest-7.noarch.rpm This will list all the EPEL packages available for used. yum --disablerepo=\"*\" --enablerepo=\"epel\" list available Check for python2-certbot-nginx package in EPEL Packages. yum --disablerepo=\"*\" --enablerepo=\"epel\" search python2-certbot-nginx This will install python certbot for nginx into VM. sudo yum install python2-certbot-nginx This will generate the certificate for VM. sudo certbot --nginx certonly Troubleshooting: If you facing getting this issue in nginx (13: Permission denied) while connecting to upstream:[nginx] Please run below command - $sudo setsebool -P httpd_can_network_connect 1 or refer link - https://stackoverflow.com/questions/23948527/13-permission-denied-while-connecting-to-upstreamnginx Note: Certficates will be generated at, /etc/letsencrypt/live/ / directory. cert.pem is the certificate and privkey.pem is private key. We are using Let's Encrypt, CA signed SSL certificates. Documentation of Let's Encrypt can be referred here 6.3 Clam AntiVirus Version 0.101.0 ClamAV is a free, cross-platform and open-source antivirus software toolkit able to detect many types of malicious software, including viruses. Steps to install ClamAV in RHEL-7.5 To install clamAV first we need to install EPEL Repository: $ sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm After that we need to install ClamAV and its related tools. $ yum -y install clamav-server clamav-data clamav-update clamav-filesystem clamav clamav-scanner-systemd clamav-devel clamav-lib clamav-server-systemd After completion of above steps, we need to configure installed ClamAV. This can be done via editing /etc/clamd.d/scan.conf . In this file we have to remove Example lines. So that ClamAV can use this file's configurations. We can easily do it via running following command - $ sed -i '/^Example/d' /etc/clamd.d/scan.conf Another thing we need to do in this file is to define our TCP server type. Open this file using - $ vim /etc/clamd.d/scan.conf here this we need to uncomment line with #LocalSocket /var/run/clamd.scan/clamd.sock . Just remove # symbol from the beginning of the line. Now we need to configure FreshClam so that it can update ClamAV db automatically. For doing that follow below steps - First create a backup of original FreshClam Configuration file - $ cp /etc/freshclam.conf /etc/freshclam.conf.bak In this freshclam.conf file, Here also we need to remove Example line from the file. Run following command to delete all Example lines- $ sed -i '/^Example/d' /etc/freshclam.conf Test freshclam via running- $ freshclam After running above command you should see an output similar to this - ClamAV update process started at Thu May 23 07:25:44 2019 . . . . main.cvd is up to date (version: 58, sigs: 4566249, f-level: 60, builder: sigmgr) Downloading daily-25584.cdiff [100%] daily.cld updated (version: 25584, sigs: 1779512, f-level: 63, builder: raynman) bytecode.cld is up to date (version: 331, sigs: 94, f-level: 63, builder: anvilleg) Database updated (6345855 signatures) from database.clamav.net (IP: 104.16.218.84) We will create a service of freshclam so that freshclam will run in the daemon mode and periodically check for updates throughout the day. To do that we will create a service file for freshclam - $ vim /usr/lib/systemd/system/clam-freshclam.service And add below content - [Unit] Description = freshclam scanner After = network.target [Service] Type = forking ExecStart = /usr/bin/freshclam -d -c 4 Restart = on-failure PrivateTmp = true RestartSec = 20sec [Install] WantedBy=multi-user.target Now save and quit. Also reload the systemd daemon to refresh the changes - $ systemctl daemon-reload Next start and enable the freshclam service - $ systemctl start clam-freshclam.service $ systemctl enable clam-freshclam.service Now freshclam setup is complete and our ClamAV db is upto date. We can continue setting up ClamAV. Now we will copy ClamAV service file to system service folder. $ mv /usr/lib/systemd/system/clamd@.service /usr/lib/systemd/system/clamd.service Since we have changed the name, we need to change it at the file that uses this service as well - $ vim /usr/lib/systemd/system/clamd@scan.service Remove @ symbol from .include /lib/systemd/system/clamd@.service line and save the file. We will edit Clamd service file now - $ vim /usr/lib/systemd/system/clamd.service Add following lines at the end of clamd.service file. [Install] WantedBy=multi-user.target And also remove %i symbol from various locations (ex: Description and ExecStart options). Note that at the end of the editing the service file should look something like this - [Unit] Description = clamd scanner daemon Documentation=man:clamd(8) man:clamd.conf(5) https://www.clamav.net/documents/ # Check for database existence # ConditionPathExistsGlob=@DBDIR@/main.{c[vl]d,inc} # ConditionPathExistsGlob=@DBDIR@/daily.{c[vl]d,inc} After = syslog.target nss-lookup.target network.target [Service] Type = forking ExecStart = /usr/sbin/clamd -c /etc/clamd.d/scan.conf Restart = on-failure [Install] WantedBy=multi-user.target Now finally start the ClamAV service. $ systemctl start clamd.service If it works fine, then enable this service and test the status of ClamAV service - $ systemctl enable clamd.service $ systemctl status clamd.service Now in MOSIP we require ClamAV to be available on Port 3310. To expose ClamAV service on Port 3310, edit scan.conf $ vi /etc/clamd.d/scan.conf and Uncomment #TCPSocket 3310 by removing # . After that restart the clamd@scan service - $ systemctl restart clamd@scan.service Since we are exposing ClamAV on 3310 port, we need to allow incoming traffic through this port. In RHEL 7 run below command to add firewall rule - $ sudo firewall-cmd --zone=public --add-port=3310/tcp --permanent $ sudo firewall-cmd --reload Reference link: link 6.4 Steps to Install and configuration CEPH NOTE: Required only if CEPH is used for packet storage. Ceph is an open source software that provides massively scalable and distributed data store. It provides highly scalable object, block and file based storage under a unified system. 1. On Red Hat Enterprise Linux 7, register the target machine with subscription-manager, verify your subscriptions, and enable the \u201cExtras\u201d repository for package dependencies. For example: $ sudo subscription-manager repos --enable=rhel-7-server-extras-rpms 2. Install and enable the Extra Packages for Enterprise Linux (EPEL) repository: $ sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm 3. Add the Ceph repository to your yum configuration file at /etc/yum.repos.d/ceph.repo with the following command. Replace {ceph-stable-release} with a stable Ceph release (e.g., luminous.) For example: cat << EOM > /etc/yum.repos.d/ceph.repo [ceph-noarch] name=Ceph noarch packages baseurl=https://download.ceph.com/rpm-{ceph-stable-release}/el7/noarch enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc EOM 4. Update your repository and install ceph-deploy: $ sudo yum update $ sudo yum install ceph-deploy CEPH NODE SETUP The admin node must have password-less SSH access to Ceph nodes. When ceph-deploy logs in to a Ceph node as a user, that particular user must have passwordless sudo privileges. INSTALL NTP We recommend installing NTP on Ceph nodes (especially on Ceph Monitor nodes) to prevent issues arising from clock drift. See [Clock](//docs.ceph.com/docs/mimic/rados/configuration/mon-config-ref/#clock) for details. $ sudo yum install ntp ntpdate ntp-doc Ensure that you enable the NTP service. Ensure that each Ceph Node uses the same NTP time server. INSTALL SSH SERVER sudo yum install openssh-server Ensure the SSH server is running on ALL Ceph Nodes. 1) Make a directory on admin node in order to keep all the keys and configuration files that ceph-deploy generates. a. mkdir cluster-config b. cd cluster-config 2) Now we create a cluster ceph-deploy new {initial-monitor-node(s)} ceph-deploy new ceph-demo-1 ceph-demo-2 This step marks the nodes as initial monitors. 3) Thereafter, we Install ceph packages on required nodes ceph-deploy install {ceph-node} [\u2026] ceph-deploy install ceph-demo-1 ceph-demo-2 This step will install the latest stable version of ceph, i.e. mimic (13.2.1) on the given nodes. 4) Now, we deploy the initial monitor nodes and gather keys ceph-deploy mon create-initial 5) Now we go ahead and copy our config file and admin key to the admin node as well as ceph-nodes in order to use ceph cli without passing these each time we execute a command. ceph-deploy admin {ceph-node}[\u2026] ceph-deploy admin ceph-demo-1 ceph-demo-2 6) Now, we deploy a manager daemon ceph-deploy mgr create {ceph-node}[\u2026] ceph-deploy mgr create ceph-demo-1 ceph-demo-2 7) We create 2 OSDs, assuming each osd has a unused disk called dev/sdb ceph-deploy osd create\u200a\u2014\u200adata {device} {ceph-node} ceph-deploy osd create \u2014 data /dev/sdb ceph-demo-1 ceph-deploy osd create \u2014 data /dev/sdb ceph-demo-2 After successfully executing these steps, our ceph cluster is up and running. The status and health of the cluster can be checked in by executing $ sudo ceph health $ sudo ceph -s We should get a status saying HEALTH_OK, and a detailed status resembling : cluster: id: 651e9802-b3f0\u20134b1d-a4d6-c57a46635bc9 health: HEALTH_OK services: mon: 2 daemons, quorum ceph-demo-1,ceph-demo-2 mgr: ceph-demo-1(active), standbys: ceph-demo-2 osd: 2 osds: 2 up, 2 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 2.0 GiB used, 18 GiB / 20 GiB avail pgs: Expanding the Existing cluster Now, to demonstrate the ease of expanding a ceph cluster at runtime, we will be adding one node in our running cluster. We will mark that node as osd, manager and monitor to increase the availability of our existing cluster. First of all, we need to make a change to our existing ceph.conf which is present inside the cluster-config directory. We add the following line into it public network = {ip-address}/{bits} public network = 10.142.0.0/24 For this, we need to follow these sample steps: 1) We install ceph packages on 3rd node ceph-deploy install {ceph-node}[\u2026] ceph-deploy install ceph-demo-3 2) We need to push the admin keys and conf to 3rd node. We do it using ceph-deploy admin {ceph-node}[\u2026] ceph-deploy admin ceph-demo-3 3) Now we will add the 3rd node as our monitor ceph-deploy mon add {ceph-nodes} ceph-deploy mon add ceph-demo-3 4) Now, we go ahead and mark 3rd node as our manager ceph-deploy mgr create {ceph-node}[\u2026] ceph-deploy mgr create ceph-demo-3 5) We add 3rd node as OSD by following same steps as done while creating the cluster. ceph-deploy osd create\u200a\u2014\u200adata {path} {ceph-node} CEPH Dashboard Now, going ahead, we can enable the CEPH dashboard in order to view all the cluster status via a UI console. Ceph in its mimic release has provided the users with a new and redesigned dashboard plugin, with the features like restricted control with username/password protection and SSL/TLS support. To enable the dashboard, we need to follow these steps: 1) ceph mgr module enable dashboard 2) ceph dashboard create-self-signed-cert Note: Self signed certificate is only for quick start purpose. 3) ceph mgr module disable dashboard 4) ceph mgr module enable dashboard Now, we will be able to see the CEPH dashboard on the port 8443, which is by default but on requirement can be configured using: ceph config set mgr mgr/dashboard/server_addr $IP ceph config set mgr mgr/dashboard/server_port $PORT To access, and to utilize full functionality of the dashboard, we need to create the login credentials. ceph dashboard set-login-credentials <username> <password> After these steps, our ceph infrastructure is ready with all the configurations to do some actual input output operations. Reference link: http://docs.ceph.com/docs/mimic/start/quick-start-preflight/ 6.5 Steps to Install and configuration LDAP ApacheDs Server installation and config Apache Directory Studio user guide 6.6 Steps to Install and configuration HDFS NOTE: Required only if HDFS is used for packet storage. Refer - Steps-to-Install-and-configuration-HDFS 6.7 Steps to Deploy Kernel Key Manager Service Kernel Keymanager Service is setup outside of Kubernetes cluster on a standalone VM. The steps to setup kernel-keymanager-service are given here To deploy keymanager service, follow below steps - 1. Prerequiste: * A machine with RHEL 7.6 installed. * Docker installed and Docker service enabled. Steps to install Docker ce. $ sudo yum install http://mirror.centos.org/centos/7/extras/x86_64/Packages/container-selinux-2.107-3.el7.noarch.rpm $ sudo yum -y install lvm2 device-mapper device-mapper-persistent-data device-mapper-event device-mapper-libs device-mapper-event-libs $ sudo wget https://download.docker.com/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo $ sudo yum -y install docker-ce $ sudo systemctl start docker $ sudo systemctl status docker Open port 8088 on the VM: sudo firewall-cmd --zone=public --add-port=8088/tcp --permanent sudo firewall-cmd --reload Note: if firewall is not installed in VM, install with \u201csudo yum install firewall\u201d And also open the port. ensure that config server is already deployed. Process to deploy Services in VM through JenkinsFile: Refer the github url for Jenkinsfile : in root directory of kernel module The last stage in the Jenkinsfile viz 'Key-Manager Deployment' in which we are sshing into this newly created VM through Jenkins to deploy this service, basically, running the docker image of key manager. For ssh, place the public key of jenkins inside this newly created VM's authorized_keys under .ssh directory. Generate Docker Registry Credential in jenkins by using docker hub username and password. This will generate the credentialsId. Replace the value for registryCredentials(credentialsId of docker hub) with yours Replace the value for key_manager_vm_ip with IP of your newly created VM. Once done the following command will be used to deploy keymanager to the machine: sudo docker run -tid --ulimit memlock=-1 -p 8088:8088 -v softhsm:/softhsm -e spring_config_url_env=\"${config_url}\" -e spring_config_label_env=\"${branch}\" -e active_profile_env=\"${profile_env}\" --name keymanager \"${registryAddress}\"/kernel-keymanager-service **NOTE- Replace the values for spring_config_url_env, spring_config_label_env, active_profile_env and registryAddress in the above command accordingly 6.8 SMS Gateway configuration Refer kernel-smsnotification-servive Readme here 6.9 Installation of ActiveMQ ActiveMQ is the message broker used for MOSIP Registartion processor module. Installation steps <version> : please check http://www.apache.org/dist/activemq/ to find out the latest version. Tested ActiveMQ version - 5.4.1. Prerequiste: A machine with RHEL 7.6 installed, Docker installed and Docker service enabled. Download activemq using command - wget https://archive.apache.org/dist/activemq/5.14.3/apache-activemq-5.14.3-bin.tar.gz Extract the archive tar -zxvf apache-activemq-<version>-bin.tar.gz Change the permission for startup script chmod 755 apache-activemq-<version> Start activemq service cd apache-activemq-<version> && sudo ./bin/activemq start Check for the installed and started activemq on port 61616. netstat -tulpn Open ports 8161 and 61616 on the VM: sudo firewall-cmd --zone=public --add-port=8161/tcp --permanent sudo firewall-cmd --zone=public --add-port=61616/tcp --permanent sudo firewall-cmd --reload Note: After Installation of activemq, same needs to be mentioned in RegistrationProcessorAbis_{active_profile}.json For e.g : Suppose activemq is configured as tcp://xxx.xxx.xxx.xx:61616, then we for dev need to mention this in RegistrationProcessorAbis_dev.json as { \"abis\": [{ \"name\": \"ABIS1\", \"host\": \"\", \"port\": \"\", \"brokerUrl\": \"tcp://xxx.xxx.xxx.xx:61616\", \"inboundQueueName\": \"abis1-inbound-address_dev\", \"outboundQueueName\": \"abis1-outbound-address_dev\", \"pingInboundQueueName\": \"\", \"pingOutboundQueueName\": \"\", \"userName\": \"admin\", \"password\": \"admin\", \"typeOfQueue\": \"ACTIVEMQ\" } ] } ActiveMQ is also being used in registration-processor-printing-stage and the details need to be mentioned in registration-processor-{active_profile}.properties in the configuration repository. E.g : For dev profile, the property in registration-processor-dev.properties, the Property corresponding to printing-stage related to activemq would be Queue username registration.processor.queue.username={username} #Queue Password registration.processor.queue.password={password} #Queue Url registration.processor.queue.url={queue_url} #Type of the Queue registration.processor.queue.typeOfQueue=ACTIVEMQ #Print Service address registration.processor.queue.address={queue_address} #Post Service address registration.processor.queue.printpostaladdress={postal_queue_address} 7. Configuring MOSIP [\u2191] MOSIP database object deployment / configuration Database deployment consists of the following 4 categories of objects to be deployed on postgresql database. User / Roles: In MOSIP, the following user / roles are defined to perform various activities sysadmin: sysadmin user/role is a super administrator role, who will have all the privileges to performa any task within the database. dbadmin: dbadmin user / role is created to handle all the database administration activities db monitoring, performance tuning, backups, restore, replication setup, etc. appadmin: appadmin user / role is used to perform all the DDL (Data Definition Language) tasks. All the db objects that are created in these databases will be owned by appadmin user. Application User: Each application will have a user / role created to perform DML (Data Manipulation Language) tasks like CRUD operations (select, insert, update, delete). The user prereguser, is created to connect from the application to perform all the DML activities. Similarly, we will have masteruser, prereguser, reguser, idauser, idrepouser, kerneluser, audituser, regprcuser to perform DML tasks for master, pre-registration, registration, ida, ID repository, kernel, audit and registration processor modules respectively. Note: From the above set of roles only application user / role is specific to a application / module. The other user / roles are common which needs to be created per postresql db instance / server. Database and Schema: Each application / module of MOSIP platform will have a database and schema defined. All the objects (tables) related to an application / module would be created under the respective database / schema. In MOSIP the following database and scehmas are defined application / module name Database tool database Name schema name Master / Administration module postgresql mosip_master master Kernel postgresql mosip_kernel kernel Pre-registration postgresql mosip_prereg prereg Registration Apache Derby mosip_reg reg Registration Processor postgresql mosip_regprc regprc ID Authentication postgresql mosip_ida ida ID Repository postgresql mosip_idrepo idrepo Audit postgresql mosip_audit audit IAM postgresql mosip_iam iam idmap postgresql mosip_idmap idmap Note: These databases can be deployed on single or separate database servers / instances. DB Objects (Tables): All the tables of each application / module will be created in their respective database and schema. appadmin user / role will own these objects and the respective application user / role will have access to perform DML operations on these objects. Seed Data: MOSIP platform is designed to provide most of its features to be configured in the system. These configuration are deployed with default setup on config server and few in database. Few of these configuration can be modified / updated by the MOSIP administrator. These configuration include, system configurations, master datasetup, etc. The steps to add new center, machine / device is detailed in Guidelines for Adding Centers, Machine, Users and Devices The system configuration and master data is available under the respective application / database related folder. for example, the master data configuration is available in csv file format under folder . The scripts to create the above objects are available under db_scripts . To deploy the database objects of each application / module except registration client , please refer to README.MD file. These scripts will contain the deployment of all the DB object categories. Note: Please skip Registration client related deployment scripts (Apache derby DB specific) as this will be executed as part of registration client software installation. Setup and configure MOSIP We are using kubernetes configuration server in MOSIP for storing and serving distributed configurations across all the applications and environments. We are storing all applications' configuration in config-templates folder inside our Github Repository here . For getting more details about how to use configuration server with our applications, following developer document can be referred: MOSIP CONFIGURATION SERVER For Deployment of configurations server, go to firstly-deploy-kernel-configuration-server in this document. Application specific configuration for all applications and services are placed in MOSIP config server. A. Global: link B. Kernel: link C. Pre-Registration: link D. Registartion-Processor: link E. IDA: link F. ID-REPO: link H. Registration: link Properties Sections that need to be changed in above module specific files once the external dependencies are installed as per your setup Global --Common properties------------ mosip.base.url --Virus Scanner----------------- mosip.kernel.virus-scanner.host mosip.kernel.virus-scanner.port --FS Adapter-HDFS ------------- mosip.kernel.fsadapter.hdfs.name-node-url # Enable if hadoop security authorization is 'true', default is false mosip.kernel.fsadapter.hdfs.authentication-enabled # If HDFS is security is configured with Kerberos, Key Distribution Center domain mosip.kernel.fsadapter.hdfs.kdc-domain #keytab file path, must be set if authentication-enable is true #read keytab file both classpath and physical path ,append appropriate prefix #for classpath prefix classpath:mosip.keytab #for physical path prefix file:/home/keys/mosip.keytab mosip.kernel.fsadapter.hdfs.keytab-file=classpath:mosip.keytab Kernel --kernel common properties----------------------- mosip.kernel.database.hostname mosip.kernel.database.port --sms notification service----------------------- mosip.kernel.sms.authkey --Email Notification service--------------------- spring.mail.host spring.mail.username spring.mail.password spring.mail.port=587 --Ldap------------ ldap_1_DS.datastore.ipaddress ldap_1_DS.datastore.port --DataBase Properties---------------------------- **_database_password **_database_username 8. MOSIP Deployment [\u2191] Currently for the Development Process MOSIP Platform is deployed as/in the Kubernetes Cluster. We are using Azure Kubernetes Service for provisioning of Cluster. As of now Kubernetes Deployment is deviced in two parts - A. One time setup of MOSIP in Kubernetes Cluster B. Continuous deployment A. One time setup of MOSIP in Kubernetes Cluster One time setup on Kubernetes involves following Steps I. Setting Up local system to communicate with Kubernetes cluster this can be done via kubectl . II. Setting Up the Basic environment for MOSIP to run in Kubernetes Cluster, In this step we will work on this link . following are the files - We will now go through each of the file and see what changes we need to perform. we will be using kubectl to do the deployments from local system. DeployIngressController.yaml - We need not to change anything here. we can directly run this file. To run this use this command kubectl apply -f DeployIngressController.yaml DeployIngress.yaml - This file contains information about routing to different Kubernetes services, So whenever any traffic comes to our Load Balancer IP it will look for this file to route the request. For eg. Let's say if some.example.com is mapped to our kubernetes loadbalancer then if a request is for some.example.com/pre-registration-ui then this request will be redirect to pre-registration-ui on port 80 service. Routes referrring to ping-server and sample-nginx can be removed as these are for testing purpose.To run this use this command kubectl apply -f DeployIngress.yaml DeployServiceIngressService.yaml - We need not to change anything here. we can directly run this file. To run this use this command kubectl apply -f DeployServiceIngressService.yaml DeployDefaultBackend.yaml - We need not to change anything here. we can directly run this file. To run this use this command kubectl apply -f DeployDefaultBackend.yaml docker-registry-secret.yml - This file helps Kubernetes to get the Docker Images from Private Docker Registry. This file is a downloaded YAML of secrets that exists in the Kubernetes. You can either create secret or use this file to deploy secret in Kubernetes. For creating secret for the first time, run below command - kubectl create secret docker-registry <registry-credential-name> --docker-server=<your-registry-server> --docker-username=<your-name> --docker-password=<your-pword> --docker-email=<your-email> Once secret is created on the Kubernetes Cluster, as a backup strategy we can download the created secret using this command kubectl get secret <registry-credential-name> -o yaml --export Once the above deployment is done, we will start deploying MOSIP services. For doing this, we need to look for these directories - Firstly Deploy Kernel Configuration server The script is inside ( https://github.com/mosip/mosip-infra/blob/master/deployment/cloud/kubernetes/configuration-server/config-server-deployment-and-service.yml ) Follow below steps: 1. Create a ssh key and configure it with your git repository. If you have already configured the ssh key for your repository, you can use that one or else follow this 2. Create a secret for Config server to connect to GIT repo. This secret contains your id_rsa key (private key), id_rsa_pub key (public key) and known_hosts which you generated above. We need this secret because config server connects to your Source code management repository, to get configuration for all the services(If you are using ssh URL for cloning the repo). For generating the required secret give the following command: ( Firstly try to connect to GIT repository from your system using ssh url and the key you created above, so that GIT service provider such as GitHub or GitLab comes in your known hosts file): `kubectl create secret generic config-server-secret --from-file=id_rsa=/path/to/.ssh/id_rsa --from-file=id_rsa.pub=/path/to/.ssh/id_rsa.pub --from-file=known_hosts=/path/to/.ssh/known_hosts` <br/> **For Encryption Decryption of properties with configuration server** <br/> <br/> Create keystore with following command: `keytool -genkeypair -alias <your-alias> -keyalg RSA -keystore server.keystore -keypass < key-secret > -storepass < store-password > --dname \"CN=<your-CN>,OU=<OU>,O=<O>,L=<L>,S=<S>,C=<C>\"` The JKS keystore uses a proprietary format. It is recommended to migrate to PKCS12 which is an industry standard format, migrate it using following command: keytool -importkeystore -srckeystore server.keystore -destkeystore server.keystore -deststoretype pkcs12 For more information look here Create file with following content to create keystore secret for encryption decryption of keys using information from keystore created above: apiVersion: v1 kind: Secret metadata: name: config-server-keystore-values-secret type: Opaque data: alias: < base-64-encoded-alias-for keystore > password: < base-64-store-password > secret: < base-64-encoded-key-secret > 5. Save the above file with any name and apply it using: kubectl apply -f < file-name > Create server.keystore as secret to volume mount it inside container: kubectl create secret generic config-server-keystore --from-file=server.keystore=< location-of-your-server.keystore-file-generated-above > Change git_url_env environment variable in kernel-config-server-deployment-and-service.yml to your git ssh url of configuration repository Change git_config_folder_env environment variable in kernel-config-server-deployment-and-service.yml to your configuration folder in git repository. Change spec->template->spec->containers->image from docker-registry.mosip.io:5000/kernel-config-server to < Your Docker Registry >/kernel-config-server Change spec->template->spec->imagePullSecrets->name from pvt-reg-cred to < Your docker registry credentials secret > Once above configuration is done, execute kubectl apply -f kernel-config-server-deployment-and-service.yml More information can be found here Deploy other components: Inside each of the directory there is a file for each service of MOSIP that is exposed as Web API. We need to deploy these files to get these running. But before doing that we need to change Private Docker Registry Address and Docker Registry Secret, so that on deployment time Kubernetes can fetch docker images from correct source using correct credentials. For doing this, follow below steps (for eg. we will use https://github.com/mosip/mosip-infra/blob/master/deployment/cloud/kubernetes/kernel-deployment/kernel-auditmanager-service-deployment-and-service.yml, but you have to repeat the process for all such files) - I. Open a deployment file. II. Change spec->template->spec->containers->image from docker-registry.mosip.io:5000/kernel-auditmanager-service to <Your Docker Registry>/kernel-auditmanager-service III. Change spec->template->spec->imagePullSecrets->name from pvt-reg-cred to <Your docker registry credentials secret> IV. Change active_profile_env to whichever profile you want to activate and spring_config_label_env to the branch from which you want to pick up the configuration V. Save the file and Run kubectl apply -f kernel-auditmanager-service-deployment-and-service.yml After above process is completed, you can run kubectl get services command to see the status of all the MOSIP Services. For Pre-Registration-UI Pre-registration-ui uses a file config.json to configure URLs of backend, which have to be provided as config map in pre-registration-ui-deployment-and-service.yml. For creating the configmap follow below steps: 1. Edit the file scripts -> https://github.com/mosip/mosip-infra/blob/master/deployment/cloud/kubernetes/pre-registration-deployment/pre-registration-ui-configuration.yaml 2. Update https://dev.mosip.io/ value with url of proxy server which points to pre-registration services. (Note: While editing, be careful with escape sequence characters) 3. Execute command Kubectl apply -f pre-registration-ui-configuration.yaml 8.1 Registration-Processor DMZ services deployment Registration Processor DMZ Services are setup externally(deployed in a separate VM). Firstly, update below files present in config folder in configuration repository, and replace the line <to uri=\"https://<dns name>/registrationprocessor/v1/uploader/securezone\" /> with the URL of packet uploader stage. 1. registration-processor-camel-routes-new-dmz-<env-name>.xml 2. registration-processor-camel-routes-update-dmz-<env-name>.xml 3. registration-processor-camel-routes-lost-dmz-<env-name>.xml 4. registration-processor-camel-routes-activate-dmz-<env-name>.xml 5. registration-processor-camel-routes-deactivate-dmz-<env-name>.xml 6. registration-processor-camel-routes-res_update-dmz-<env-name>.xml We are deploying DMZ services into another VM having docker installed. The steps to setup DMZ environment and services deployment: 1. Need to set Up VM with RHEL 7.6 2. Installing the Docker ce: $ sudo yum install http://mirror.centos.org/centos/7/extras/x86_64/Packages/container-selinux-2.107- 3.el7.noarch.rpm $ sudo yum -y install lvm2 device-mapper device-mapper-persistent-data device-mapper-event device-mapper-libs device-mapper-event-libs $ sudo wget https://download.docker.com/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo $ sudo yum -y install docker-ce $ sudo systemctl start docker $ sudo systemctl status docker Need to copy the Jenkins server public key(id_rsa.pub) inside this newly created VM's authorized_keys(because through jenkins job, we will ssh into new VM and deploy) After installing Docker Start the Docker Service command to start the Docker service systemctl start docker command to check Docker is running: systemctl status docker Open the port 8081, 8083 from the VM: Mosip uses port 8081 for registration-processor-packet-receiver-stage and 8083 for registration-processor-registration-status-service. The port ids need to be updated in ngnix configuration. sudo firewall-cmd --zone=public --add-port=8081/tcp --permanent sudo firewall-cmd --reload sudo firewall-cmd --zone=public --add-port=8083/tcp --permanent sudo firewall-cmd --reload Note: if firewall is not installed in VM, install with \u201csudo yum install firewall\u201d And also open the port from AZURE OR AWS or any cloud where the VM is launched. Process to deploy Services in VM through JenkinsFile: The last stage in the Jenkinsfile viz DMZ_Deployment in which we are sshing into this newly created VM through Jenkins to deploy these services, basically, running the docker images of registration processor. Changes to be made in this stage-> a. Replace the value for registryCredentials(credentialsId of docker hub) with yours. b. Replace the value for the variable -> dmz_reg_proc_dev_ip with the IP of your newly created VM. Refer the github url for Jenkinsfile : here Also, instead of following as described in 4th point to use Jenkinsfile, we can do it manually. Steps are -> a. Login into the DMZ VM. b. Perform docker hub login c. Execute the following commands docker run --restart always -it -d -p 8083:8083 -e active_profile_env=\"${profile_env}\" -e spring_config_label_env=\"${label_env}\" -e spring_config_url_env=\"${config_url}\" \"${registryAddress}\"/registration-processor-registration-status-service docker run --restart always -it -d --network host --privileged=true -v /home/ftp1/LANDING_ZONE:/home/ftp1/LANDING_ZONE -v /home/ftp1/ARCHIVE_PACKET_LOCATION:/home/ftp1/ARCHIVE_PACKET_LOCATION -e active_profile_env=\"${profile_env}\" -e spring_config_label_env=\"${label_env}\" -e spring_config_url_env=\"${config_url}\" \"${registryAddress}\"/registration-processor-packet-receiver-stage docker run --restart always -it -d --network host --privileged=true -e active_profile_env=\"${profile_env}\" -e spring_config_label_env=\"${label_env}\" -e spring_config_url_env=\"${config_url}\" -e zone_env=dmz \"${registryAddress}\"/registration-processor-common-camel-bridge Note - Please change the environmental variables(active_profile_env, spring_config_label_env, spring_config_url_env ,registryAddress) in the above commands accordingly whether you are executing manually in your new VM or through Jenkinsfile. Packet uploader stage in secure zone will fetch file from dmz to upload it into Distributed File System,to connect to dmz vm either we can login using username and password or using ppk file. If password value is available in config property name registration.processor.dmz.server.password then uploader will connect using username and password. otherwise it will login using ppk file available in config with property name registration.processor.vm.ppk. PPK generation command ssh-keygen -t rsa -b 4096 -f mykey. 8.2 Kernel Salt Generator Kernel Salt Generator Job is a one-time job which is run to populate salts to be used to hash and encrypt data. This generic job takes schema and table name as input, and generates and populates salts in the given schema and table. Salt Generator Deployment steps a. Login into the VM. Open the port 8092 from the VM: sudo firewall-cmd --zone=public --add-port=8092/tcp --permanent sudo firewall-cmd --reload And also open the port from AZURE OR AWS or any cloud where the VM is launched. b. Perform docker hub login c. Execute the following commands sequentially one after the other. Wait for the completion of previous command before executing next commands. 1. docker run -it -p 8092:8092 -e active_profile_env=\"${profile_env}\" -e spring_config_label_env=\"${label_env}\" -e spring_config_url_env=\"${config_url}\" -e spring_config_name_env=id-repository -e db_url=mosip.idrepo.identity.db.shard.url -e schema_name=idrepo -e table_name=uin_hash_salt \"${registryAddress}\"/id-repository-salt-generator 2. docker run -it -p 8092:8092 -e active_profile_env=\"${profile_env}\" -e spring_config_label_env=\"${label_env}\" -e spring_config_url_env=\"${config_url}\" -e spring_config_name_env=id-repository -e db_url=mosip.idrepo.identity.db.shard.url -e schema_name=idrepo -e table_name=uin_encrypt_salt \"${registryAddress}\"/id-repository-salt-generator 3. docker run -it -p 8092:8092 -e active_profile_env=\"${profile_env}\" -e spring_config_label_env=\"${label_env}\" -e spring_config_url_env=\"${config_url}\" -e spring_config_name_env=id-repository -e db_url=mosip.idrepo.vid.db.url -e schema_name=idmap -e table_name=uin_hash_salt \"${registryAddress}\"/id-repository-salt-generator 4. docker run -it -p 8092:8092 -e active_profile_env=\"${profile_env}\" -e spring_config_label_env=\"${label_env}\" -e spring_config_url_env=\"${config_url}\" -e spring_config_name_env=id-repository -e db_url=mosip.idrepo.vid.db.url -e schema_name=idmap -e table_name=uin_encrypt_salt \"${registryAddress}\"/id-repository-salt-generator 5. docker run -it -d -p 8092:8092 -e active_profile_env=\"${profile_env}\" -e spring_config_label_env=\"${label_env}\" -e spring_config_url_env=\"${config_url}\" -e spring_config_name_env=id-authentication -e schema_name=ida -e table_name=uin_hash_salt \"${registryAddress}\"/authentication-salt-generator 6. docker run -it -d -p 8092:8092 -e active_profile_env=\"${profile_env}\" -e spring_config_label_env=\"${label_env}\" -e spring_config_url_env=\"${config_url}\" -e spring_config_name_env=id-authentication -e schema_name=ida -e table_name=uin_encrypt_salt \"${registryAddress}\"/authentication-salt-generator Note - Please change the value for variables active_profile_env, spring_config_label_env, spring_config_url_env and registryAddress in the above four commands accordingly 8.3 First User Registration and Onboarding Refer to wiki for detailed procedure on First User Registration and Onboarding","title":"Getting Started"},{"location":"Getting-Started/#getting-started","text":"1. Getting the Source Code 2. Setup and Configure Jenkins 3. Setup and Configure Jfrog Artifactory Version 6.5.2 4. Setup and Configure SonarQube version 7.3 5. Setup and Configure Docker Registry 6. Installing External Dependencies 7. Configuring MOSIP 8. MOSIP Deployment","title":"Getting Started"},{"location":"Getting-Started/#1-getting-the-source-code","text":"Knowledge of Linux and Azure with Kubernetes are required to follow the instructions. MOSIP source code can be obtained via creating a fork of mosip-platform Github repository from the URL . To know more about how to fork code from Github follow this guide . Once Forked, start the process of setting up your CI/CD tools to build and run MOSIP. NOTE MOSIP configuration has been seperated from the source code. For running the source code, you will be needing a fork of mosip-config repository from this URL . All the configuration files will be stored under config-templates folder under this repository.","title":"1. Getting the Source Code [\u2191]"},{"location":"Getting-Started/#2-setup-and-configure-jenkins","text":"In this step, we will setup jenkins and configure it. Configuration contains steps like creating credentials, creating pipelines using xml files present in MOSIP source code, connecting Jenkins to recently forked repository and creating webhooks. Lets look at these steps one by one -","title":"2. Setup and Configure Jenkins [\u2191]"},{"location":"Getting-Started/#a-installing-jenkins-version-21501","text":"Jenkins installation is standard(see How to install Jenkins ), but to use MOSIP supported build pipelines you have to install Jenkins in an Redhat 7.6 environment. The prerequisite for installing Jenkins is you should have java already installed and path for JAVA_HOME is also set. Also the following plugins have to be installed list of plugins - Github Plugin Artifactory Plugin Credentials Plugin Docker Pipeline Plugin Email Extension Plugin Pipeline Plugin Publish Over SSH Plugin SonarQube Scanner for Jenkins Plugin SSH Agent Plugin Pipeline Utility Steps Plugin M2 Release Plugin SSH Credentials Plugin * Office 365 Plugin Once the plugin installation is complete, run this command in Jenkins Script Console - System.setProperty(\"hudson.model.DirectoryBrowserSupport.CSP\", \"\") This above command modifies Content Security Policy in Jenkins to enable loading of style and javascript for HTML Reports.","title":"A. Installing Jenkins version 2.150.1"},{"location":"Getting-Started/#b-setting-up-github-forin-jenkins","text":"Setting up Github for/in Jenkins involves putting the Jenkins Webhook url in Github Repo so that Github can inform Jenkins for push events(look at Webhooks and Github hook ). After hooks are in place, setup Github credentials inside Jenkins, so that on webhook event our pipeline can checkout the code from Github. To set up Github Credentials, follow these steps - I. Goto Jenkins II. Goto Credentials -> System III. Goto Global credentials IV. Click on Add Credentials V. Now use following details Kind=Username with password Scope=Global (Jenkins, nodes, items, all child items, etc) Username=<Your Github Username> Password=<Your Github Password> ID=Some Unique Identifier to refer to this credentials (to autogenerate this, leave this blank) Description=<It is optional> VI. Now since our Jenkinsfile usage this github credentials, update the credentials id in the Jenkinsfile.","title":"B. Setting Up Github for/in Jenkins"},{"location":"Getting-Started/#c-create-pipelines","text":"Next step after Jenkins installation is to configure/create Jenkins Jobs. These Jenkins Jobs are written as Jenkins Pipelines and respective Jenkinsfile in URL . MOSIP currently has 5 Jenkins jobs that take care of CI/CD process for Development Environment. They are - master-branch-build-all-modules Jenkinsfile for master-branch-build-all-modules can be found in URL , named MasterJenkinsfile This Job is used to build MOSIP as a single unit. This Job also acts like a nightly process to check the build status of MOSIP code in Master Branch. To create this Job you need to create a new Item in Jenkins as a Pipeline Project. Here is the configuration for Pipeline you might have to explicitly change to use MOSIP provided Jenkinsfile- As it can be seen from the above image this pipeline uses Jenkinsfile present in master branch of mosip-platform repository. You need to provide the Github credentials that this pipeline will take to connect and download this Jenkinsfile at the time of the build. Let us now look into this Jenkinsfile. Jenkinsfile for this pipeline is written in Groovy Language using the scripted style of writing code. Then we have module specific Jenkinsfile for individual Modules. These Modules are: Kernel Registration Registration-Processor Pre-Registration ID-Repository ID-Authentication Each Module's CI/CD Jenkins script can be found in URL . This Jenkins script will be named Jenkinsfile and is responsible to build and deploy the entire Module to Dev environment For promoting these modules to QA, there is a pipeline named PromoteToQAJenkinsFile which is located in root directory of mosip source code. This pipeline tags the entire code, runs build process, and once everything is successful, it deploys the entire code to QA environment. In each Jenkinsfile you will see some variables starting with params. These variables are passed as parameters into the Jenkins jobs. You have to setup these parameters variables in your jenkins to use these Jenkinsfiles. These Variables include: NOTE -> To set up parameters for a jenkins job, go inside the jenkins job-> Click Configure->Click on \"This project is parameterized\" and provide the parameter name and value. BRANCH_NAME REGISTRY_URL REGISTRY_NAME REGISTRY_CREDENTIALS GIT_URL GIT_CREDENTIALS BUIILD_OPTION NOTE We are building docker images in each of the JenkinsFile, so docker should be installed and accessible for Jenkins user. Please install Docker version 18.09.3 in your Jenkins instance.","title":"C. Create Pipelines"},{"location":"Getting-Started/#3-setup-and-configure-jfrog-artifactory-version-652","text":"For installing and setting up Jfrog, following steps here need to be followed. Once the setup is complete, please add the following remote repositories to your Jfrog configuration and point them to libs-release virtual repository: Maven Central Jcentre * Openimaj To configure Maven to resolve artifacts through Artifactory you need to modify the settings.xml of Jenkins machine's m2_home to point to JFrog. To generate these settings, go to Artifact Repository Browser of the Artifacts module, select Set Me Up. In the Set Me Up dialog, set Maven in the Tool field and click \"Generate Maven Settings\". For more information on artifactory configuration refer here NOTE JFrog Artifactory setup by MOSIP is open to public for read only access. So if any of the modules are dependent on previous modules, that you don't have built, you need to connect to our JFrog server to pull those dependencies. For doing that, in the settings.xml file that you generated above, replace url of ID with repository snapshot and release to our Jfrog URLs which will be : 1. <url>http://<abcd>.mosip.io/artifactory/libs-snapshot</url> for libs-snapshot 2. <url>http://<abcd>.mosip.io/artifactory/libs-release</url> for libs-release Once you are done with pulling the dependencies you need, you can replace it back to your Jfrog URLs. Also if you are planning to import all versions of the Mosip modules in Jfrog to your VM or Jfrog, make sure you have enough space in your Jfrog VM where you will be importing these dependencies.","title":"3. Setup and Configure Jfrog Artifactory Version 6.5.2 [\u2191]"},{"location":"Getting-Started/#4-setup-and-configure-sonarqube-version-73","text":"SonarQube server can be setup by following single instructions given here . For configuring SonarQube with Jenkins, steps given here can be followed. Steps to install SonarQube on Ubuntu 16.04 Perform a system update * sudo apt-get update * sudo apt-get -y upgrade Install JDK * sudo add-apt-repository ppa:webupd8team/java * sudo apt-get update * sudo apt install oracle-java8-installer We can now check the version of Java by typing: * java -version Install and configure PostgreSQL * sudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt/ `lsb_release -cs`-pgdg main\" >> /etc/apt/sources.list.d/pgdg.list' * wget -q https://www.postgresql.org/media/keys/ACCC4CF8.asc -O - | sudo apt-key add - * sudo apt-get -y install postgresql postgresql-contrib Start PostgreSQL server and enable it to start automatically at boot time by running: * sudo systemctl start postgresql * sudo systemctl enable postgresql Change the password for the default PostgreSQL user. * sudo passwd postgres Switch to the postgres user. * su - postgres Create a new user by typing: * createuser sonar Switch to the PostgreSQL shell. * psql Set a password for the newly created user for SonarQube database. * ALTER USER sonar WITH ENCRYPTED password 'StrongPassword'; Create a new database for PostgreSQL database by running: * CREATE DATABASE sonar OWNER sonar; Exit from the psql shell: * \\q Switch back to the sudo user by running the exit command. Download and configure SonarQube * wget https://sonarsource.bintray.com/Distribution/sonarqube/sonarqube-6.4.zip Install unzip by running: * apt-get -y install unzip Unzip the archive using the following command. * sudo unzip sonarqube-6.4.zip -d /opt Rename the directory: * sudo mv /opt/sonarqube-6.4 /opt/sonarqube * sudo nano /opt/sonarqube/conf/sonar.properties Find the following lines. #sonar.jdbc.username= #sonar.jdbc.password= Uncomment and provide the PostgreSQL username and password of the database that we have created earlier. It should look like: sonar.jdbc.username=sonar sonar.jdbc.password=StrongPassword Next, find:","title":"4. Setup and Configure SonarQube version 7.3 [\u2191]"},{"location":"Getting-Started/#sonarjdbcurljdbcpostgresqllocalhostsonar","text":"Uncomment the line, save the file and exit from the editor. Configure Systemd service SonarQube can be started directly using the startup script provided in the installer package. As a matter of convenience, you should setup a Systemd unit file for SonarQube. * nano /etc/systemd/system/sonar.service Populate the file with: [Unit] Description=SonarQube service After=syslog.target network.target [Service] Type=forking ExecStart=/opt/sonarqube/bin/linux-x86-64/sonar.sh start ExecStop=/opt/sonarqube/bin/linux-x86-64/sonar.sh stop User=root Group=root Restart=always [Install] WantedBy=multi-user.target Start the application by running: * sudo systemctl start sonar Enable the SonarQube service to automatically start at boot time. * sudo systemctl enable sonar To check if the service is running, run: * sudo systemctl status sonar","title":"sonar.jdbc.url=jdbc:postgresql://localhost/sonar"},{"location":"Getting-Started/#5-setup-and-configure-docker-registry","text":"In this step we will setup and configure a private docker registry, which will be basic authenticated, SSL secured. In our setup we are using azure blobs as storage for our docker images. More options for configuring registry can be found here We are deploying Docker registry as Containerized services. For setting up the registry, Docker and Docker Compose need to be installed. We have setted up the registry in a machine with Redhat 7.6 installed. Once installation is done, the yaml files which we will be using to setup the registry can be found in this link We are using Registry image : registry:2.5.1, registry with any other version can be deployed from here . For routing purpose, we are using HAproxy image dockercloud/haproxy:1.6.2, other options such as ngnix etc. can also be used for the same purpose. We have the following docker-compose files, under this link 1. registry-docker-compose.yml: For basic registry and haproxy setup. 2. registry-docker-compose-basic-authentication.yml: For securing the docker registry through base authentication. For basic authentication, you have to setup a htpasswd file and add a simple user to it. For generating this htpaswd file: * Create Htpasswd_dir directory mkdir -p ~/htpasswd_dir * Create htpasswd file with your username and password docker run --rm --entrypoint htpasswd registry:2 -Bbn <username> \"<password>\" > ~/htpasswd_dir/htpasswd * In the registry-docker-compose-basic-authentication.yml file, replace and with specific values. registry-docker-compose-azure-storage.yml: This file is used for configuring azure blob storage. We are assuming that Azure blob has already been configured by you. Replace REGISTRY_STORAGE_AZURE_ACCOUNTNAME, REGISTRY_STORAGE_AZURE_ACCOUNTKEY, REGISTRY_STORAGE_AZURE_CONTAINER with appropriate values configured by you while setting up azure blob storage. registry-docker-compose-tls-enabled.yml: We are using Let's Encrypt , CA signed SSL certificates. Documentation of Let's Encrypt can be referred here Once Certificates have been generated, replace the property and property in registry-docker-compose-tls-enabled.yml with appropriate values. After completing all the above changes, use docker-compose tool to bring up the container using the following command: docker-compose -f registry-docker-compose.yml -f registry-docker-compose-basic-authentication.yml -f registry-docker-compose-azure-storage.yml -f registry-docker-compose-tls-enabled.yml up -d Once the registry is up and running, variables registryUrl , registryName , registryCredentials can be configured accordingly in Jenkinsfile. For configuring registry Credentials in Jenkins, Username/Password credentials need to be added in Jenkins Global Credentials and credential ID needs to be provided in registryCredentials variable in all the Jenkinsfiles.","title":"5. Setup and Configure Docker Registry [\u2191]"},{"location":"Getting-Started/#6-installing-external-dependencies","text":"","title":"6. Installing External Dependencies [\u2191]"},{"location":"Getting-Started/#61-install-and-use-postgresql-version-102-on-rhel-75","text":"Often simply Postgres, is an object-relational database management system (ORDBMS) with an emphasis on extensibility and standards compliance. It can handle workloads ranging from small single-machine applications to large Internet-facing applications (or for data warehousing) with many concurrent users Postgresql Prerequisites On a Linux or Mac system, you must have superuser privileges to perform a PostgreSQL installation. To perform an installation on a Windows system, you must have administrator privileges.","title":"6.1 Install and use PostgreSql Version 10.2 on RHEL 7.5"},{"location":"Getting-Started/#steps-to-install-postgresql-in-rhel-75","text":"","title":"Steps to install Postgresql in RHEL-7.5"},{"location":"Getting-Started/#download-and-install-postgresql","text":"$ sudo yum install https://download.postgresql.org/pub/repos/yum/10/redhat/rhel-7-x86_64/pgdg-redhat10-10-2.noarch.rpm $ sudo yum-config-manager --disable pgdg95","title":"Download and install PostgreSQL."},{"location":"Getting-Started/#checking-the-postgresql-packages","text":"$ sudo yum update $ sudo yum list postgresql* ##### Installation command $ sudo yum install postgresql10 postgresql10-server $sudo /usr/pgsql-10/bin/postgresql-10-setup initdb $sudo systemctl enable postgresql-10 ##### Postgresql service stop/start/restart command $ sudo systemctl start postgresql-10 $ sudo systemctl status postgresql-10 $ sudo systemctl stop postgresql-10 To changing default port 5432 to 9001 and connection + buffer size we need to edit the postgresql.conf file from below path PostgreSQL is running on default port 5432. you decide to change the default port, please ensure that your new port number does not conflict with any services running on that port. ##### Steps to change the default port : ###### Open the file and modify the below changes $ sudo vi /var/lib/pgsql/10/data/postgresql.conf listen_addresses = '*' (changed to * instead of local host ) port = 9001 ( uncomment port=5432 and change the port number ###### Open the port 9001 from the VM $ sudo firewall-cmd --zone=public --add-port=9001/tcp --permanent $ sudo firewall-cmd --reload","title":"checking  the postgresql packages"},{"location":"Getting-Started/#to-increase-the-buffer-size-and-number-of-postgresql-connection-same-fine-modify-the-below-changes-also","text":"$ sudo vi /var/lib/pgsql/10/data/postgresql.conf unix_socket_directories = '/var/run/postgresql, /tmp' max_connections = 1000 shared_buffers = 2GB $ sudo systemctl start postgresql-10","title":"To increase the buffer size and number of postgreSql connection  same fine modify the below changes also"},{"location":"Getting-Started/#to-change-the-default-password","text":"Login to postgrsql $ sudo su postgres bash-4.2$ psql -p 9001 postgres=# \\password postgres Enter new password: Enter it again: postgres=# \\q sudo systemctl restart postgresql-10 It will ask new password to login to postgresql","title":"To change the default password"},{"location":"Getting-Started/#example-for-sourcing-the-sql-file-form-command-line","text":"$ psql --username=postgres --host=<server ip> --port=9001 --dbname=postgres Open the file $ sudo vim /var/lib/pgsql/10/data/pg_hba.conf","title":"# example  for sourcing the sql file form command line"},{"location":"Getting-Started/#default-lines-are-present-in-pg_habconf-file","text":"TYPE DATABASE USER ADDRESS METHOD local all all peer host all all 127.0.0.1/32 ident host all all ::1/128 ident local replication all peer host replication all 127.0.0.1/32 ident host replication all ::1/128 ident","title":"Default lines are present in pg_hab.conf file "},{"location":"Getting-Started/#modify-with-below-changes-in-file-varlibpgsql10datapg_hbaconf","text":"local all all md5 host all all 127.0.0.1/32 ident host all all 0.0.0.0/0 md5 host all all ::1/128 ident local replication all peer host replication all 127.0.0.1/32 ident host replication all ::1/128 ident sudo systemctl restart postgresql-10 sudo systemctl status postgresql-10 Reference link: https://www.tecmint.com/install-postgresql-on-centos-rhel-fedora","title":"Modify  with below changes in file  /var/lib/pgsql/10/data/pg_hba.conf"},{"location":"Getting-Started/#62-install-and-use-nginx-version-1158-on-rhel-75","text":"We are using nginx for webserver andalso proxy server for MOSIP project Create the file named /etc/yum.repos.d/nginx.repo using a text editor such as vim command $ sudo vi /etc/yum.repos.d/nginx.repo Append following for RHEL 7.5 [nginx] name=nginx repo baseurl=http://nginx.org/packages/mainline/rhel/7/$basearch/ gpgcheck=0 enabled=1 After updating repo, please run following commands to install and enable nginx - $ sudo yum update $ sudo yum install nginx $ sudo systemctl enable nginx To start, stop, restart or get status of nginx use the following commands - $ sudo systemctl start nginx $ sudo systemctl stop nginx $ sudo systemctl restart nginx $ sudo systemctl status nginx To edit files use a text editor such as vi $ sudo vi /etc/nginx/nginx.conf Example to configure the nginx for dev environment - ``` user madmin; worker_processes 2; error_log /var/log/nginx/error.log warn; pid /var/run/nginx.pid; events { worker_connections 1024; } http { include /etc/nginx/mime.types; default_type application/octet-stream; log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; access_log /var/log/nginx/access.log main; client_max_body_size 10m; sendfile on; tcp_nopush on; proxy_max_temp_file_size 0; sendfile_max_chunk 10m; keepalive_timeout 65; gzip on; gzip_disable \"msie6\"; gzip_vary on; gzip_proxied any; gzip_comp_level 6; gzip_buffers 16 8k; gzip_http_version 1.1; gzip_min_length 256; gzip_types text/plain text/css application/json application/x-javascript text/xml application/xml application/javascript application/octet-stream application/xml+rss text/javascript application/vnd.ms-fontobject application/x-font-ttf font/opentype image/svg+xml image/x-icon image/png image/jpg; #include /etc/nginx/conf.d/*.conf;","title":"6.2 Install and use Nginx Version-1.15.8 on RHEL 7.5"},{"location":"Getting-Started/#http-configuration","text":"server { listen 80 default_server; listen [::]:80 default_server ipv6only=on; server_name <your-domain-name>; location / { root /usr/share/nginx/html; index index.html index.htm; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; proxy_set_header Host $host; proxy_connect_timeout 3600s; proxy_send_timeout 3600s; proxy_read_timeout 3600s; } return 301 https://$host$request_uri; }","title":"HTTP configuration"},{"location":"Getting-Started/#https-configuration-for-your-domain","text":"server { client_max_body_size 20M; listen *:443 ssl http2; listen [::]:443 ssl http2; server_name dev.mosip.io; ssl on; ssl_certificate <your-letsencrypt-fullchainpem-path>; ssl_certificate_key <your-letsencrypt-privatekey-pem-path>; location /v1/keymanager/ { proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; #proxy_set_header Cookie $http_cookie; proxy_pass http://<your-keymanager-vm-ip>:<port>/v1/keymanager/; } location /registrationprocessor/v1/packetreceiver/ { proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_pass http://<your-dmz-vm-ip>:<port>/registrationprocessor/v1/packetreceiver/; } location /registrationprocessor/v1/registrationstatus/ { proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_pass http://<your-dmz-vm-ip>:<port>/registrationprocessor/v1/registrationstatus/; } location / { proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; #proxy_set_header X-Forwarded-Host $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_connect_timeout 3600s; proxy_send_timeout 3600s; proxy_read_timeout 3600s; proxy_pass https://<your-dev-k8-cluster-endpoint>; //kubernetes end point #proxy_intercept_errors on; #error_page 301 302 307 = @handle_redirects; } } } ``` Use below command to open the port 80/443 from RHEL 7.5 VM $ sudo firewall-cmd --zone=public --add-port=80/tcp --permanent $ sudo firewall-cmd --zone=public --add-port=443/tcp --permanent $ sudo firewall-cmd --reload","title":"HTTPS configuration for your domain"},{"location":"Getting-Started/#generate-ssltls-for-https-","text":"RHEL 7 version, these are the following commands you have to run to generate certificates for nginx server. wget https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm It will install EPEL script for RHEL 7 OS. It is same as PPA for Ubuntu, it will install some extra packages for enterprise linux edition. You can read more about it https://fedoraproject.org/wiki/EPEL. This command will run the EPEL install scripts and enable the EPEL packages for RHEL7 sudo yum install epel-release-latest-7.noarch.rpm This will list all the EPEL packages available for used. yum --disablerepo=\"*\" --enablerepo=\"epel\" list available Check for python2-certbot-nginx package in EPEL Packages. yum --disablerepo=\"*\" --enablerepo=\"epel\" search python2-certbot-nginx This will install python certbot for nginx into VM. sudo yum install python2-certbot-nginx This will generate the certificate for VM. sudo certbot --nginx certonly Troubleshooting: If you facing getting this issue in nginx (13: Permission denied) while connecting to upstream:[nginx] Please run below command - $sudo setsebool -P httpd_can_network_connect 1 or refer link - https://stackoverflow.com/questions/23948527/13-permission-denied-while-connecting-to-upstreamnginx Note: Certficates will be generated at, /etc/letsencrypt/live/ / directory. cert.pem is the certificate and privkey.pem is private key. We are using Let's Encrypt, CA signed SSL certificates. Documentation of Let's Encrypt can be referred here","title":"Generate SSL/TLS for HTTPS -"},{"location":"Getting-Started/#63-clam-antivirus-version-01010","text":"ClamAV is a free, cross-platform and open-source antivirus software toolkit able to detect many types of malicious software, including viruses.","title":"6.3 Clam AntiVirus Version 0.101.0"},{"location":"Getting-Started/#steps-to-install-clamav-in-rhel-75","text":"To install clamAV first we need to install EPEL Repository: $ sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm After that we need to install ClamAV and its related tools. $ yum -y install clamav-server clamav-data clamav-update clamav-filesystem clamav clamav-scanner-systemd clamav-devel clamav-lib clamav-server-systemd After completion of above steps, we need to configure installed ClamAV. This can be done via editing /etc/clamd.d/scan.conf . In this file we have to remove Example lines. So that ClamAV can use this file's configurations. We can easily do it via running following command - $ sed -i '/^Example/d' /etc/clamd.d/scan.conf Another thing we need to do in this file is to define our TCP server type. Open this file using - $ vim /etc/clamd.d/scan.conf here this we need to uncomment line with #LocalSocket /var/run/clamd.scan/clamd.sock . Just remove # symbol from the beginning of the line. Now we need to configure FreshClam so that it can update ClamAV db automatically. For doing that follow below steps - First create a backup of original FreshClam Configuration file - $ cp /etc/freshclam.conf /etc/freshclam.conf.bak In this freshclam.conf file, Here also we need to remove Example line from the file. Run following command to delete all Example lines- $ sed -i '/^Example/d' /etc/freshclam.conf Test freshclam via running- $ freshclam After running above command you should see an output similar to this - ClamAV update process started at Thu May 23 07:25:44 2019 . . . . main.cvd is up to date (version: 58, sigs: 4566249, f-level: 60, builder: sigmgr) Downloading daily-25584.cdiff [100%] daily.cld updated (version: 25584, sigs: 1779512, f-level: 63, builder: raynman) bytecode.cld is up to date (version: 331, sigs: 94, f-level: 63, builder: anvilleg) Database updated (6345855 signatures) from database.clamav.net (IP: 104.16.218.84) We will create a service of freshclam so that freshclam will run in the daemon mode and periodically check for updates throughout the day. To do that we will create a service file for freshclam - $ vim /usr/lib/systemd/system/clam-freshclam.service And add below content - [Unit] Description = freshclam scanner After = network.target [Service] Type = forking ExecStart = /usr/bin/freshclam -d -c 4 Restart = on-failure PrivateTmp = true RestartSec = 20sec [Install] WantedBy=multi-user.target Now save and quit. Also reload the systemd daemon to refresh the changes - $ systemctl daemon-reload Next start and enable the freshclam service - $ systemctl start clam-freshclam.service $ systemctl enable clam-freshclam.service Now freshclam setup is complete and our ClamAV db is upto date. We can continue setting up ClamAV. Now we will copy ClamAV service file to system service folder. $ mv /usr/lib/systemd/system/clamd@.service /usr/lib/systemd/system/clamd.service Since we have changed the name, we need to change it at the file that uses this service as well - $ vim /usr/lib/systemd/system/clamd@scan.service Remove @ symbol from .include /lib/systemd/system/clamd@.service line and save the file. We will edit Clamd service file now - $ vim /usr/lib/systemd/system/clamd.service Add following lines at the end of clamd.service file. [Install] WantedBy=multi-user.target And also remove %i symbol from various locations (ex: Description and ExecStart options). Note that at the end of the editing the service file should look something like this - [Unit] Description = clamd scanner daemon Documentation=man:clamd(8) man:clamd.conf(5) https://www.clamav.net/documents/ # Check for database existence # ConditionPathExistsGlob=@DBDIR@/main.{c[vl]d,inc} # ConditionPathExistsGlob=@DBDIR@/daily.{c[vl]d,inc} After = syslog.target nss-lookup.target network.target [Service] Type = forking ExecStart = /usr/sbin/clamd -c /etc/clamd.d/scan.conf Restart = on-failure [Install] WantedBy=multi-user.target Now finally start the ClamAV service. $ systemctl start clamd.service If it works fine, then enable this service and test the status of ClamAV service - $ systemctl enable clamd.service $ systemctl status clamd.service Now in MOSIP we require ClamAV to be available on Port 3310. To expose ClamAV service on Port 3310, edit scan.conf $ vi /etc/clamd.d/scan.conf and Uncomment #TCPSocket 3310 by removing # . After that restart the clamd@scan service - $ systemctl restart clamd@scan.service Since we are exposing ClamAV on 3310 port, we need to allow incoming traffic through this port. In RHEL 7 run below command to add firewall rule - $ sudo firewall-cmd --zone=public --add-port=3310/tcp --permanent $ sudo firewall-cmd --reload","title":"Steps to install ClamAV in RHEL-7.5"},{"location":"Getting-Started/#reference-link-link","text":"","title":"Reference link: link"},{"location":"Getting-Started/#64-steps-to-install-and-configuration-ceph","text":"NOTE: Required only if CEPH is used for packet storage. Ceph is an open source software that provides massively scalable and distributed data store. It provides highly scalable object, block and file based storage under a unified system.","title":"6.4 Steps to Install and configuration CEPH"},{"location":"Getting-Started/#1-on-red-hat-enterprise-linux-7-register-the-target-machine-with-subscription-manager-verify-your-subscriptions-and-enable-the-extras-repository-for-package-dependencies-for-example","text":"$ sudo subscription-manager repos --enable=rhel-7-server-extras-rpms","title":"1. On Red Hat Enterprise Linux 7, register the target machine with subscription-manager, verify your subscriptions, and enable the \u201cExtras\u201d repository for package dependencies. For example:"},{"location":"Getting-Started/#2-install-and-enable-the-extra-packages-for-enterprise-linux-epel-repository","text":"$ sudo yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm","title":"2. Install and enable the Extra Packages for Enterprise Linux (EPEL) repository:"},{"location":"Getting-Started/#3-add-the-ceph-repository-to-your-yum-configuration-file-at-etcyumreposdcephrepo-with-the-following-command-replace-ceph-stable-release-with-a-stable-ceph-release-eg-luminous-for-example","text":"cat << EOM > /etc/yum.repos.d/ceph.repo [ceph-noarch] name=Ceph noarch packages baseurl=https://download.ceph.com/rpm-{ceph-stable-release}/el7/noarch enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc EOM","title":"3. Add the Ceph repository to your yum configuration file at /etc/yum.repos.d/ceph.repo with the following command.  Replace {ceph-stable-release} with a stable Ceph release (e.g., luminous.) For example:"},{"location":"Getting-Started/#4-update-your-repository-and-install-ceph-deploy","text":"$ sudo yum update $ sudo yum install ceph-deploy","title":"4. Update your repository and install ceph-deploy:"},{"location":"Getting-Started/#ceph-node-setup","text":"The admin node must have password-less SSH access to Ceph nodes. When ceph-deploy logs in to a Ceph node as a user, that particular user must have passwordless sudo privileges.","title":"CEPH NODE SETUP"},{"location":"Getting-Started/#install-ntp","text":"We recommend installing NTP on Ceph nodes (especially on Ceph Monitor nodes) to prevent issues arising from clock drift. See [Clock](//docs.ceph.com/docs/mimic/rados/configuration/mon-config-ref/#clock) for details. $ sudo yum install ntp ntpdate ntp-doc Ensure that you enable the NTP service. Ensure that each Ceph Node uses the same NTP time server.","title":"INSTALL NTP"},{"location":"Getting-Started/#install-ssh-server","text":"sudo yum install openssh-server Ensure the SSH server is running on ALL Ceph Nodes. 1) Make a directory on admin node in order to keep all the keys and configuration files that ceph-deploy generates. a. mkdir cluster-config b. cd cluster-config 2) Now we create a cluster ceph-deploy new {initial-monitor-node(s)} ceph-deploy new ceph-demo-1 ceph-demo-2","title":"INSTALL SSH SERVER"},{"location":"Getting-Started/#this-step-marks-the-nodes-as-initial-monitors","text":"3) Thereafter, we Install ceph packages on required nodes ceph-deploy install {ceph-node} [\u2026] ceph-deploy install ceph-demo-1 ceph-demo-2","title":"This step marks the nodes as initial monitors."},{"location":"Getting-Started/#this-step-will-install-the-latest-stable-version-of-ceph-ie-mimic-1321-on-the-given-nodes","text":"4) Now, we deploy the initial monitor nodes and gather keys ceph-deploy mon create-initial 5) Now we go ahead and copy our config file and admin key to the admin node as well as ceph-nodes in order to use ceph cli without passing these each time we execute a command. ceph-deploy admin {ceph-node}[\u2026] ceph-deploy admin ceph-demo-1 ceph-demo-2 6) Now, we deploy a manager daemon ceph-deploy mgr create {ceph-node}[\u2026] ceph-deploy mgr create ceph-demo-1 ceph-demo-2 7) We create 2 OSDs, assuming each osd has a unused disk called dev/sdb ceph-deploy osd create\u200a\u2014\u200adata {device} {ceph-node} ceph-deploy osd create \u2014 data /dev/sdb ceph-demo-1 ceph-deploy osd create \u2014 data /dev/sdb ceph-demo-2","title":"This step will install the latest stable version of ceph, i.e. mimic (13.2.1) on the given nodes."},{"location":"Getting-Started/#after-successfully-executing-these-steps-our-ceph-cluster-is-up-and-running-the-status-and-health-of-the-cluster-can-be-checked-in-by-executing","text":"$ sudo ceph health $ sudo ceph -s","title":"After successfully executing these steps, our ceph cluster is up and running. The status and health of the cluster can be checked in by executing"},{"location":"Getting-Started/#we-should-get-a-status-saying-health_ok-and-a-detailed-status-resembling","text":"cluster: id: 651e9802-b3f0\u20134b1d-a4d6-c57a46635bc9 health: HEALTH_OK services: mon: 2 daemons, quorum ceph-demo-1,ceph-demo-2 mgr: ceph-demo-1(active), standbys: ceph-demo-2 osd: 2 osds: 2 up, 2 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 2.0 GiB used, 18 GiB / 20 GiB avail pgs:","title":"We should get a status saying HEALTH_OK, and a detailed status resembling :"},{"location":"Getting-Started/#expanding-the-existing-cluster","text":"Now, to demonstrate the ease of expanding a ceph cluster at runtime, we will be adding one node in our running cluster. We will mark that node as osd, manager and monitor to increase the availability of our existing cluster. First of all, we need to make a change to our existing ceph.conf which is present inside the cluster-config directory. We add the following line into it","title":"Expanding the Existing cluster"},{"location":"Getting-Started/#public-network-ip-addressbits","text":"public network = 10.142.0.0/24","title":"public network = {ip-address}/{bits}"},{"location":"Getting-Started/#for-this-we-need-to-follow-these-sample-steps","text":"1) We install ceph packages on 3rd node ceph-deploy install {ceph-node}[\u2026] ceph-deploy install ceph-demo-3 2) We need to push the admin keys and conf to 3rd node. We do it using ceph-deploy admin {ceph-node}[\u2026] ceph-deploy admin ceph-demo-3 3) Now we will add the 3rd node as our monitor ceph-deploy mon add {ceph-nodes} ceph-deploy mon add ceph-demo-3 4) Now, we go ahead and mark 3rd node as our manager ceph-deploy mgr create {ceph-node}[\u2026] ceph-deploy mgr create ceph-demo-3 5) We add 3rd node as OSD by following same steps as done while creating the cluster. ceph-deploy osd create\u200a\u2014\u200adata {path} {ceph-node}","title":"For this, we need to follow these sample steps:"},{"location":"Getting-Started/#ceph-dashboard","text":"Now, going ahead, we can enable the CEPH dashboard in order to view all the cluster status via a UI console. Ceph in its mimic release has provided the users with a new and redesigned dashboard plugin, with the features like restricted control with username/password protection and SSL/TLS support.","title":"CEPH Dashboard"},{"location":"Getting-Started/#to-enable-the-dashboard-we-need-to-follow-these-steps","text":"1) ceph mgr module enable dashboard 2) ceph dashboard create-self-signed-cert Note: Self signed certificate is only for quick start purpose. 3) ceph mgr module disable dashboard 4) ceph mgr module enable dashboard Now, we will be able to see the CEPH dashboard on the port 8443, which is by default but on requirement can be configured using: ceph config set mgr mgr/dashboard/server_addr $IP ceph config set mgr mgr/dashboard/server_port $PORT To access, and to utilize full functionality of the dashboard, we need to create the login credentials. ceph dashboard set-login-credentials <username> <password> After these steps, our ceph infrastructure is ready with all the configurations to do some actual input output operations.","title":"To enable the dashboard, we need to follow these steps:"},{"location":"Getting-Started/#reference-link","text":"http://docs.ceph.com/docs/mimic/start/quick-start-preflight/","title":"Reference link:"},{"location":"Getting-Started/#65-steps-to-install-and-configuration-ldap","text":"ApacheDs Server installation and config Apache Directory Studio user guide","title":"6.5 Steps to Install and configuration LDAP"},{"location":"Getting-Started/#66-steps-to-install-and-configuration-hdfs","text":"NOTE: Required only if HDFS is used for packet storage. Refer - Steps-to-Install-and-configuration-HDFS","title":"6.6 Steps to Install and configuration HDFS"},{"location":"Getting-Started/#67-steps-to-deploy-kernel-key-manager-service","text":"Kernel Keymanager Service is setup outside of Kubernetes cluster on a standalone VM. The steps to setup kernel-keymanager-service are given here To deploy keymanager service, follow below steps - 1. Prerequiste: * A machine with RHEL 7.6 installed. * Docker installed and Docker service enabled. Steps to install Docker ce. $ sudo yum install http://mirror.centos.org/centos/7/extras/x86_64/Packages/container-selinux-2.107-3.el7.noarch.rpm $ sudo yum -y install lvm2 device-mapper device-mapper-persistent-data device-mapper-event device-mapper-libs device-mapper-event-libs $ sudo wget https://download.docker.com/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo $ sudo yum -y install docker-ce $ sudo systemctl start docker $ sudo systemctl status docker Open port 8088 on the VM: sudo firewall-cmd --zone=public --add-port=8088/tcp --permanent sudo firewall-cmd --reload Note: if firewall is not installed in VM, install with \u201csudo yum install firewall\u201d And also open the port. ensure that config server is already deployed. Process to deploy Services in VM through JenkinsFile: Refer the github url for Jenkinsfile : in root directory of kernel module The last stage in the Jenkinsfile viz 'Key-Manager Deployment' in which we are sshing into this newly created VM through Jenkins to deploy this service, basically, running the docker image of key manager. For ssh, place the public key of jenkins inside this newly created VM's authorized_keys under .ssh directory. Generate Docker Registry Credential in jenkins by using docker hub username and password. This will generate the credentialsId. Replace the value for registryCredentials(credentialsId of docker hub) with yours Replace the value for key_manager_vm_ip with IP of your newly created VM. Once done the following command will be used to deploy keymanager to the machine: sudo docker run -tid --ulimit memlock=-1 -p 8088:8088 -v softhsm:/softhsm -e spring_config_url_env=\"${config_url}\" -e spring_config_label_env=\"${branch}\" -e active_profile_env=\"${profile_env}\" --name keymanager \"${registryAddress}\"/kernel-keymanager-service **NOTE- Replace the values for spring_config_url_env, spring_config_label_env, active_profile_env and registryAddress in the above command accordingly","title":"6.7 Steps to Deploy Kernel Key Manager Service"},{"location":"Getting-Started/#68-sms-gateway-configuration","text":"Refer kernel-smsnotification-servive Readme here","title":"6.8 SMS Gateway configuration"},{"location":"Getting-Started/#69-installation-of-activemq","text":"ActiveMQ is the message broker used for MOSIP Registartion processor module.","title":"6.9 Installation of ActiveMQ"},{"location":"Getting-Started/#installation-steps","text":"<version> : please check http://www.apache.org/dist/activemq/ to find out the latest version. Tested ActiveMQ version - 5.4.1. Prerequiste: A machine with RHEL 7.6 installed, Docker installed and Docker service enabled. Download activemq using command - wget https://archive.apache.org/dist/activemq/5.14.3/apache-activemq-5.14.3-bin.tar.gz Extract the archive tar -zxvf apache-activemq-<version>-bin.tar.gz Change the permission for startup script chmod 755 apache-activemq-<version> Start activemq service cd apache-activemq-<version> && sudo ./bin/activemq start Check for the installed and started activemq on port 61616. netstat -tulpn Open ports 8161 and 61616 on the VM: sudo firewall-cmd --zone=public --add-port=8161/tcp --permanent sudo firewall-cmd --zone=public --add-port=61616/tcp --permanent sudo firewall-cmd --reload Note: After Installation of activemq, same needs to be mentioned in RegistrationProcessorAbis_{active_profile}.json For e.g : Suppose activemq is configured as tcp://xxx.xxx.xxx.xx:61616, then we for dev need to mention this in RegistrationProcessorAbis_dev.json as { \"abis\": [{ \"name\": \"ABIS1\", \"host\": \"\", \"port\": \"\", \"brokerUrl\": \"tcp://xxx.xxx.xxx.xx:61616\", \"inboundQueueName\": \"abis1-inbound-address_dev\", \"outboundQueueName\": \"abis1-outbound-address_dev\", \"pingInboundQueueName\": \"\", \"pingOutboundQueueName\": \"\", \"userName\": \"admin\", \"password\": \"admin\", \"typeOfQueue\": \"ACTIVEMQ\" } ] } ActiveMQ is also being used in registration-processor-printing-stage and the details need to be mentioned in registration-processor-{active_profile}.properties in the configuration repository. E.g : For dev profile, the property in registration-processor-dev.properties, the Property corresponding to printing-stage related to activemq would be Queue username registration.processor.queue.username={username} #Queue Password registration.processor.queue.password={password} #Queue Url registration.processor.queue.url={queue_url} #Type of the Queue registration.processor.queue.typeOfQueue=ACTIVEMQ #Print Service address registration.processor.queue.address={queue_address} #Post Service address registration.processor.queue.printpostaladdress={postal_queue_address}","title":"Installation steps"},{"location":"Getting-Started/#7-configuring-mosip","text":"","title":"7. Configuring MOSIP [\u2191]"},{"location":"Getting-Started/#mosip-database-object-deployment-configuration","text":"Database deployment consists of the following 4 categories of objects to be deployed on postgresql database. User / Roles: In MOSIP, the following user / roles are defined to perform various activities sysadmin: sysadmin user/role is a super administrator role, who will have all the privileges to performa any task within the database. dbadmin: dbadmin user / role is created to handle all the database administration activities db monitoring, performance tuning, backups, restore, replication setup, etc. appadmin: appadmin user / role is used to perform all the DDL (Data Definition Language) tasks. All the db objects that are created in these databases will be owned by appadmin user. Application User: Each application will have a user / role created to perform DML (Data Manipulation Language) tasks like CRUD operations (select, insert, update, delete). The user prereguser, is created to connect from the application to perform all the DML activities. Similarly, we will have masteruser, prereguser, reguser, idauser, idrepouser, kerneluser, audituser, regprcuser to perform DML tasks for master, pre-registration, registration, ida, ID repository, kernel, audit and registration processor modules respectively. Note: From the above set of roles only application user / role is specific to a application / module. The other user / roles are common which needs to be created per postresql db instance / server. Database and Schema: Each application / module of MOSIP platform will have a database and schema defined. All the objects (tables) related to an application / module would be created under the respective database / schema. In MOSIP the following database and scehmas are defined application / module name Database tool database Name schema name Master / Administration module postgresql mosip_master master Kernel postgresql mosip_kernel kernel Pre-registration postgresql mosip_prereg prereg Registration Apache Derby mosip_reg reg Registration Processor postgresql mosip_regprc regprc ID Authentication postgresql mosip_ida ida ID Repository postgresql mosip_idrepo idrepo Audit postgresql mosip_audit audit IAM postgresql mosip_iam iam idmap postgresql mosip_idmap idmap Note: These databases can be deployed on single or separate database servers / instances. DB Objects (Tables): All the tables of each application / module will be created in their respective database and schema. appadmin user / role will own these objects and the respective application user / role will have access to perform DML operations on these objects. Seed Data: MOSIP platform is designed to provide most of its features to be configured in the system. These configuration are deployed with default setup on config server and few in database. Few of these configuration can be modified / updated by the MOSIP administrator. These configuration include, system configurations, master datasetup, etc. The steps to add new center, machine / device is detailed in Guidelines for Adding Centers, Machine, Users and Devices The system configuration and master data is available under the respective application / database related folder. for example, the master data configuration is available in csv file format under folder . The scripts to create the above objects are available under db_scripts . To deploy the database objects of each application / module except registration client , please refer to README.MD file. These scripts will contain the deployment of all the DB object categories. Note: Please skip Registration client related deployment scripts (Apache derby DB specific) as this will be executed as part of registration client software installation.","title":"MOSIP database object deployment / configuration"},{"location":"Getting-Started/#setup-and-configure-mosip","text":"We are using kubernetes configuration server in MOSIP for storing and serving distributed configurations across all the applications and environments. We are storing all applications' configuration in config-templates folder inside our Github Repository here . For getting more details about how to use configuration server with our applications, following developer document can be referred: MOSIP CONFIGURATION SERVER For Deployment of configurations server, go to firstly-deploy-kernel-configuration-server in this document. Application specific configuration for all applications and services are placed in MOSIP config server. A. Global: link B. Kernel: link C. Pre-Registration: link D. Registartion-Processor: link E. IDA: link F. ID-REPO: link H. Registration: link Properties Sections that need to be changed in above module specific files once the external dependencies are installed as per your setup","title":"Setup and configure MOSIP"},{"location":"Getting-Started/#global","text":"","title":"Global"},{"location":"Getting-Started/#-common-properties-","text":"mosip.base.url","title":"--Common properties------------"},{"location":"Getting-Started/#-virus-scanner-","text":"mosip.kernel.virus-scanner.host mosip.kernel.virus-scanner.port","title":"--Virus Scanner-----------------"},{"location":"Getting-Started/#-fs-adapter-hdfs-","text":"mosip.kernel.fsadapter.hdfs.name-node-url # Enable if hadoop security authorization is 'true', default is false mosip.kernel.fsadapter.hdfs.authentication-enabled # If HDFS is security is configured with Kerberos, Key Distribution Center domain mosip.kernel.fsadapter.hdfs.kdc-domain #keytab file path, must be set if authentication-enable is true #read keytab file both classpath and physical path ,append appropriate prefix #for classpath prefix classpath:mosip.keytab #for physical path prefix file:/home/keys/mosip.keytab mosip.kernel.fsadapter.hdfs.keytab-file=classpath:mosip.keytab","title":"--FS Adapter-HDFS -------------"},{"location":"Getting-Started/#kernel","text":"","title":"Kernel"},{"location":"Getting-Started/#-kernel-common-properties-","text":"mosip.kernel.database.hostname mosip.kernel.database.port","title":"--kernel common properties-----------------------"},{"location":"Getting-Started/#-sms-notification-service-","text":"mosip.kernel.sms.authkey","title":"--sms notification service-----------------------"},{"location":"Getting-Started/#-email-notification-service-","text":"spring.mail.host spring.mail.username spring.mail.password spring.mail.port=587","title":"--Email Notification service---------------------"},{"location":"Getting-Started/#-ldap-","text":"ldap_1_DS.datastore.ipaddress ldap_1_DS.datastore.port","title":"--Ldap------------"},{"location":"Getting-Started/#-database-properties-","text":"**_database_password **_database_username","title":"--DataBase Properties----------------------------"},{"location":"Getting-Started/#8-mosip-deployment","text":"Currently for the Development Process MOSIP Platform is deployed as/in the Kubernetes Cluster. We are using Azure Kubernetes Service for provisioning of Cluster. As of now Kubernetes Deployment is deviced in two parts - A. One time setup of MOSIP in Kubernetes Cluster B. Continuous deployment","title":"8. MOSIP Deployment [\u2191]"},{"location":"Getting-Started/#a-one-time-setup-of-mosip-in-kubernetes-cluster","text":"One time setup on Kubernetes involves following Steps I. Setting Up local system to communicate with Kubernetes cluster this can be done via kubectl . II. Setting Up the Basic environment for MOSIP to run in Kubernetes Cluster, In this step we will work on this link . following are the files - We will now go through each of the file and see what changes we need to perform. we will be using kubectl to do the deployments from local system. DeployIngressController.yaml - We need not to change anything here. we can directly run this file. To run this use this command kubectl apply -f DeployIngressController.yaml DeployIngress.yaml - This file contains information about routing to different Kubernetes services, So whenever any traffic comes to our Load Balancer IP it will look for this file to route the request. For eg. Let's say if some.example.com is mapped to our kubernetes loadbalancer then if a request is for some.example.com/pre-registration-ui then this request will be redirect to pre-registration-ui on port 80 service. Routes referrring to ping-server and sample-nginx can be removed as these are for testing purpose.To run this use this command kubectl apply -f DeployIngress.yaml DeployServiceIngressService.yaml - We need not to change anything here. we can directly run this file. To run this use this command kubectl apply -f DeployServiceIngressService.yaml DeployDefaultBackend.yaml - We need not to change anything here. we can directly run this file. To run this use this command kubectl apply -f DeployDefaultBackend.yaml docker-registry-secret.yml - This file helps Kubernetes to get the Docker Images from Private Docker Registry. This file is a downloaded YAML of secrets that exists in the Kubernetes. You can either create secret or use this file to deploy secret in Kubernetes. For creating secret for the first time, run below command - kubectl create secret docker-registry <registry-credential-name> --docker-server=<your-registry-server> --docker-username=<your-name> --docker-password=<your-pword> --docker-email=<your-email> Once secret is created on the Kubernetes Cluster, as a backup strategy we can download the created secret using this command kubectl get secret <registry-credential-name> -o yaml --export Once the above deployment is done, we will start deploying MOSIP services. For doing this, we need to look for these directories -","title":"A. One time setup of MOSIP in Kubernetes Cluster"},{"location":"Getting-Started/#firstly-deploy-kernel-configuration-server","text":"The script is inside ( https://github.com/mosip/mosip-infra/blob/master/deployment/cloud/kubernetes/configuration-server/config-server-deployment-and-service.yml ) Follow below steps: 1. Create a ssh key and configure it with your git repository. If you have already configured the ssh key for your repository, you can use that one or else follow this 2. Create a secret for Config server to connect to GIT repo. This secret contains your id_rsa key (private key), id_rsa_pub key (public key) and known_hosts which you generated above. We need this secret because config server connects to your Source code management repository, to get configuration for all the services(If you are using ssh URL for cloning the repo). For generating the required secret give the following command: ( Firstly try to connect to GIT repository from your system using ssh url and the key you created above, so that GIT service provider such as GitHub or GitLab comes in your known hosts file): `kubectl create secret generic config-server-secret --from-file=id_rsa=/path/to/.ssh/id_rsa --from-file=id_rsa.pub=/path/to/.ssh/id_rsa.pub --from-file=known_hosts=/path/to/.ssh/known_hosts` <br/> **For Encryption Decryption of properties with configuration server** <br/> <br/> Create keystore with following command: `keytool -genkeypair -alias <your-alias> -keyalg RSA -keystore server.keystore -keypass < key-secret > -storepass < store-password > --dname \"CN=<your-CN>,OU=<OU>,O=<O>,L=<L>,S=<S>,C=<C>\"` The JKS keystore uses a proprietary format. It is recommended to migrate to PKCS12 which is an industry standard format, migrate it using following command: keytool -importkeystore -srckeystore server.keystore -destkeystore server.keystore -deststoretype pkcs12 For more information look here Create file with following content to create keystore secret for encryption decryption of keys using information from keystore created above: apiVersion: v1 kind: Secret metadata: name: config-server-keystore-values-secret type: Opaque data: alias: < base-64-encoded-alias-for keystore > password: < base-64-store-password > secret: < base-64-encoded-key-secret > 5. Save the above file with any name and apply it using: kubectl apply -f < file-name > Create server.keystore as secret to volume mount it inside container: kubectl create secret generic config-server-keystore --from-file=server.keystore=< location-of-your-server.keystore-file-generated-above > Change git_url_env environment variable in kernel-config-server-deployment-and-service.yml to your git ssh url of configuration repository Change git_config_folder_env environment variable in kernel-config-server-deployment-and-service.yml to your configuration folder in git repository. Change spec->template->spec->containers->image from docker-registry.mosip.io:5000/kernel-config-server to < Your Docker Registry >/kernel-config-server Change spec->template->spec->imagePullSecrets->name from pvt-reg-cred to < Your docker registry credentials secret > Once above configuration is done, execute kubectl apply -f kernel-config-server-deployment-and-service.yml More information can be found here","title":"Firstly Deploy Kernel Configuration server"},{"location":"Getting-Started/#deploy-other-components","text":"Inside each of the directory there is a file for each service of MOSIP that is exposed as Web API. We need to deploy these files to get these running. But before doing that we need to change Private Docker Registry Address and Docker Registry Secret, so that on deployment time Kubernetes can fetch docker images from correct source using correct credentials. For doing this, follow below steps (for eg. we will use https://github.com/mosip/mosip-infra/blob/master/deployment/cloud/kubernetes/kernel-deployment/kernel-auditmanager-service-deployment-and-service.yml, but you have to repeat the process for all such files) - I. Open a deployment file. II. Change spec->template->spec->containers->image from docker-registry.mosip.io:5000/kernel-auditmanager-service to <Your Docker Registry>/kernel-auditmanager-service III. Change spec->template->spec->imagePullSecrets->name from pvt-reg-cred to <Your docker registry credentials secret> IV. Change active_profile_env to whichever profile you want to activate and spring_config_label_env to the branch from which you want to pick up the configuration V. Save the file and Run kubectl apply -f kernel-auditmanager-service-deployment-and-service.yml After above process is completed, you can run kubectl get services command to see the status of all the MOSIP Services. For Pre-Registration-UI Pre-registration-ui uses a file config.json to configure URLs of backend, which have to be provided as config map in pre-registration-ui-deployment-and-service.yml. For creating the configmap follow below steps: 1. Edit the file scripts -> https://github.com/mosip/mosip-infra/blob/master/deployment/cloud/kubernetes/pre-registration-deployment/pre-registration-ui-configuration.yaml 2. Update https://dev.mosip.io/ value with url of proxy server which points to pre-registration services. (Note: While editing, be careful with escape sequence characters) 3. Execute command Kubectl apply -f pre-registration-ui-configuration.yaml","title":"Deploy other components: "},{"location":"Getting-Started/#81-registration-processor-dmz-services-deployment","text":"Registration Processor DMZ Services are setup externally(deployed in a separate VM). Firstly, update below files present in config folder in configuration repository, and replace the line <to uri=\"https://<dns name>/registrationprocessor/v1/uploader/securezone\" /> with the URL of packet uploader stage. 1. registration-processor-camel-routes-new-dmz-<env-name>.xml 2. registration-processor-camel-routes-update-dmz-<env-name>.xml 3. registration-processor-camel-routes-lost-dmz-<env-name>.xml 4. registration-processor-camel-routes-activate-dmz-<env-name>.xml 5. registration-processor-camel-routes-deactivate-dmz-<env-name>.xml 6. registration-processor-camel-routes-res_update-dmz-<env-name>.xml We are deploying DMZ services into another VM having docker installed. The steps to setup DMZ environment and services deployment: 1. Need to set Up VM with RHEL 7.6 2. Installing the Docker ce: $ sudo yum install http://mirror.centos.org/centos/7/extras/x86_64/Packages/container-selinux-2.107- 3.el7.noarch.rpm $ sudo yum -y install lvm2 device-mapper device-mapper-persistent-data device-mapper-event device-mapper-libs device-mapper-event-libs $ sudo wget https://download.docker.com/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo $ sudo yum -y install docker-ce $ sudo systemctl start docker $ sudo systemctl status docker Need to copy the Jenkins server public key(id_rsa.pub) inside this newly created VM's authorized_keys(because through jenkins job, we will ssh into new VM and deploy) After installing Docker Start the Docker Service command to start the Docker service systemctl start docker command to check Docker is running: systemctl status docker Open the port 8081, 8083 from the VM: Mosip uses port 8081 for registration-processor-packet-receiver-stage and 8083 for registration-processor-registration-status-service. The port ids need to be updated in ngnix configuration. sudo firewall-cmd --zone=public --add-port=8081/tcp --permanent sudo firewall-cmd --reload sudo firewall-cmd --zone=public --add-port=8083/tcp --permanent sudo firewall-cmd --reload Note: if firewall is not installed in VM, install with \u201csudo yum install firewall\u201d And also open the port from AZURE OR AWS or any cloud where the VM is launched. Process to deploy Services in VM through JenkinsFile: The last stage in the Jenkinsfile viz DMZ_Deployment in which we are sshing into this newly created VM through Jenkins to deploy these services, basically, running the docker images of registration processor. Changes to be made in this stage-> a. Replace the value for registryCredentials(credentialsId of docker hub) with yours. b. Replace the value for the variable -> dmz_reg_proc_dev_ip with the IP of your newly created VM. Refer the github url for Jenkinsfile : here Also, instead of following as described in 4th point to use Jenkinsfile, we can do it manually. Steps are -> a. Login into the DMZ VM. b. Perform docker hub login c. Execute the following commands docker run --restart always -it -d -p 8083:8083 -e active_profile_env=\"${profile_env}\" -e spring_config_label_env=\"${label_env}\" -e spring_config_url_env=\"${config_url}\" \"${registryAddress}\"/registration-processor-registration-status-service docker run --restart always -it -d --network host --privileged=true -v /home/ftp1/LANDING_ZONE:/home/ftp1/LANDING_ZONE -v /home/ftp1/ARCHIVE_PACKET_LOCATION:/home/ftp1/ARCHIVE_PACKET_LOCATION -e active_profile_env=\"${profile_env}\" -e spring_config_label_env=\"${label_env}\" -e spring_config_url_env=\"${config_url}\" \"${registryAddress}\"/registration-processor-packet-receiver-stage docker run --restart always -it -d --network host --privileged=true -e active_profile_env=\"${profile_env}\" -e spring_config_label_env=\"${label_env}\" -e spring_config_url_env=\"${config_url}\" -e zone_env=dmz \"${registryAddress}\"/registration-processor-common-camel-bridge Note - Please change the environmental variables(active_profile_env, spring_config_label_env, spring_config_url_env ,registryAddress) in the above commands accordingly whether you are executing manually in your new VM or through Jenkinsfile. Packet uploader stage in secure zone will fetch file from dmz to upload it into Distributed File System,to connect to dmz vm either we can login using username and password or using ppk file. If password value is available in config property name registration.processor.dmz.server.password then uploader will connect using username and password. otherwise it will login using ppk file available in config with property name registration.processor.vm.ppk. PPK generation command ssh-keygen -t rsa -b 4096 -f mykey.","title":"8.1  Registration-Processor DMZ services deployment"},{"location":"Getting-Started/#82-kernel-salt-generator","text":"Kernel Salt Generator Job is a one-time job which is run to populate salts to be used to hash and encrypt data. This generic job takes schema and table name as input, and generates and populates salts in the given schema and table. Salt Generator Deployment steps a. Login into the VM. Open the port 8092 from the VM: sudo firewall-cmd --zone=public --add-port=8092/tcp --permanent sudo firewall-cmd --reload And also open the port from AZURE OR AWS or any cloud where the VM is launched. b. Perform docker hub login c. Execute the following commands sequentially one after the other. Wait for the completion of previous command before executing next commands. 1. docker run -it -p 8092:8092 -e active_profile_env=\"${profile_env}\" -e spring_config_label_env=\"${label_env}\" -e spring_config_url_env=\"${config_url}\" -e spring_config_name_env=id-repository -e db_url=mosip.idrepo.identity.db.shard.url -e schema_name=idrepo -e table_name=uin_hash_salt \"${registryAddress}\"/id-repository-salt-generator 2. docker run -it -p 8092:8092 -e active_profile_env=\"${profile_env}\" -e spring_config_label_env=\"${label_env}\" -e spring_config_url_env=\"${config_url}\" -e spring_config_name_env=id-repository -e db_url=mosip.idrepo.identity.db.shard.url -e schema_name=idrepo -e table_name=uin_encrypt_salt \"${registryAddress}\"/id-repository-salt-generator 3. docker run -it -p 8092:8092 -e active_profile_env=\"${profile_env}\" -e spring_config_label_env=\"${label_env}\" -e spring_config_url_env=\"${config_url}\" -e spring_config_name_env=id-repository -e db_url=mosip.idrepo.vid.db.url -e schema_name=idmap -e table_name=uin_hash_salt \"${registryAddress}\"/id-repository-salt-generator 4. docker run -it -p 8092:8092 -e active_profile_env=\"${profile_env}\" -e spring_config_label_env=\"${label_env}\" -e spring_config_url_env=\"${config_url}\" -e spring_config_name_env=id-repository -e db_url=mosip.idrepo.vid.db.url -e schema_name=idmap -e table_name=uin_encrypt_salt \"${registryAddress}\"/id-repository-salt-generator 5. docker run -it -d -p 8092:8092 -e active_profile_env=\"${profile_env}\" -e spring_config_label_env=\"${label_env}\" -e spring_config_url_env=\"${config_url}\" -e spring_config_name_env=id-authentication -e schema_name=ida -e table_name=uin_hash_salt \"${registryAddress}\"/authentication-salt-generator 6. docker run -it -d -p 8092:8092 -e active_profile_env=\"${profile_env}\" -e spring_config_label_env=\"${label_env}\" -e spring_config_url_env=\"${config_url}\" -e spring_config_name_env=id-authentication -e schema_name=ida -e table_name=uin_encrypt_salt \"${registryAddress}\"/authentication-salt-generator Note - Please change the value for variables active_profile_env, spring_config_label_env, spring_config_url_env and registryAddress in the above four commands accordingly","title":"8.2 Kernel Salt Generator"},{"location":"Getting-Started/#83-first-user-registration-and-onboarding","text":"Refer to wiki for detailed procedure on First User Registration and Onboarding","title":"8.3 First User Registration and Onboarding"},{"location":"Infrastructure/","text":"","title":"Infrastructure Recommendations"},{"location":"MOSIP-Device-Service-Specification/","text":"Aug 2019 | Version: 0.9.2 Status: Draft Table of Contents Introduction & Background Glossary of Terms Device Specification Device Trust Device Service - Communication Interfaces 5.1. Device Discovery 5.2. Device Info 5.3 Capture 5.4 Device Stream 5.5 Device Registration Capture Device Server 6.1 Registration 6.2. De-Register 1. Management Server 1. Compliance 1. Introduction & Background Objective The objective of this specification document is to establish the technical and compliance standards/ protocols that are necessary for a biometric device to be used in MOSIP solutions. Target Audience This is a biometric device specification document and aims to help the biometric device manufactures, their developers, and their designers in building MOSIP compliant devices. This document assumes that the readers are familiar with MOSIP registration and authentication services. MOSIP Devices All devices that collect biometric data for MOSIP should operate within the specification of this document. 2. Glossary of Terms Device Provider - An entity that manufactures or imports the devices in their name. This entity should have legal rights to obtain an organization level digital certificate from the respective authority in the country. Foundational Trust Provider - An entity that manufactures the foundational trust module. Device - A hardware capable of capturing biometric information. L1 Certified Device / L1 Device - A device certified as capable of performing encryption in line with this spec in its trusted zone. L0 Certified Device / L0 Device - A device certified as one where the encryption is done on the host machine device driver or the MOSIP device service. Foundational Trust Provider Certificate - A digital certificate issued to the \u201cFoundational Trust Provider\u201d. This certificate proves that the provider has successfully gone through the required Foundational Trust Provider evaluation. The entity is expected to keep this certificate in secure possession in an HSM. All the individual trust certificates are issued using this certificate as the root. This certificate would be issued by the countries in conjunction with MOSIP. Device Provider Certificate - A digital certificate issued to the \u201cDevice Provider\u201d. This certificate proves that the provider has been certified for L0/L1 respective compliance. The entity is expected to keep this certificate in secure possession in an HSM. All the individual trust certificates are issued using this certificate as the root. This certificate is issued by the countries in conjunction with MOSIP. Registration - The process of applying for a Foundational Id. KYC - Know Your Customer. The process of providing consent to perform profile verification and update. Auth - The process of verifying one\u2019s identity. FPS - Frames Per Second Management Server - A server run by the device provider to manage the life cycle of the biometric devices. Device Registration - The process of registering the device with MOSIP servers. Signature - All signature are formated as per RFC 7515. header in signature - Header in signature means the attribute with \"type\" set to \"MDSSign\" \"alg\" set to RS256 and x5c set to base64encoded certificate. payload is the byte array of the actual data, always represented as base64urlencoded. signature - base64urlencoded signature bytes 3. Device Specification The MOSIP device specification provides compliance guidelines to devices for them to work with MOSIP. The compliance is based on device capability, trust and communication protocols. A MOSIP compliant device would follow the standards established in this document. It is expected that the devices are compliant to this specification and tested and validated. The details of each of these are outlined in the subsequent sections. Device Capability: The MOSIP compliant device is expected to perform the following: Should have the ability to collect one or more biometric Should have the ability to sign the captured biometric image or template. Should have the ability to protect secret keys Should have no mechanism to inject the biometric Base Specifications for Devices: 1. Fingerprint Capture Factor Registration Devices Authentication Devices Minimum Resolution > 500 native dpi. Bare minimum recommended. Higher densities are preferred > 500 native dpi. Bare minimum recommended. Higher densities are preferred Extractor Quality MINEX compliance Number of Minutiae generated by extractor to be in conformance to ISO Specification. Tested for at least 12 Minutiae points generated under test conditions MINEX compliance Number of Minutiae generated by extractor to be in conformance to ISO Specification. Tested for at least 12 Minutiae points generated under test conditions FRR < 2% FRR in respective country < 2% FRR in respective country FAR 0.01% 0.01% DPI 500 500 Image Specification ISO 19794-4 ISO 19794-4 and ISO 19794-2 ESD >= 8kv >= 8kv EMC compliance FCC class A or equivalent FCC class A or equivalent Operating Temperature 0 - 50 C -30 -to 50 C Liveness detection As per IEEE 2790 As per IEEE 2790 Preview > 3 FPS Jpeg lossless frames with NFIQ 2 score superimposed None Image Format JPEG 2000 lossless JPEG2000 lossless Quality Score NFIQ 2 NFIQ 2 * Sufficiency to be validated for registration ** MOSIP adapters can change this if needed *** MOSIP adapters to decide on the availability of this feature IRIS Capture Refer ISO 19796-6:2011 Part 6 Specifications. Factor Registration Devices Authentication Devices Rotation angle Before compression, the Iris image will have to be pre-processed to calculate rotation angle. Refer section 6.3.1 of ISO 19794-6 for rotation angle calculation for rectilinear images. Rotation Uncertainty Refer ISO 19794-6 Minimum Diameter As per ISO 19794-6:2011 medium and higher quality images are only acceptable,. Hence for this Standard, minimum acceptable Iris diameter will be 150 pixels Same Margin 50% left and right of Iris diameter 25% top and bottom of Iris diameter Color The iris images shall be captured and stored in grey scale with pixel depth of 8 bits/pixel. Illumination The eye should be illuminated using infrared or any other source that could produce high quality grayscale image. Image Format JPEG 2000 lossless JPEG 2000 lossless Aspect Ratio 1:1 Image Quality 76-100 IREXII - IQCE 76-100 IREXII - IQCE Operation Temperature* -30 C to +50 C -30 C to +50 C EMC compliance FCC Class A or equivalent FCC Class A or equivalent Preview > 3 FPS Jpeg lossless frames with quality score superimposed Not Applicable Image Specification ISO 19794-6 ISO 19794-6 ISO Format K3 K7 * MOSIP adopters to decide and finalize Face Capture Refer ISO 19794-5:2011 Factor Registration Devices Authentication Devices Minimum Resolution 1080 Pixels at 2.8 mm with 110 degree view 1080 Pixels at 2.8 mm Skin Tone All All Operation Temperature* -30 C to +50 C -30 C to +50 C EMC compliance FCC Class A or equivalent FCC Class A or equivalent Image Specification ISO/IEC 19794 - 5 ISO/IEC 19794 - 5 Image quality ICAO - Full frontal image, +/- 5 degrees rotation, 24 bit RGB, white background, 35 mm width, 45 mm height Image format JPEG 2000 lossless JPEG 2000 lossless We recommend that countries look at ergonomics, accessibility, ease of usage, and common availability of devices while choosing devices for use in registration and authentication scenarios. 4. Device Trust MOSIP compliant devices provide a trust environment for the devices to be used in registration, KYC and auth scenarios. The trust level is established based on the device support for trusted execution. Foundational Trust Module (FTM): The foundational trust module would be created using a secure microprocessor capable of performing all required biometric processing and secure storage of keys. The foundational device trust would satisfy the below requirements. 1. The module has the ability to securely generate, store and process cryptographic keys. 2. Generation of asymmetric keys and symmetric keys in random. 3. The module has the ability to protect keys from extraction. 4. The module has to protect the keys from physical tampering, temperature, frequency and voltage related attacks. 5. The module has the ability to perform a cryptographically validatable secure boot. 6. The module has the ability to run trusted applications. The foundational device trust derived from this module is used to enable trust-based computing for biometric capture. The foundational device trust module provides for a trusted execution environment based on the following. 1. Secure Boot 1. Ability to cryptographically verify code before execution. 2. Ability to check for integrity violation of the module/device 3. Halt upon failure. 4. Ability to securely upgrade and perform forward only upgrades, to thwart downgrade attacks. 1. Secure application 1. Ability to run applications that are trusted. 2. Protect against downgrading of applications. Foundational Trust Module Identity: The foundational module upon its first boot is expected to generate a random asymmetric key pair and provide the public part of the key to obtain a valid certificate. The entire certificate issuance would be in a secured provisioning facility. The certificate issued to the module will have a defined validity period as per the MOSIP certificate policy document defined by the MOSIP adopters. Device Identity: As MOSIP deals with biometrics it is imperative that all devices that connect to MOSIP are identifiable. MOSIP believes in cryptographic Identity as its basis for trust. Physical Id: An identification mark that shows MOSIP compliance and a readable unique device serial number (minimum of 12 digits), make and model. The same information has to be available over a 2D QR Code or Barcode. Digital Id: A digital device Id in MOSIP would be a signed JSON (RFC 7515) as follows: { \u201cserialNo\u201d: \"Serial number\", \u201cmake\u201d: \"Make of the device\", \u201cmodel\u201d : \"Model of the device\", \u201ctype\u201d: [\u201cFingerprint\u201d, \u201cIris\u201d, \u201cFace\"], //More types will be added. \u201csubType\u201d: \"subtypes of the biometric device\", \u201cdeviceProvider\u201d: \"Device provider name\", \u201cdeviceProviderId\u201d: \"Device provider Id\", \u201cdateTime\u201d: \"Datetime in ISO format with timezone. Identity request time\" } Signed with the JSON Web Signature (RFC 7515) using the \u201cFoundational Trust Module\u201d Identity key, this data is the fundamental identity of the device. Every MOSIP compliant device will need the foundational trust module. The only exception to this rule is for the devices that have the purpose (explained below during device registration) as \"Registration\". Those devices are called as L0 where there is not FTM. These devices would sign the request with device key. Signed Digital Id would look as follows. \"digitalId\": \"base64urlencoded(header).base64urlencoded(payload).base64urlencoded(signature)\" Unsigned digital Id would look as follows. \"digitalId\": \"base64urlencoded(payload)\" payload is the Digital ID json object. Accepted Values : serialNo - Same as the Physical Id make - Brand name model - Model of the device type - [\u201cFingerprint\u201d, \u201cIris\u201d, \u201cFace\u2019\u2019], //More types will be added. subType - subtype is based on the type. Finger - \u201cSlab\u201d, \u201cSingle\u201d, \u201cTouchless\u201d Iris - \u201cSingle\u201d, \u201cDouble\u201d, Face - Full face deviceProvider - Device provider name, This would be a legal entity in the country, deviceProviderId: Device provider Id issued by MOSIP dateTime: ISO format with timezone. Identity request time 5. Device Service - Communication Interfaces The section explains the necessary details of the biometric device connectivity, accessibility, discoverability and protocols used to build and communicate with the device. The device should implement only the following set of APIs. All the API\u2019s are independent of the physical layer and the operating system, with the invocation being different across operating systems. While the operating system names are defined in this spec a similar technology can be used for unspecified operating systems. It is expected that the device service ensures that the device is connected locally to the host. 5.1 Device Discovery: Specifications for Windows and Linux Device discovery would be used to identify MOSIP compliant devices in a system by the applications. The protocol is designed as simple plug and play with all the necessary abstraction to the specifics. Discovery Request: Request: { \u201ctype\u201d: \"type of the device\" } Accepted Values type: \u201cBiometric Device\u201d, \u201cFingerprint\u201d, \u201cFace\u201d, \u201cIris\u201d Note: \u201cBiometric Device\u201d - is a special type and used in case if you are looking for any biometric device. Response: [ { \u201cdeviceId\u201d: \"internal Id\", \u201cdeviceStatus\u201d: \"device status\", \u201ccertification\u201d: \"certification level\", \u201cserviceVersion\u201d: \"device service version\", \u201cdeviceSubId\u201d: \"device sub Id\u2019s\", \u201ccallbackId\u201d: \"baseurl to reach to the device\u201c, \"digitalId\": \"unsigned digital id of the device\", \"deviceCode\": \"A unique code given by MOSIP after successful registration\", \"specVersion\": [\"Array of supported MDS specification version\"], \"purpose\": \"Auth or Registration or empty if not registered\", \"error\": { \"errorcode\": \"101\", \"errorinfo\": \"Invalid JSON Value Type For Discovery.. } }, ... ] Accepted values: deviceStatus - \u201cActive\u201d, \u201cInactive or Not Registered\u201d certification - \u201cL0\u201d, \u201cL1\u201d - Level of certification serviceVersion - Version of the MDS specification that is supported. deviceId - Internal Id to identify the actual biometric device within the device service. deviceSubId - is the internal Id of the device. For example in case of iris capture, the device can have two modules in a single device, it is possible to address each device with a sub Id so we can identify or command each of it in isolation. Sub Id is a simple index which always starts with 1 and increases sequentially for each sub device present. callbackId - this differs as per the OS. In case of Linux and windows operating systems it is a http URL. In the case of android, it is the intent name. In IOS it is the URL scheme. The call back url takes precedence over future request as a base URL. digitalId - unsigned digital id as per the Digital Id definition. deviceCode: A unique code given by MOSIP after successful registration, specVersion - Array of supported MDS specification version\", purpose - Purpose of the device in the MOSIP ecosystem. errorCode - standardized error code. errorInfo - description of the error that can be displayed to end user. Multi lingual support. Note: The response is an array that we could have a single device enumerating with multiple biometric options. Note: The service should ensure to respond only if the type parameter matches the type of device or the type parameter is a \u201cBiometric Device\u201d. Windows/Linux: All the device API will be based on the HTTP specification. The device always binds to 127.0.0.1 with any of the available ports ranging from 4501 - 4600. The IP address used for binding has to be 127.0.0.1 and not localhost. The applications that require access to MOSIP devices could discover them by sending the http request to the supported port range. HTTP Request: MOSIPDISC http://127.0.0.1:<device_service_port>/device HOST: 127.0.0.1: <apps port> EXT: <app name> HTTP Response: HTTP/1.1 200 ok CACHE-CONTROL:no-store LOCATION:http://127.0.0.1:<device_service_port> Content-Length: length in bytes of the body Content-Type: application/json Connection: Closed Note: the pay loads are json in both the cases and are part of the body. *callbackId would be set to the http://127.0.0.1 : . So, the caller will use the respective verb and the url to call the service. Android: All devices on an android device should listen to the following intent io.mosip.device Upon invocation of this intent the devices are expected to respond back with the json response filtered by the respective type. *callbackId would be set to the appId. So, the caller will create the intent appId.Info or appId.Capture IOS: All device on an IOS device would respond to the url schema as follows. MOSIPDISC:// ?ext= &type= If a MOSIP compliant device service app exist then the url would launch the service. The service in return should respond back to the caller using the call-back-app-url with the base64 encoded json as the url parameter for the key data. Note: In IOS there are restrictions to have multiple apps registering to the same URL schema. *callbackId would be set to the device service appname. So, the caller has to call appnameInfo or appnameCapture as the url scheme. 5.2 Device Info: The device information API would be used to identify the MOSIP compliant devices and their status by the applications. Device Info Request: Request: NONE Accepted Values Response: [ { deviceInfo: { \u201cstatus\u201d: \"current status\", \u201cdeviceId\u201d: \"internal Id\", \u201cdeviceStatus\u201d: \"device status\", \u201cfirmware\u201d: \"firmware version\", \u201ccertification\u201d: \"certification level\", \u201cserviceVersion\u201d: \"device service version\", \u201cdeviceSubId\u201d: \"device sub Id\u2019s\", \u201ccallbackId\u201d: \"baseurl to reach to the device\u201c, \"digitalId\": \"signed digital id as described in the digital id section of this document\", \"deviceCode\": \"A unique code given by MOSIP after successful registration\", \"purpose\": \"Auth or Registration\", \"specVersion\": [\"Array of supported MDS specification version\"], }, \"error\": { \"errorcode\": \"101\", \"errorinfo\": \"Invalid JSON Value \" } } ... ] The final JSON is Signed with the JSON Web Signature using the \u201cFoundational Trust Module\u201d Identity key, this data is the fundamental identity of the device. Every MOSIP compliant device will need the foundational trust module. So the API would respond in the following format. [ { \"deviceInfo\": \"base64urlencode(header).base64urlencode(payload).base64urlencode(signature)\" \"error\": { \"errorcode\": \"100\", \"errorinfo\": \"Device not registered. In this case the device info will be only base64urlencode(payload)\" } } ] Allowed values: deviceInfo.status - \u201cReady\u201d, \u201cBusy\u201d, \u201cNot Ready\u201d, \"Not Registered\" deviceInfo.deviceId - Internal Id to identify the actual biometric device within the device service. deviceInfo.deviceStatus - \"Active\" or \"InActive\" deviceInfo.firmware - Exact version of the firmware deviceInfo.certification - \u201cL0\u201d, \u201cL1\u201d - Level of certification deviceInfo.serviceVersion - Version of the current document. deviceInfo.biometric device within the device service. deviceInfo.deviceSubId - is the internal id of the device. In case of iris when we have two iris capture modules in a single device, it is possible to address each device with a sub Id so we can identify or command each of it in isolation. This in an index that always starts with 1 and increments sequentially. deviceInfo.callbackId - base URL to communicate deviceInfo.digitalId - as defined under the digital id section for unsigned digital id. deviceInfo.purpose - \"Auth\" or \"Registration\" or empty in case the status is \"Not Registered\" deviceInfo.specVersion: \"Array of MDS specification version\", error - relevant errors as defined under the \"Error section\" of this document Note : The response is an array that we could have a single device enumerating with multiple biometric options. Note : The service should ensure to respond only if the type parameter matches the type of device or the type parameter is a \u201cBiometric Device\u201d. Windows/Linux: The applications that require more details of the MOSIP devices could get them by sending the http request to the supported port range. HTTP Request: MOSIPDINFO http://127.0.0.1:4501/info HOST: 127.0.0.1: <apps port> EXT: <app name> HTTP Response: HTTP/1.1 200 ok CACHE-CONTROL:no-store LOCATION:http://127.0.0.1:<device_service_port> Content-Length: length in bytes of the body Content-Type: application/json Connection: Closed Note: the pay loads are json in both the cases and are part of the body. Android: On an android device should listen to the following intent appId.Info Upon invocation of this intent the devices are expected to respond back with the json response filtered by the respective type. IOS: On an IOS device would respond to the url schema as follows. APPIDINFO:// ?ext= &type= If a MOSIP compliant device service app exist then the url would launch the service. The service in return should respond back to the called using the call-back-app-url with the base64 encoded json as the url parameter for the key data. Note: In IOS there are restrictions to have multiple app registering to the same URL schema. 5.3 Capture: The capture request would be used to capture a biometric from MOSIP compliant devices by the applications. The capture call will respond with success to only one call at a time. So in case of a parallel call the device info details are sent with status as \u201cBusy\u201d Capture Request: Request: { \u201cenv\u201d: \"target environment\", \"purpose\": \"Auth or Registration\", \"specVersion\": \"expected version of the biometric element\", \"timeout\" : <timeout for capture>, \u201ccaptureTime\u201d: <time of capture request in ISO format including timezone>, \"domainUri\": <uri of the auth server>, \u201ctransactionId\u201d: <transaction Id for the current capture>, \u201cbio\u201d: [ { \u201ctype\u201d: <type of the biometric data>, \u201ccount\u201d: <fingerprint/Iris count, in case of face max is set to 1>, \u201crequestedScore\u201d: <expected quality score that should match to complete a successful capture>, \u201cdeviceId\u201d: <internal Id>, \u201cdeviceSubId\u201d: <specific device Id>, \u201cpreviousHash\u201d: <hash of the previous block> } ], customOpts: { //max of 50 key value pair. This is so that vendor specific parameters can be sent if necessary. The values cannot be hardcoded and have to be configured by the apps server and should be modifiable upon need by the applications. Vendors are free to include additional parameters and fine-tuning parameters. None of these values should go undocumented by the vendor. No sensitive data should be available in the customOpts. } } Allowed Values: env - Allowed values are Staging| Developer| Pre-Production | Production purpose - Allowed values are Auth| Registration version - version of the biometric block as specified in the authentication or customer registration specification. timeout - Max time the app will wait for the capture. Its expected that the api will respond back before timeout with the best frame. All timeouts are in milliseconds captureTime - time of capture in ISO format with timezone. domainUri - unique uri per auth providers. This can be used to federate across multiple providers or countries or unions. transactionId - unqiue Id of the transaction. This is an internal Id to the application thats providing the service. Use different id for every successfull auth. So even if the trnsaction fails after auth we expect this number to be unique. bio.type - \u201cFIR\u201d , \u201cIIR\u201d, \u201cFace\u201d bio.count - number of biometric data that is collected for a given type. The device should validate and ensure that this number is in line with the type of biometric that's captured. bio.requestedScore - what is the expected quality? Upon reaching the necessary quality the biometric device is expected to auto capture the image. bio.deviceId - a unique Id per device service. In case a single device handles both face and iris the device Id will identify iris and camera uniquely. In case the Id is sent as 0 then the device is expected to capture biometric from both the devices. bio.deviceSubId - a specific device sub Id. Should be set to 0 if we don't know any specific device sub Id. bio.previousHash - For the first capture the previousHash is hash of empty utf8 string. From the second capture the previous captures hash (as hex encoded) is used as input. This is used to chain all the captures across modalities so all captures have happened for the same transaction and during the same time period. customOpts - If in case the device vendor has additional parameters that they can take and act accordingly then those values can be sent by the application developers to the device service. Response: [ { \"specVersion\" : \"MDS spec version\", \"data\": { \"digitalId\" : \"unsigned digital Id as described in this document\", \"deviceCode\": \"A unique code given by MOSIP after successfull registration\", \"deviceServiceVersion\": \"Service version\", \"bioType\": \"FIR\", \"bioSubType\": \"UNKNOWN\", \"purpose\": \"Auth or Registration\", \"env\": \"target environment\", \"domainUri\": \"uri of the auth server\", \"bioValue\": \"encrypted with session key and base64urlencoded biometric data\", \"transactionId\": \"unique transaction id\", \"timestamp\": \"ISO format datetime with time zone\", \"requestedScore\": \"floating point number to represent the minimum required score for the capture\", \"qualityScore\": \"floating point number representing the score for the current capture\" }, \"hash\": \"sha256(sha256 hash in hex format of the previous data block + sha256 hash in hex format of the current data block before encryption)\", \"sessionKey\": \"encrypted with MOSIP public key (dynamically selected based on the uri) and encoded session key biometric\", \"error\": { \"errorcode\": \"101\", \"errorinfo\": \"Invalid JSON Value\" } }, { \"specVersion\" : \"MDS spec version\", \"data\": { \"digitalId\": \"unsigned digital Id as described in this document\", \"deviceCode\": \"A unique code given by MOSIP after successfull registration\", \"deviceServiceVersion\": \"Service version\", \"bioType\": \"FIR\", \"bioSubType\": \"LEFT\", \"purpose\": \"Auth or Registration\", \"env\": \"target environment\", \"domainUri\": \"uri of the auth server\", \"bioValue\": \"encrypted with session key and base64urlencoded biometric data\", \"transactionId\": \"unique transaction id\", \"timestamp\": \"ISO Format date time with timezone\" }, \"hash\": \"sha256(sha256 hash in hex format of the previous data block + sha256 hash in hex format of the current data block before encryption)\", \"sessionKey\": \"encrypted with MOSIP public key and encoded session key biometric\", \"error\": { \"errorcode\": \"101\", \"errorinfo\": \"Invalid JSON Value\" } } ] Accepted values: data.bioValue - Encrypted and Encoded to base64urlencode biometric value. AES GCM encryption with a random key. The IV for the encryption is set to last 16 digits of the timestamp. ISO formated bioValue. Look at the Authentication document to understand more about the encryption. data - The entire data object is stored as follows. \"data\" : \"base64urlencode(header).base64urlencode(payload).base64urlencode(signature) payload is defined as the entire byte array of data block. The data block hash - the value of the previousHash atribute in the request object or the value of hash atribute of the previous data block (used to chain every single data block) concatenated with the hex encode sha256 hash of the current data block before encryption. sessionKey - Random AES key used for the encryption of the bioValue. The encryption key is encrypted using the public key with RSA OAEP. Sent as base64urlencoded Windows/Linux: The applications that requires to capture biometric data from a MOSIP devices could do so by sending the http request to the supported port range. HTTP Request: CAPTURE http://127.0.0.1 : /capture HOST: 127.0.0.1: EXT: HTTP Response: HTTP/1.1 200 ok CACHE-CONTROL:no-store LOCATION:[http://127.0.0.1](http://127.0.0.1):<device service port> Content-Length: length in bytes of the body Content-Type: application/json Connection: Closed Note: the pay loads are json in both the cases and are part of the body. Android: All device on an android device should listen to the following intent appid.capture Upon this intend the devices are expected to respond back with the json response filtered by the respective type. IOS: All device on an IOS device would respond to the url schema as follows. APPIDCAPTURE:// ?ext= &type= If a MOSIP compliant device service app exist then the url would launch the service. The service in return should respond back to the called using the call-back-app-url with the base64 encoded json as the url parameter for the key data. 5.4 Device Stream The device would open a stream channel to send the live video streams. This would help when there is an assisted operation to collect biometric. Please note the stream API\u2019s are available only for registration environment. Device Stream Request: Used only for the registration module compatible devices. This api is visible only for the devices that are registered for the purpose as \"Registration\". Request: { \u201cdeviceId\u201d: \"internal Id\", \u201cdeviceSubId\u201d: \"device sub Id\u2019s\", } Accepted Values deviceId - Internal Id deviceSubId - The sub id of the device thats responsoible to stream the data. Response: Live Video stream with quality of 3 frames per second or more using M-JPEG2000 https://en.wikipedia.org/wiki/Motion_JPEG Note: Preview should have the quality markings and segement marking. The preview would also be used to display any error message to the user screen. All error messages should be localizable. Accepted values: Windows/Linux: The applications that require more details of the MOSIP devices could get them by sending the http request to the supported port range. HTTP Request: STREAM http://127.0.0.1:4501/stream HOST: 127.0.0.1: EXT: HTTP Response: HTTP Chunk of frames to be displayed. Minimum frames 3 per second Android: No support for streaming IOS: No support for streaming 5.5 Device Registration Capture: The registration client application will send sample API. The sample API\u2019s response will provide the actual biometric data in a digitally signed non encrypted form. When the Device Registration Capture API is called the frames should not be added to the stream. The device is expected to send the images as well as its extraction values. For e.g. the segmented JPEG image is in the bioValue and the segmented and extracted will fit into the bioExtract. The requestedScore is in the scale of 1-100. In case the requestedScore is for all the count as average. So, in cases where you have four fingers the average of all will be considered for capture threshold. Device Registration Capture Request: The API is used by the devices that are compatible for the registration module. Request: { \u201cenv\u201d: \"target environment\", \"specVersion\": \"expected MDS spec version\", \"timeout\": \"timeout for registration capture\", \u201ccaptureTime\u201d: \"time of capture request in ISO format including timezone\", \u201cregistrationId\u201d: \"registration Id for the current capture\", \u201cbio\u201d: [ { \u201ctype\u201d: \"type of the biometric data\", \u201ccount\u201d: \"fingerprint/Iris count, in case of face max is set to 1\", \u201cexception\u201d: [\"finger or iris to be excluded\"], \u201crequestedScore\u201d: \"expected quality score that should match to complete a successful capture.\", \u201cdeviceId\u201d: \"internal Id\", \u201cdeviceSubId\u201d: \"specific device Id\", \u201cpreviousHash\u201d: \"hash of the previous block\" } ], customOpts: { //max of 50 key value pair. This is so that vendor specific parameters can be sent if necessary. The values cannot be hardcoded and have to be configured by the apps server and should be modifiable upon need by the applications. Vendors are free to include additional parameters and fine-tuning parameters. None of these values should go undocumented by the vendor. No sensitive data should be available in the customOpts. } } Accepted Values env - Allowed values are Staging| Developer| Pre-Production | Production version - version of the biometric block as specified in the registration specification. timeout - Max time the app will wait for the capture. captureTime - time of capture in ISO format with timezone. bio.type - \u201cFMR\u201d, \u201cFIR\u201d , \u201cIIR\u201d, \u201cFace\u201d bio.count - number of biometric data that is collected for a given type. The device should validate and ensure this number is in line with the type of biometric that's captured. bio.exception: \u201cLF_INDEX\u201d, \u201cLF_MIDDLE\u201d, \u201cLF_RING\u201d, \u201cLF_LITTLE\u201d, \u201cLF_THUMB\u201d \u201cRF_INDEX\u201d, \u201cRF_MIDDLE\u201d, \u201cRF_RING\u201d, \u201cRF_LITTLE\u201d, \u201cRF_THUMB\u201d, \u201cL_IRIS\u201d, \u201cR_IRIS\u201d. This is an array and all the exceptions are marked. In case of an empty element assume there is no exception. bio.requestedScore - what is the expected quality? Upon reaching the necessary quality the biometric device is expected to auto capture the image. bio.deviceId - a unique Id per device service. In case a single device handles both face and iris the device Id will identify iris and camera uniquely. In case the Id is sent as 0 then the device is expected to capture biometric from both the devices. bio.deviceSubId - a specific device sub Id. Should be set to 0 if we don't know any specific device sub Id. In case of Fingerprint/IRIS its 1 for left and 2 for right fingerprint/iris. 3 for thumb/two iris. bio.previousHash - The previous hash for the image captured by this device per registration. For the first capture (per registration) use the hash of empty string. Response: {\"biometrics\": [ { \"specVersion\" : \"MDS Spec version\", \"data\": { \"digitalId\": \"Unsigned digital id of the device as per the Digital Id definition..\", \"deviceCode\": \"A unique code given by MOSIP after successfull registration\", \"deviceServiceVersion\": \"\", \"bioSubType\": \"Middle Finger\", \"purpose\": \"Auth or Registration\", \"env\": \"target environment\", \"bioValue\": \"<base64urlencoded biometric data (raw image)>\", \"bioExtract\": \"<base64urlencoded extracted biometric (ISO format)\", \"registrationId\": \"1234567890\", \"timestamp\": \"2019-02-15T10:01:57.086+05:30\", \"requestedScore\": \"<floating point number to represent the minimum required score for the capture. This ranges from 0-100>\", \"qualityScore\": \"<floating point number representing the score for the current capture. This ranges from 0-100>\" }, \"hash\": \"sha256(sha256 hash in hex format of the previous data block + sha256 hash in hex format of the current data block)\", \"error\": { \"errorcode\": \"101\", \"errorinfo\": \"Invalid JSON Value Type For Discovery.. ex: {type: \u201cBiometric Device\u201d or \u201cFingerprint\u201d or \u201cFace\u201d or \u201cIris\u201d or \u201cVein\u201d} \" } }, { \"specVersion\" : \"MDS Spec version\", \"data\": { \"deviceCode\": \"\", \"digitalId\": \"Unsigned digital id of the device as per the Digital Id definition.\", \"deviceServiceVersion\": \"\", \"bioSubType\": \"LEFT\", \"purpose\": \"Auth or Registration\", \"env\": \"<target environment>\", \"bioValue\": \"<base64urlencoded biometric data (raw image)>\", \"bioExtract\": \"<base64urlencoded extracted biometric (ISO format)\", \"registrationId\": \"1234567890\", \"timestamp\": \"2019-02-15T10:01:57.086+05:30\" }, \"hash\": \"sha256(sha256 hash in hex format of the previous data block + sha256 hash in hex format of the current data block before encryption)\", \"error\": { \"errorcode\": \"101\", \"errorinfo\": \"Invalid JSON Value Type For Discovery.. ex: {type: \u201cBiometric Device\u201d or \u201cFingerprint\u201d or \u201cFace\u201d or \u201cIris\u201d or \u201cVein\u201d} \" } } ] } Accepted values: data - base64urlencode(header).base64urlencode(payload).base64urlencode(signature) Windows/Linux: The applications that require more details of the MOSIP devices could get them by sending the http request to the supported port range. HTTP Request: RCAPTURE http://127.0.0.1:4501/capture HOST: 127.0.0.1: EXT: HTTP Response: HTTP response. Android: No support for Registration Capture IOS: No support for Registration Capture 6. Device Server The device server exposes two external device APIs to manage devices. These will be consumed from Management Server created by the device provider. Refer to the subsequent section in this document. 6.1 Registration: The MOSIP server would provide the following device registration API which is whitelisted to the management servers of the device provider or their partners. Note: This API is exposed by the MOSIP server to the device providers. http://device.mosip.io /device/register HTTP Request Type: POST { \u201cdeviceData\u201d: { \u201cdeviceId\u201d: \"unique Id to identify a biometric capture device\", \"purpose\": \"Auth or Registration. Can not be empty\", \u201cdeviceInfo\u201d: { \u201cdeviceSubId\u201d: \"an array of sub Ids that are available\", \u201ccertification\u201d: \"certification level\", \"digitalId\": \"signed digital id of the device\", \u201cfirmware\u201d: \"firmware version\", \u201cdeviceExpiry\u201d: \"device expiry date\", \u201ctimestamp\u201d: \"ISO format datetime with timezone\" }, \u201cfoundationalTrustProviderId\u201d : \"foundation trust provider Id, in case of L0 this is empty\" } } Accepted Values: device data is sent in the following format. \"deviceData\" : base64urlencode(header).base64urlencode(payload).base64urlencode(signature) payload is the object in deviceData. Response: { \"response\": { \u201cstatus\u201d: \"registration status\", \u201cerror\u201d: { \"code\": \"error code if registration fails\", \"message\": \"description of the error code\", } \"deviceCode\": \"UUID RFC4122 Version 4 for the device issued by the mosip server\", \"timestamp\": \"timestamp in ISO format\", \"env\": \"prod/development/stage\" } } The response is of the following format \"response\" : base64urlencode(header).base64urlencode(payload).base64urlencode(signature) The response should be sent to the device. The device is expected to store the deviceCode within its storage in a safe manner. This device code is used during the capture stage. Note : The device once registered for a specific purpose can not be changed after successfull registration. The device can only be used for that specific mosip process. 6.2 De-Register: http://device.mosip.io /device/deregister Request: { device: { \u201cdeviceCode\u201d: <device code>, \u201cenv\u201d: <environment> \u201ctimestamp\u201d: <iso format time of successful registration of the device> } \u201csignature\u201d: \u201csignature of the device element using the device provider certificate\u201d } 7. Management Server The management server has the following objectives. 1. Validate the devices to ensure its a genuine device from the respective device provider. This can be achieved using the device info and the certificates for the Foundational Trust Module. 1. Register the genuine device with the MOSIP device server. 1. Manage/Sync time between the end device the server. The time to be synced should be the only trusted time accepted by the device. 1. Ability to issue commands to the end device for 1. Key rotation of the end device 1. De-registration of the device 1. Collect device info 1. A central repository of all the approved devices from the device provider. 1. Safe storage of keys using HSM FIPS 140-2 Level 3. These keys are used to issue the device certificate upon registration. The Management Server is created and hosted by the device provider outside of MOSIP software. The communication protocols between the MDS and the Management Server can be decided by the respective device provider. Such communication should be restricted to the above specified interactions only. No transactional information should be sent to this server. 8. Compliance L1 Certified Device / L1 Device - A device certified as capable of performing encryption on the device inside its trusted zone. L0 Certified Device / L0 Device - A device certified as one where the encryption is done on the host inside its device driver or the MOSIP device service. API Compatible Device Discovery L0/L1 Device Info L0/L1 Capture L1 Registration Capture L0/L1 9. Error Codes 0 Success 100 Device not registered 101 Unable to detect a biometric object 102 Technical error during extraction. 103 Device tamper detected 104 Unable to connect to management server 105 Image orientation error 106 Device not found 107 Device public key expired 108 Domain public key missing 5xx Custom errors. The device provider is free to choose his error code and error messages.","title":"MOSIP Device Service Specification"},{"location":"MOSIP-Device-Service-Specification/#aug-2019-version-092","text":"","title":"Aug 2019 | Version: 0.9.2"},{"location":"MOSIP-Device-Service-Specification/#status-draft","text":"","title":"Status: Draft"},{"location":"MOSIP-Device-Service-Specification/#table-of-contents","text":"Introduction & Background Glossary of Terms Device Specification Device Trust Device Service - Communication Interfaces 5.1. Device Discovery 5.2. Device Info 5.3 Capture 5.4 Device Stream 5.5 Device Registration Capture Device Server 6.1 Registration 6.2. De-Register 1. Management Server 1. Compliance","title":"Table of Contents"},{"location":"MOSIP-Device-Service-Specification/#1-introduction-background","text":"","title":"1. Introduction &amp; Background"},{"location":"MOSIP-Device-Service-Specification/#objective","text":"The objective of this specification document is to establish the technical and compliance standards/ protocols that are necessary for a biometric device to be used in MOSIP solutions.","title":"Objective"},{"location":"MOSIP-Device-Service-Specification/#target-audience","text":"This is a biometric device specification document and aims to help the biometric device manufactures, their developers, and their designers in building MOSIP compliant devices. This document assumes that the readers are familiar with MOSIP registration and authentication services.","title":"Target Audience"},{"location":"MOSIP-Device-Service-Specification/#mosip-devices","text":"All devices that collect biometric data for MOSIP should operate within the specification of this document.","title":"MOSIP Devices"},{"location":"MOSIP-Device-Service-Specification/#2-glossary-of-terms","text":"Device Provider - An entity that manufactures or imports the devices in their name. This entity should have legal rights to obtain an organization level digital certificate from the respective authority in the country. Foundational Trust Provider - An entity that manufactures the foundational trust module. Device - A hardware capable of capturing biometric information. L1 Certified Device / L1 Device - A device certified as capable of performing encryption in line with this spec in its trusted zone. L0 Certified Device / L0 Device - A device certified as one where the encryption is done on the host machine device driver or the MOSIP device service. Foundational Trust Provider Certificate - A digital certificate issued to the \u201cFoundational Trust Provider\u201d. This certificate proves that the provider has successfully gone through the required Foundational Trust Provider evaluation. The entity is expected to keep this certificate in secure possession in an HSM. All the individual trust certificates are issued using this certificate as the root. This certificate would be issued by the countries in conjunction with MOSIP. Device Provider Certificate - A digital certificate issued to the \u201cDevice Provider\u201d. This certificate proves that the provider has been certified for L0/L1 respective compliance. The entity is expected to keep this certificate in secure possession in an HSM. All the individual trust certificates are issued using this certificate as the root. This certificate is issued by the countries in conjunction with MOSIP. Registration - The process of applying for a Foundational Id. KYC - Know Your Customer. The process of providing consent to perform profile verification and update. Auth - The process of verifying one\u2019s identity. FPS - Frames Per Second Management Server - A server run by the device provider to manage the life cycle of the biometric devices. Device Registration - The process of registering the device with MOSIP servers. Signature - All signature are formated as per RFC 7515. header in signature - Header in signature means the attribute with \"type\" set to \"MDSSign\" \"alg\" set to RS256 and x5c set to base64encoded certificate. payload is the byte array of the actual data, always represented as base64urlencoded. signature - base64urlencoded signature bytes","title":"2. Glossary of Terms"},{"location":"MOSIP-Device-Service-Specification/#3-device-specification","text":"The MOSIP device specification provides compliance guidelines to devices for them to work with MOSIP. The compliance is based on device capability, trust and communication protocols. A MOSIP compliant device would follow the standards established in this document. It is expected that the devices are compliant to this specification and tested and validated. The details of each of these are outlined in the subsequent sections. Device Capability: The MOSIP compliant device is expected to perform the following: Should have the ability to collect one or more biometric Should have the ability to sign the captured biometric image or template. Should have the ability to protect secret keys Should have no mechanism to inject the biometric Base Specifications for Devices: 1. Fingerprint Capture Factor Registration Devices Authentication Devices Minimum Resolution > 500 native dpi. Bare minimum recommended. Higher densities are preferred > 500 native dpi. Bare minimum recommended. Higher densities are preferred Extractor Quality MINEX compliance Number of Minutiae generated by extractor to be in conformance to ISO Specification. Tested for at least 12 Minutiae points generated under test conditions MINEX compliance Number of Minutiae generated by extractor to be in conformance to ISO Specification. Tested for at least 12 Minutiae points generated under test conditions FRR < 2% FRR in respective country < 2% FRR in respective country FAR 0.01% 0.01% DPI 500 500 Image Specification ISO 19794-4 ISO 19794-4 and ISO 19794-2 ESD >= 8kv >= 8kv EMC compliance FCC class A or equivalent FCC class A or equivalent Operating Temperature 0 - 50 C -30 -to 50 C Liveness detection As per IEEE 2790 As per IEEE 2790 Preview > 3 FPS Jpeg lossless frames with NFIQ 2 score superimposed None Image Format JPEG 2000 lossless JPEG2000 lossless Quality Score NFIQ 2 NFIQ 2 * Sufficiency to be validated for registration ** MOSIP adapters can change this if needed *** MOSIP adapters to decide on the availability of this feature IRIS Capture Refer ISO 19796-6:2011 Part 6 Specifications. Factor Registration Devices Authentication Devices Rotation angle Before compression, the Iris image will have to be pre-processed to calculate rotation angle. Refer section 6.3.1 of ISO 19794-6 for rotation angle calculation for rectilinear images. Rotation Uncertainty Refer ISO 19794-6 Minimum Diameter As per ISO 19794-6:2011 medium and higher quality images are only acceptable,. Hence for this Standard, minimum acceptable Iris diameter will be 150 pixels Same Margin 50% left and right of Iris diameter 25% top and bottom of Iris diameter Color The iris images shall be captured and stored in grey scale with pixel depth of 8 bits/pixel. Illumination The eye should be illuminated using infrared or any other source that could produce high quality grayscale image. Image Format JPEG 2000 lossless JPEG 2000 lossless Aspect Ratio 1:1 Image Quality 76-100 IREXII - IQCE 76-100 IREXII - IQCE Operation Temperature* -30 C to +50 C -30 C to +50 C EMC compliance FCC Class A or equivalent FCC Class A or equivalent Preview > 3 FPS Jpeg lossless frames with quality score superimposed Not Applicable Image Specification ISO 19794-6 ISO 19794-6 ISO Format K3 K7 * MOSIP adopters to decide and finalize Face Capture Refer ISO 19794-5:2011 Factor Registration Devices Authentication Devices Minimum Resolution 1080 Pixels at 2.8 mm with 110 degree view 1080 Pixels at 2.8 mm Skin Tone All All Operation Temperature* -30 C to +50 C -30 C to +50 C EMC compliance FCC Class A or equivalent FCC Class A or equivalent Image Specification ISO/IEC 19794 - 5 ISO/IEC 19794 - 5 Image quality ICAO - Full frontal image, +/- 5 degrees rotation, 24 bit RGB, white background, 35 mm width, 45 mm height Image format JPEG 2000 lossless JPEG 2000 lossless We recommend that countries look at ergonomics, accessibility, ease of usage, and common availability of devices while choosing devices for use in registration and authentication scenarios.","title":"3. Device Specification"},{"location":"MOSIP-Device-Service-Specification/#4-device-trust","text":"MOSIP compliant devices provide a trust environment for the devices to be used in registration, KYC and auth scenarios. The trust level is established based on the device support for trusted execution.","title":"4. Device Trust"},{"location":"MOSIP-Device-Service-Specification/#foundational-trust-module-ftm","text":"The foundational trust module would be created using a secure microprocessor capable of performing all required biometric processing and secure storage of keys. The foundational device trust would satisfy the below requirements. 1. The module has the ability to securely generate, store and process cryptographic keys. 2. Generation of asymmetric keys and symmetric keys in random. 3. The module has the ability to protect keys from extraction. 4. The module has to protect the keys from physical tampering, temperature, frequency and voltage related attacks. 5. The module has the ability to perform a cryptographically validatable secure boot. 6. The module has the ability to run trusted applications. The foundational device trust derived from this module is used to enable trust-based computing for biometric capture. The foundational device trust module provides for a trusted execution environment based on the following. 1. Secure Boot 1. Ability to cryptographically verify code before execution. 2. Ability to check for integrity violation of the module/device 3. Halt upon failure. 4. Ability to securely upgrade and perform forward only upgrades, to thwart downgrade attacks. 1. Secure application 1. Ability to run applications that are trusted. 2. Protect against downgrading of applications. Foundational Trust Module Identity: The foundational module upon its first boot is expected to generate a random asymmetric key pair and provide the public part of the key to obtain a valid certificate. The entire certificate issuance would be in a secured provisioning facility. The certificate issued to the module will have a defined validity period as per the MOSIP certificate policy document defined by the MOSIP adopters. Device Identity: As MOSIP deals with biometrics it is imperative that all devices that connect to MOSIP are identifiable. MOSIP believes in cryptographic Identity as its basis for trust. Physical Id: An identification mark that shows MOSIP compliance and a readable unique device serial number (minimum of 12 digits), make and model. The same information has to be available over a 2D QR Code or Barcode. Digital Id: A digital device Id in MOSIP would be a signed JSON (RFC 7515) as follows: { \u201cserialNo\u201d: \"Serial number\", \u201cmake\u201d: \"Make of the device\", \u201cmodel\u201d : \"Model of the device\", \u201ctype\u201d: [\u201cFingerprint\u201d, \u201cIris\u201d, \u201cFace\"], //More types will be added. \u201csubType\u201d: \"subtypes of the biometric device\", \u201cdeviceProvider\u201d: \"Device provider name\", \u201cdeviceProviderId\u201d: \"Device provider Id\", \u201cdateTime\u201d: \"Datetime in ISO format with timezone. Identity request time\" } Signed with the JSON Web Signature (RFC 7515) using the \u201cFoundational Trust Module\u201d Identity key, this data is the fundamental identity of the device. Every MOSIP compliant device will need the foundational trust module. The only exception to this rule is for the devices that have the purpose (explained below during device registration) as \"Registration\". Those devices are called as L0 where there is not FTM. These devices would sign the request with device key. Signed Digital Id would look as follows. \"digitalId\": \"base64urlencoded(header).base64urlencoded(payload).base64urlencoded(signature)\" Unsigned digital Id would look as follows. \"digitalId\": \"base64urlencoded(payload)\" payload is the Digital ID json object. Accepted Values : serialNo - Same as the Physical Id make - Brand name model - Model of the device type - [\u201cFingerprint\u201d, \u201cIris\u201d, \u201cFace\u2019\u2019], //More types will be added. subType - subtype is based on the type. Finger - \u201cSlab\u201d, \u201cSingle\u201d, \u201cTouchless\u201d Iris - \u201cSingle\u201d, \u201cDouble\u201d, Face - Full face deviceProvider - Device provider name, This would be a legal entity in the country, deviceProviderId: Device provider Id issued by MOSIP dateTime: ISO format with timezone. Identity request time","title":"Foundational Trust Module (FTM):"},{"location":"MOSIP-Device-Service-Specification/#5-device-service-communication-interfaces","text":"The section explains the necessary details of the biometric device connectivity, accessibility, discoverability and protocols used to build and communicate with the device. The device should implement only the following set of APIs. All the API\u2019s are independent of the physical layer and the operating system, with the invocation being different across operating systems. While the operating system names are defined in this spec a similar technology can be used for unspecified operating systems. It is expected that the device service ensures that the device is connected locally to the host.","title":"5. Device Service - Communication Interfaces"},{"location":"MOSIP-Device-Service-Specification/#51-device-discovery","text":"","title":"5.1 Device Discovery:"},{"location":"MOSIP-Device-Service-Specification/#specifications-for-windows-and-linux","text":"Device discovery would be used to identify MOSIP compliant devices in a system by the applications. The protocol is designed as simple plug and play with all the necessary abstraction to the specifics. Discovery Request: Request: { \u201ctype\u201d: \"type of the device\" } Accepted Values type: \u201cBiometric Device\u201d, \u201cFingerprint\u201d, \u201cFace\u201d, \u201cIris\u201d Note: \u201cBiometric Device\u201d - is a special type and used in case if you are looking for any biometric device. Response: [ { \u201cdeviceId\u201d: \"internal Id\", \u201cdeviceStatus\u201d: \"device status\", \u201ccertification\u201d: \"certification level\", \u201cserviceVersion\u201d: \"device service version\", \u201cdeviceSubId\u201d: \"device sub Id\u2019s\", \u201ccallbackId\u201d: \"baseurl to reach to the device\u201c, \"digitalId\": \"unsigned digital id of the device\", \"deviceCode\": \"A unique code given by MOSIP after successful registration\", \"specVersion\": [\"Array of supported MDS specification version\"], \"purpose\": \"Auth or Registration or empty if not registered\", \"error\": { \"errorcode\": \"101\", \"errorinfo\": \"Invalid JSON Value Type For Discovery.. } }, ... ] Accepted values: deviceStatus - \u201cActive\u201d, \u201cInactive or Not Registered\u201d certification - \u201cL0\u201d, \u201cL1\u201d - Level of certification serviceVersion - Version of the MDS specification that is supported. deviceId - Internal Id to identify the actual biometric device within the device service. deviceSubId - is the internal Id of the device. For example in case of iris capture, the device can have two modules in a single device, it is possible to address each device with a sub Id so we can identify or command each of it in isolation. Sub Id is a simple index which always starts with 1 and increases sequentially for each sub device present. callbackId - this differs as per the OS. In case of Linux and windows operating systems it is a http URL. In the case of android, it is the intent name. In IOS it is the URL scheme. The call back url takes precedence over future request as a base URL. digitalId - unsigned digital id as per the Digital Id definition. deviceCode: A unique code given by MOSIP after successful registration, specVersion - Array of supported MDS specification version\", purpose - Purpose of the device in the MOSIP ecosystem. errorCode - standardized error code. errorInfo - description of the error that can be displayed to end user. Multi lingual support. Note: The response is an array that we could have a single device enumerating with multiple biometric options. Note: The service should ensure to respond only if the type parameter matches the type of device or the type parameter is a \u201cBiometric Device\u201d.","title":"Specifications for Windows and Linux"},{"location":"MOSIP-Device-Service-Specification/#windowslinux","text":"All the device API will be based on the HTTP specification. The device always binds to 127.0.0.1 with any of the available ports ranging from 4501 - 4600. The IP address used for binding has to be 127.0.0.1 and not localhost. The applications that require access to MOSIP devices could discover them by sending the http request to the supported port range. HTTP Request: MOSIPDISC http://127.0.0.1:<device_service_port>/device HOST: 127.0.0.1: <apps port> EXT: <app name> HTTP Response: HTTP/1.1 200 ok CACHE-CONTROL:no-store LOCATION:http://127.0.0.1:<device_service_port> Content-Length: length in bytes of the body Content-Type: application/json Connection: Closed Note: the pay loads are json in both the cases and are part of the body. *callbackId would be set to the http://127.0.0.1 : . So, the caller will use the respective verb and the url to call the service.","title":"Windows/Linux:"},{"location":"MOSIP-Device-Service-Specification/#android","text":"All devices on an android device should listen to the following intent io.mosip.device Upon invocation of this intent the devices are expected to respond back with the json response filtered by the respective type. *callbackId would be set to the appId. So, the caller will create the intent appId.Info or appId.Capture","title":"Android:"},{"location":"MOSIP-Device-Service-Specification/#ios","text":"All device on an IOS device would respond to the url schema as follows. MOSIPDISC:// ?ext= &type= If a MOSIP compliant device service app exist then the url would launch the service. The service in return should respond back to the caller using the call-back-app-url with the base64 encoded json as the url parameter for the key data. Note: In IOS there are restrictions to have multiple apps registering to the same URL schema. *callbackId would be set to the device service appname. So, the caller has to call appnameInfo or appnameCapture as the url scheme.","title":"IOS:"},{"location":"MOSIP-Device-Service-Specification/#52-device-info","text":"The device information API would be used to identify the MOSIP compliant devices and their status by the applications. Device Info Request: Request: NONE Accepted Values Response: [ { deviceInfo: { \u201cstatus\u201d: \"current status\", \u201cdeviceId\u201d: \"internal Id\", \u201cdeviceStatus\u201d: \"device status\", \u201cfirmware\u201d: \"firmware version\", \u201ccertification\u201d: \"certification level\", \u201cserviceVersion\u201d: \"device service version\", \u201cdeviceSubId\u201d: \"device sub Id\u2019s\", \u201ccallbackId\u201d: \"baseurl to reach to the device\u201c, \"digitalId\": \"signed digital id as described in the digital id section of this document\", \"deviceCode\": \"A unique code given by MOSIP after successful registration\", \"purpose\": \"Auth or Registration\", \"specVersion\": [\"Array of supported MDS specification version\"], }, \"error\": { \"errorcode\": \"101\", \"errorinfo\": \"Invalid JSON Value \" } } ... ] The final JSON is Signed with the JSON Web Signature using the \u201cFoundational Trust Module\u201d Identity key, this data is the fundamental identity of the device. Every MOSIP compliant device will need the foundational trust module. So the API would respond in the following format. [ { \"deviceInfo\": \"base64urlencode(header).base64urlencode(payload).base64urlencode(signature)\" \"error\": { \"errorcode\": \"100\", \"errorinfo\": \"Device not registered. In this case the device info will be only base64urlencode(payload)\" } } ] Allowed values: deviceInfo.status - \u201cReady\u201d, \u201cBusy\u201d, \u201cNot Ready\u201d, \"Not Registered\" deviceInfo.deviceId - Internal Id to identify the actual biometric device within the device service. deviceInfo.deviceStatus - \"Active\" or \"InActive\" deviceInfo.firmware - Exact version of the firmware deviceInfo.certification - \u201cL0\u201d, \u201cL1\u201d - Level of certification deviceInfo.serviceVersion - Version of the current document. deviceInfo.biometric device within the device service. deviceInfo.deviceSubId - is the internal id of the device. In case of iris when we have two iris capture modules in a single device, it is possible to address each device with a sub Id so we can identify or command each of it in isolation. This in an index that always starts with 1 and increments sequentially. deviceInfo.callbackId - base URL to communicate deviceInfo.digitalId - as defined under the digital id section for unsigned digital id. deviceInfo.purpose - \"Auth\" or \"Registration\" or empty in case the status is \"Not Registered\" deviceInfo.specVersion: \"Array of MDS specification version\", error - relevant errors as defined under the \"Error section\" of this document Note : The response is an array that we could have a single device enumerating with multiple biometric options. Note : The service should ensure to respond only if the type parameter matches the type of device or the type parameter is a \u201cBiometric Device\u201d.","title":"5.2 Device Info:"},{"location":"MOSIP-Device-Service-Specification/#windowslinux_1","text":"The applications that require more details of the MOSIP devices could get them by sending the http request to the supported port range. HTTP Request: MOSIPDINFO http://127.0.0.1:4501/info HOST: 127.0.0.1: <apps port> EXT: <app name> HTTP Response: HTTP/1.1 200 ok CACHE-CONTROL:no-store LOCATION:http://127.0.0.1:<device_service_port> Content-Length: length in bytes of the body Content-Type: application/json Connection: Closed Note: the pay loads are json in both the cases and are part of the body.","title":"Windows/Linux:"},{"location":"MOSIP-Device-Service-Specification/#android_1","text":"On an android device should listen to the following intent appId.Info Upon invocation of this intent the devices are expected to respond back with the json response filtered by the respective type.","title":"Android:"},{"location":"MOSIP-Device-Service-Specification/#ios_1","text":"On an IOS device would respond to the url schema as follows. APPIDINFO:// ?ext= &type= If a MOSIP compliant device service app exist then the url would launch the service. The service in return should respond back to the called using the call-back-app-url with the base64 encoded json as the url parameter for the key data. Note: In IOS there are restrictions to have multiple app registering to the same URL schema.","title":"IOS:"},{"location":"MOSIP-Device-Service-Specification/#53-capture","text":"The capture request would be used to capture a biometric from MOSIP compliant devices by the applications. The capture call will respond with success to only one call at a time. So in case of a parallel call the device info details are sent with status as \u201cBusy\u201d Capture Request: Request: { \u201cenv\u201d: \"target environment\", \"purpose\": \"Auth or Registration\", \"specVersion\": \"expected version of the biometric element\", \"timeout\" : <timeout for capture>, \u201ccaptureTime\u201d: <time of capture request in ISO format including timezone>, \"domainUri\": <uri of the auth server>, \u201ctransactionId\u201d: <transaction Id for the current capture>, \u201cbio\u201d: [ { \u201ctype\u201d: <type of the biometric data>, \u201ccount\u201d: <fingerprint/Iris count, in case of face max is set to 1>, \u201crequestedScore\u201d: <expected quality score that should match to complete a successful capture>, \u201cdeviceId\u201d: <internal Id>, \u201cdeviceSubId\u201d: <specific device Id>, \u201cpreviousHash\u201d: <hash of the previous block> } ], customOpts: { //max of 50 key value pair. This is so that vendor specific parameters can be sent if necessary. The values cannot be hardcoded and have to be configured by the apps server and should be modifiable upon need by the applications. Vendors are free to include additional parameters and fine-tuning parameters. None of these values should go undocumented by the vendor. No sensitive data should be available in the customOpts. } } Allowed Values: env - Allowed values are Staging| Developer| Pre-Production | Production purpose - Allowed values are Auth| Registration version - version of the biometric block as specified in the authentication or customer registration specification. timeout - Max time the app will wait for the capture. Its expected that the api will respond back before timeout with the best frame. All timeouts are in milliseconds captureTime - time of capture in ISO format with timezone. domainUri - unique uri per auth providers. This can be used to federate across multiple providers or countries or unions. transactionId - unqiue Id of the transaction. This is an internal Id to the application thats providing the service. Use different id for every successfull auth. So even if the trnsaction fails after auth we expect this number to be unique. bio.type - \u201cFIR\u201d , \u201cIIR\u201d, \u201cFace\u201d bio.count - number of biometric data that is collected for a given type. The device should validate and ensure that this number is in line with the type of biometric that's captured. bio.requestedScore - what is the expected quality? Upon reaching the necessary quality the biometric device is expected to auto capture the image. bio.deviceId - a unique Id per device service. In case a single device handles both face and iris the device Id will identify iris and camera uniquely. In case the Id is sent as 0 then the device is expected to capture biometric from both the devices. bio.deviceSubId - a specific device sub Id. Should be set to 0 if we don't know any specific device sub Id. bio.previousHash - For the first capture the previousHash is hash of empty utf8 string. From the second capture the previous captures hash (as hex encoded) is used as input. This is used to chain all the captures across modalities so all captures have happened for the same transaction and during the same time period. customOpts - If in case the device vendor has additional parameters that they can take and act accordingly then those values can be sent by the application developers to the device service. Response: [ { \"specVersion\" : \"MDS spec version\", \"data\": { \"digitalId\" : \"unsigned digital Id as described in this document\", \"deviceCode\": \"A unique code given by MOSIP after successfull registration\", \"deviceServiceVersion\": \"Service version\", \"bioType\": \"FIR\", \"bioSubType\": \"UNKNOWN\", \"purpose\": \"Auth or Registration\", \"env\": \"target environment\", \"domainUri\": \"uri of the auth server\", \"bioValue\": \"encrypted with session key and base64urlencoded biometric data\", \"transactionId\": \"unique transaction id\", \"timestamp\": \"ISO format datetime with time zone\", \"requestedScore\": \"floating point number to represent the minimum required score for the capture\", \"qualityScore\": \"floating point number representing the score for the current capture\" }, \"hash\": \"sha256(sha256 hash in hex format of the previous data block + sha256 hash in hex format of the current data block before encryption)\", \"sessionKey\": \"encrypted with MOSIP public key (dynamically selected based on the uri) and encoded session key biometric\", \"error\": { \"errorcode\": \"101\", \"errorinfo\": \"Invalid JSON Value\" } }, { \"specVersion\" : \"MDS spec version\", \"data\": { \"digitalId\": \"unsigned digital Id as described in this document\", \"deviceCode\": \"A unique code given by MOSIP after successfull registration\", \"deviceServiceVersion\": \"Service version\", \"bioType\": \"FIR\", \"bioSubType\": \"LEFT\", \"purpose\": \"Auth or Registration\", \"env\": \"target environment\", \"domainUri\": \"uri of the auth server\", \"bioValue\": \"encrypted with session key and base64urlencoded biometric data\", \"transactionId\": \"unique transaction id\", \"timestamp\": \"ISO Format date time with timezone\" }, \"hash\": \"sha256(sha256 hash in hex format of the previous data block + sha256 hash in hex format of the current data block before encryption)\", \"sessionKey\": \"encrypted with MOSIP public key and encoded session key biometric\", \"error\": { \"errorcode\": \"101\", \"errorinfo\": \"Invalid JSON Value\" } } ] Accepted values: data.bioValue - Encrypted and Encoded to base64urlencode biometric value. AES GCM encryption with a random key. The IV for the encryption is set to last 16 digits of the timestamp. ISO formated bioValue. Look at the Authentication document to understand more about the encryption. data - The entire data object is stored as follows. \"data\" : \"base64urlencode(header).base64urlencode(payload).base64urlencode(signature) payload is defined as the entire byte array of data block. The data block hash - the value of the previousHash atribute in the request object or the value of hash atribute of the previous data block (used to chain every single data block) concatenated with the hex encode sha256 hash of the current data block before encryption. sessionKey - Random AES key used for the encryption of the bioValue. The encryption key is encrypted using the public key with RSA OAEP. Sent as base64urlencoded","title":"5.3 Capture:"},{"location":"MOSIP-Device-Service-Specification/#windowslinux_2","text":"The applications that requires to capture biometric data from a MOSIP devices could do so by sending the http request to the supported port range. HTTP Request: CAPTURE http://127.0.0.1 : /capture HOST: 127.0.0.1: EXT: HTTP Response: HTTP/1.1 200 ok CACHE-CONTROL:no-store LOCATION:[http://127.0.0.1](http://127.0.0.1):<device service port> Content-Length: length in bytes of the body Content-Type: application/json Connection: Closed Note: the pay loads are json in both the cases and are part of the body.","title":"Windows/Linux:"},{"location":"MOSIP-Device-Service-Specification/#android_2","text":"All device on an android device should listen to the following intent appid.capture Upon this intend the devices are expected to respond back with the json response filtered by the respective type.","title":"Android:"},{"location":"MOSIP-Device-Service-Specification/#ios_2","text":"All device on an IOS device would respond to the url schema as follows. APPIDCAPTURE:// ?ext= &type= If a MOSIP compliant device service app exist then the url would launch the service. The service in return should respond back to the called using the call-back-app-url with the base64 encoded json as the url parameter for the key data.","title":"IOS:"},{"location":"MOSIP-Device-Service-Specification/#54-device-stream","text":"The device would open a stream channel to send the live video streams. This would help when there is an assisted operation to collect biometric. Please note the stream API\u2019s are available only for registration environment. Device Stream Request: Used only for the registration module compatible devices. This api is visible only for the devices that are registered for the purpose as \"Registration\". Request: { \u201cdeviceId\u201d: \"internal Id\", \u201cdeviceSubId\u201d: \"device sub Id\u2019s\", } Accepted Values deviceId - Internal Id deviceSubId - The sub id of the device thats responsoible to stream the data. Response: Live Video stream with quality of 3 frames per second or more using M-JPEG2000 https://en.wikipedia.org/wiki/Motion_JPEG Note: Preview should have the quality markings and segement marking. The preview would also be used to display any error message to the user screen. All error messages should be localizable. Accepted values:","title":"5.4 Device Stream"},{"location":"MOSIP-Device-Service-Specification/#windowslinux_3","text":"The applications that require more details of the MOSIP devices could get them by sending the http request to the supported port range. HTTP Request: STREAM http://127.0.0.1:4501/stream HOST: 127.0.0.1: EXT: HTTP Response: HTTP Chunk of frames to be displayed. Minimum frames 3 per second","title":"Windows/Linux:"},{"location":"MOSIP-Device-Service-Specification/#android_3","text":"No support for streaming","title":"Android:"},{"location":"MOSIP-Device-Service-Specification/#ios_3","text":"No support for streaming","title":"IOS:"},{"location":"MOSIP-Device-Service-Specification/#55-device-registration-capture","text":"The registration client application will send sample API. The sample API\u2019s response will provide the actual biometric data in a digitally signed non encrypted form. When the Device Registration Capture API is called the frames should not be added to the stream. The device is expected to send the images as well as its extraction values. For e.g. the segmented JPEG image is in the bioValue and the segmented and extracted will fit into the bioExtract. The requestedScore is in the scale of 1-100. In case the requestedScore is for all the count as average. So, in cases where you have four fingers the average of all will be considered for capture threshold. Device Registration Capture Request: The API is used by the devices that are compatible for the registration module. Request: { \u201cenv\u201d: \"target environment\", \"specVersion\": \"expected MDS spec version\", \"timeout\": \"timeout for registration capture\", \u201ccaptureTime\u201d: \"time of capture request in ISO format including timezone\", \u201cregistrationId\u201d: \"registration Id for the current capture\", \u201cbio\u201d: [ { \u201ctype\u201d: \"type of the biometric data\", \u201ccount\u201d: \"fingerprint/Iris count, in case of face max is set to 1\", \u201cexception\u201d: [\"finger or iris to be excluded\"], \u201crequestedScore\u201d: \"expected quality score that should match to complete a successful capture.\", \u201cdeviceId\u201d: \"internal Id\", \u201cdeviceSubId\u201d: \"specific device Id\", \u201cpreviousHash\u201d: \"hash of the previous block\" } ], customOpts: { //max of 50 key value pair. This is so that vendor specific parameters can be sent if necessary. The values cannot be hardcoded and have to be configured by the apps server and should be modifiable upon need by the applications. Vendors are free to include additional parameters and fine-tuning parameters. None of these values should go undocumented by the vendor. No sensitive data should be available in the customOpts. } } Accepted Values env - Allowed values are Staging| Developer| Pre-Production | Production version - version of the biometric block as specified in the registration specification. timeout - Max time the app will wait for the capture. captureTime - time of capture in ISO format with timezone. bio.type - \u201cFMR\u201d, \u201cFIR\u201d , \u201cIIR\u201d, \u201cFace\u201d bio.count - number of biometric data that is collected for a given type. The device should validate and ensure this number is in line with the type of biometric that's captured. bio.exception: \u201cLF_INDEX\u201d, \u201cLF_MIDDLE\u201d, \u201cLF_RING\u201d, \u201cLF_LITTLE\u201d, \u201cLF_THUMB\u201d \u201cRF_INDEX\u201d, \u201cRF_MIDDLE\u201d, \u201cRF_RING\u201d, \u201cRF_LITTLE\u201d, \u201cRF_THUMB\u201d, \u201cL_IRIS\u201d, \u201cR_IRIS\u201d. This is an array and all the exceptions are marked. In case of an empty element assume there is no exception. bio.requestedScore - what is the expected quality? Upon reaching the necessary quality the biometric device is expected to auto capture the image. bio.deviceId - a unique Id per device service. In case a single device handles both face and iris the device Id will identify iris and camera uniquely. In case the Id is sent as 0 then the device is expected to capture biometric from both the devices. bio.deviceSubId - a specific device sub Id. Should be set to 0 if we don't know any specific device sub Id. In case of Fingerprint/IRIS its 1 for left and 2 for right fingerprint/iris. 3 for thumb/two iris. bio.previousHash - The previous hash for the image captured by this device per registration. For the first capture (per registration) use the hash of empty string. Response: {\"biometrics\": [ { \"specVersion\" : \"MDS Spec version\", \"data\": { \"digitalId\": \"Unsigned digital id of the device as per the Digital Id definition..\", \"deviceCode\": \"A unique code given by MOSIP after successfull registration\", \"deviceServiceVersion\": \"\", \"bioSubType\": \"Middle Finger\", \"purpose\": \"Auth or Registration\", \"env\": \"target environment\", \"bioValue\": \"<base64urlencoded biometric data (raw image)>\", \"bioExtract\": \"<base64urlencoded extracted biometric (ISO format)\", \"registrationId\": \"1234567890\", \"timestamp\": \"2019-02-15T10:01:57.086+05:30\", \"requestedScore\": \"<floating point number to represent the minimum required score for the capture. This ranges from 0-100>\", \"qualityScore\": \"<floating point number representing the score for the current capture. This ranges from 0-100>\" }, \"hash\": \"sha256(sha256 hash in hex format of the previous data block + sha256 hash in hex format of the current data block)\", \"error\": { \"errorcode\": \"101\", \"errorinfo\": \"Invalid JSON Value Type For Discovery.. ex: {type: \u201cBiometric Device\u201d or \u201cFingerprint\u201d or \u201cFace\u201d or \u201cIris\u201d or \u201cVein\u201d} \" } }, { \"specVersion\" : \"MDS Spec version\", \"data\": { \"deviceCode\": \"\", \"digitalId\": \"Unsigned digital id of the device as per the Digital Id definition.\", \"deviceServiceVersion\": \"\", \"bioSubType\": \"LEFT\", \"purpose\": \"Auth or Registration\", \"env\": \"<target environment>\", \"bioValue\": \"<base64urlencoded biometric data (raw image)>\", \"bioExtract\": \"<base64urlencoded extracted biometric (ISO format)\", \"registrationId\": \"1234567890\", \"timestamp\": \"2019-02-15T10:01:57.086+05:30\" }, \"hash\": \"sha256(sha256 hash in hex format of the previous data block + sha256 hash in hex format of the current data block before encryption)\", \"error\": { \"errorcode\": \"101\", \"errorinfo\": \"Invalid JSON Value Type For Discovery.. ex: {type: \u201cBiometric Device\u201d or \u201cFingerprint\u201d or \u201cFace\u201d or \u201cIris\u201d or \u201cVein\u201d} \" } } ] } Accepted values: data - base64urlencode(header).base64urlencode(payload).base64urlencode(signature)","title":"5.5 Device Registration Capture:"},{"location":"MOSIP-Device-Service-Specification/#windowslinux_4","text":"The applications that require more details of the MOSIP devices could get them by sending the http request to the supported port range. HTTP Request: RCAPTURE http://127.0.0.1:4501/capture HOST: 127.0.0.1: EXT: HTTP Response: HTTP response.","title":"Windows/Linux:"},{"location":"MOSIP-Device-Service-Specification/#android_4","text":"No support for Registration Capture","title":"Android:"},{"location":"MOSIP-Device-Service-Specification/#ios_4","text":"No support for Registration Capture","title":"IOS:"},{"location":"MOSIP-Device-Service-Specification/#6-device-server","text":"The device server exposes two external device APIs to manage devices. These will be consumed from Management Server created by the device provider. Refer to the subsequent section in this document.","title":"6. Device Server"},{"location":"MOSIP-Device-Service-Specification/#61-registration","text":"The MOSIP server would provide the following device registration API which is whitelisted to the management servers of the device provider or their partners. Note: This API is exposed by the MOSIP server to the device providers. http://device.mosip.io /device/register HTTP Request Type: POST { \u201cdeviceData\u201d: { \u201cdeviceId\u201d: \"unique Id to identify a biometric capture device\", \"purpose\": \"Auth or Registration. Can not be empty\", \u201cdeviceInfo\u201d: { \u201cdeviceSubId\u201d: \"an array of sub Ids that are available\", \u201ccertification\u201d: \"certification level\", \"digitalId\": \"signed digital id of the device\", \u201cfirmware\u201d: \"firmware version\", \u201cdeviceExpiry\u201d: \"device expiry date\", \u201ctimestamp\u201d: \"ISO format datetime with timezone\" }, \u201cfoundationalTrustProviderId\u201d : \"foundation trust provider Id, in case of L0 this is empty\" } } Accepted Values: device data is sent in the following format. \"deviceData\" : base64urlencode(header).base64urlencode(payload).base64urlencode(signature) payload is the object in deviceData. Response: { \"response\": { \u201cstatus\u201d: \"registration status\", \u201cerror\u201d: { \"code\": \"error code if registration fails\", \"message\": \"description of the error code\", } \"deviceCode\": \"UUID RFC4122 Version 4 for the device issued by the mosip server\", \"timestamp\": \"timestamp in ISO format\", \"env\": \"prod/development/stage\" } } The response is of the following format \"response\" : base64urlencode(header).base64urlencode(payload).base64urlencode(signature) The response should be sent to the device. The device is expected to store the deviceCode within its storage in a safe manner. This device code is used during the capture stage. Note : The device once registered for a specific purpose can not be changed after successfull registration. The device can only be used for that specific mosip process.","title":"6.1 Registration:"},{"location":"MOSIP-Device-Service-Specification/#62-de-register","text":"http://device.mosip.io /device/deregister Request: { device: { \u201cdeviceCode\u201d: <device code>, \u201cenv\u201d: <environment> \u201ctimestamp\u201d: <iso format time of successful registration of the device> } \u201csignature\u201d: \u201csignature of the device element using the device provider certificate\u201d }","title":"6.2 De-Register:"},{"location":"MOSIP-Device-Service-Specification/#7-management-server","text":"The management server has the following objectives. 1. Validate the devices to ensure its a genuine device from the respective device provider. This can be achieved using the device info and the certificates for the Foundational Trust Module. 1. Register the genuine device with the MOSIP device server. 1. Manage/Sync time between the end device the server. The time to be synced should be the only trusted time accepted by the device. 1. Ability to issue commands to the end device for 1. Key rotation of the end device 1. De-registration of the device 1. Collect device info 1. A central repository of all the approved devices from the device provider. 1. Safe storage of keys using HSM FIPS 140-2 Level 3. These keys are used to issue the device certificate upon registration. The Management Server is created and hosted by the device provider outside of MOSIP software. The communication protocols between the MDS and the Management Server can be decided by the respective device provider. Such communication should be restricted to the above specified interactions only. No transactional information should be sent to this server.","title":"7. Management Server"},{"location":"MOSIP-Device-Service-Specification/#8-compliance","text":"L1 Certified Device / L1 Device - A device certified as capable of performing encryption on the device inside its trusted zone. L0 Certified Device / L0 Device - A device certified as one where the encryption is done on the host inside its device driver or the MOSIP device service. API Compatible Device Discovery L0/L1 Device Info L0/L1 Capture L1 Registration Capture L0/L1","title":"8. Compliance"},{"location":"MOSIP-Device-Service-Specification/#9-error-codes","text":"0 Success 100 Device not registered 101 Unable to detect a biometric object 102 Technical error during extraction. 103 Device tamper detected 104 Unable to connect to management server 105 Image orientation error 106 Device not found 107 Device public key expired 108 Domain public key missing 5xx Custom errors. The device provider is free to choose his error code and error messages.","title":"9. Error Codes"},{"location":"MOSIP-Releases/","text":"Latest Release Version: 0.9.0 (Stable) Name: Proxy Biometrics Date: 01-Jul-2019 Release Notes Source code Documentation","title":"Releases"},{"location":"MOSIP-Releases/#latest-release","text":"Version: 0.9.0 (Stable) Name: Proxy Biometrics Date: 01-Jul-2019 Release Notes Source code Documentation","title":"Latest Release"},{"location":"Roadmap/","text":"July 2019 to June 2020 Seed Contribution Repo Separation with CI/CD Community setup Interface mutation checks Commit auto tests Code promotion Release process Native Launcher Real Biometric Release Vanilla Platform Complete the stubbed config Complete modularity Data Enrichment Rigidity Security Testing Performance Testing Maintainability Database Sharding High Availability Disaster Recovery Data Center Ops Admin Application Resident Services API Partner Management API ID Auth Backlog Reg Proc Backlog Reg Client Backlog Kernel Backlog Pre-Reg (UI cleanup, Captcha) Reporting Community Contributions Functional Backlog Configurable UI Integrations eIDAS Connector CRVS Integration Analytics Fraud Management eKYC Server A/B Testing Cloud Setup Device Management Server Compliance Certification Tools Mobile App Android Reg Client IOS Reg Client Online Registration eKYC reference app","title":"Roadmap"},{"location":"Roadmap/#july-2019-to-june-2020","text":"Seed Contribution Repo Separation with CI/CD Community setup Interface mutation checks Commit auto tests Code promotion Release process Native Launcher Real Biometric Release Vanilla Platform Complete the stubbed config Complete modularity Data Enrichment Rigidity Security Testing Performance Testing Maintainability Database Sharding High Availability Disaster Recovery Data Center Ops Admin Application Resident Services API Partner Management API ID Auth Backlog Reg Proc Backlog Reg Client Backlog Kernel Backlog Pre-Reg (UI cleanup, Captcha) Reporting Community Contributions Functional Backlog Configurable UI Integrations eIDAS Connector CRVS Integration Analytics Fraud Management eKYC Server A/B Testing Cloud Setup Device Management Server Compliance Certification Tools Mobile App Android Reg Client IOS Reg Client Online Registration eKYC reference app","title":"July 2019 to June 2020"},{"location":"Technology-Stack/","text":"Technology Stack This page lists all the technologies used in building MOSIP Category Tool/Technology Version License Remarks Operating System Red Hat 7.5 Programming language Java 1.8 GNU V2 Application development framework Spring family various versions Apache License 2.0 Microservices development framework Spring boot Apache License 2.0 Thick client framework JavaFX 9 GPL V2 with linking exception Web application framework Angular 5 MIT SEDA framework vert.x 3.5.1 Apache License 2.0 Database for server PostgreSQL 10.5 PostgreSQL license Permissive open source similar to BSD or MIT Database for client DerbyDB 10.4 Apache License 2.0 File system storage CEPH 13.2.0 LGPL 2.1 Build tool Apache Maven 3.53 Apache License 2.0 Containerization Docker 18.03.x CE Apache License 2.0 Reg-client-Automation TestFx 4.0.15-alpha Apache License 2.0 Reg-client-Automation Awaitility 3.0.0 Apache License 2.0 Reg-client-Automation Junit-platform-launcher 1.3.2 EPL 2.0 Reg-client-Automation Extentreports 2.41.2 BSD 3-clause Reg-client-Automation Junit-jupiter-engine 5.0.0 EPL 1.0 Reg-client-Automation Apiguardian-api 1.0.0 Apache 2.0 Rest API test automation Rest Assured 3.0.7 Framework to run test cases TestNG 6.11 Test Management Zephyr Cloud Jira & Zephyr Cloud Jira & Zephyr","title":"Technology Stack"},{"location":"Technology-Stack/#technology-stack","text":"This page lists all the technologies used in building MOSIP Category Tool/Technology Version License Remarks Operating System Red Hat 7.5 Programming language Java 1.8 GNU V2 Application development framework Spring family various versions Apache License 2.0 Microservices development framework Spring boot Apache License 2.0 Thick client framework JavaFX 9 GPL V2 with linking exception Web application framework Angular 5 MIT SEDA framework vert.x 3.5.1 Apache License 2.0 Database for server PostgreSQL 10.5 PostgreSQL license Permissive open source similar to BSD or MIT Database for client DerbyDB 10.4 Apache License 2.0 File system storage CEPH 13.2.0 LGPL 2.1 Build tool Apache Maven 3.53 Apache License 2.0 Containerization Docker 18.03.x CE Apache License 2.0 Reg-client-Automation TestFx 4.0.15-alpha Apache License 2.0 Reg-client-Automation Awaitility 3.0.0 Apache License 2.0 Reg-client-Automation Junit-platform-launcher 1.3.2 EPL 2.0 Reg-client-Automation Extentreports 2.41.2 BSD 3-clause Reg-client-Automation Junit-jupiter-engine 5.0.0 EPL 1.0 Reg-client-Automation Apiguardian-api 1.0.0 Apache 2.0 Rest API test automation Rest Assured 3.0.7 Framework to run test cases TestNG 6.11 Test Management Zephyr Cloud Jira & Zephyr Cloud Jira & Zephyr","title":"Technology Stack"},{"location":"Tester-Documentation/","text":"1. Introduction 1.1 Overview The MOSIP architecture mainly consists of the following functional blocks/modules Pre-Registration - Web application designed in Angular JS A resident can provide his demographic details in this web application and book an appointment for his future registration at a registration center Registration Client - A Desktop thick client application developed in JavaFX. A resident is registered through the Registration Client software to generate get a unique identification number. The software captures demographic and biometrics information of the residents. It is connected to scanner devices (finger print, iris), camera and printer to capture resident biometrics information Registration Processor - A backend server application developed using SEDA framework It processes the client packets and generates UIN based on de-dup information from ABIS (Automated Biometrics Identification System) IDA (ID Authentication) - A backend authentication server developed using spring family. It authenticates the resident based on registered set of biometric and demographic information Test automation is the key to the success of comprehensive test coverage and test data. However in the context of MOSIP testing, where there are external devices and integration with third party software, test automation cannot be exhaustive and comprehensive test coverage can be achieved by testing driven by manual intervention, along with test automation. In this document we will also talk about utilities for test data generation, tools for test automation and test strategy in general. 1.2 Scope 1.2.1 Test Coverage Each of the modules has the following building blocks which are the testable entities, at the module level System Integration Testing - This involves testing functional workflows across the modules, starting from Pre-Reg and ending in IDA Test Automation - tools, approach, test code configuration management process, regular usage Module Testable Entities Levels of Testing Comments Pre-Registration UI REST APIs UI Functional Testing Individual API testing API level integration testing Registration Client Java APIs UI Functional Testing (with simulators and with devices) Individual API testing API level integration testing Registration Processor Java APIs SEDA vert.x stages Individual API testing Integration workflow testing including the APIs and Vert.x for processing various packet types IDA REST APIs Individual API testing Integration workflow testing Kernel REST APIs Individual API testing Integration workflow testing 1.2.2 Data Coverage Data utility tools - approach, usage 1.2.3 Test Management Tools 1.2.4 Defect Management & Lifecycle 2. Test Approach Each module is tested, both manually and through automation software for effective test coverage. A progressively evolving test approach is being adopted in both cases. 1. Manual Testing starts with module level functional coverage followed by --> integration across modules --> End to end workflow testing 1. Automation Testing starts with the fundamental building blocks like APIs, and grows up the stack. * Individual API verification is followed by --> API Integration testing --> integration across modules --> End to end workflow testing 4. Test Automation User Guides 4.1 Kernel Test Automation Suite - User Guide 4.1.1 About the Kernel Module A critical interface module of MOSIP, Kernel is the core package on which MOSIP services are built upon and is a platform, which provides higher-level services and functions that shall be reused by other modules of MOSIP. Kernel provides a foundation to build and run the services by providing several significant necessary technical functions. Kernel makes it easy to build the higher-level services (domain services, batch services and core services) by taking care of fundamental features so that individual services are concerned with specific business functions. Kernel provides an active framework that ensures structure and rules within which the higher-level services operate. Kernel automation works with Restful and Java API\u2019s. The test execution module of the Kernel module involving API\u2019s is as depicted below 4.1.2 Pre-requisites for understanding Java API automation Knowledge on Java 8 Basic knowledge on Rest assured tool Knowledge on Maven Knowledge on TestNg framework Knowledge on GitHub Good analytical and debugging skill 4.1.3 Procedure to check out the test code from the repository Create a workspace in the local system Open git bash in the workspace Enter the command :- git clone link MOSIP project shall be cloned Import the \u201cautomationtests\u201d project into the eclipse. 4.1.4 Pre-configuration information prior to test run None 4.1.5 Procedure to Add new test cases into the API test suite From the automationtests project, the test suites and cases can be located in the folder [ src/main/resources ] Every API tests structure (model, api name and test case) are stored in a folder/sub-folder approach. Let us take an example of \u201c Email Notification service \u201d and explain how to add a new test Every test case will have 2 json files named [ request.json and response.json ] in its sub-folder as shown below In the request.json file, we need to mention the input that needs to be send to the API and response.json file contains the expected result for that particular input. Based on the test cases, we need to add the test case folders with request and response files. The readTestCases method from TestCaseReader class will read the folder names and give the test case names and readRequestResponseJson method from TestCaseReader class will read the request and response files from the tests. 4.1.6 Procedure to execute or Run the tests on a new environment To run the automation suite of Kernel you will need an xml file named [ testngKernel.xml ], which will be available under [ src/main/resources ]. Add what are the test need to run in that xml file. Add the path of the xml file in pom.xml file under maven surefire plugin. 4.1.7 Running a test suite Right click project Select \u201cRun as configuration\u201d Under configuration select Maven build and create new maven build Select current project as workspace. Pass the below commands in the Goals:- Command:- clean install - Denv.user = required environment -Denv.endpoint =application url -Denv.testLevel =testLevel Where Required environment- In which environment the suite needs to run. (Ex:- qa, dev, int) TestLevel- Type of tests like (Ex:-smoke, regression, smokeAndRegression) Note: - Here regression means all tests other than smoke tests. 6. Select or Click the button \u201cRUN\u201d 1. Once the execution is completed, Test report will be generated in target/surefire-reports folder with the name MOSIP_ModuleLevelAutoRun_TestNGReport.html . 4.1.8 Analyze the test reports Open the report in Internet Explorer The report will give the module name, total number of test case execution with pass, skipped and fail count Report will provide the build version and also execution time Report will show API name and corresponding test case names with execution time For failed test cases, it will show the cause of failure 4.2 Pre-Registration Test Automation Suite - User Guide 4.2.1 About the Pre-Registration Module This is the web channel of MOSIP, which facilitates capturing individual information, relevant documents and booking an appointment with a registration center. This helps to reduce registration time and optimize the process. The current web application is highly modular by design and with multi language support. This UI can be customized or modified as per the country's requirements. A country can also build a new web/mobile application on top of the back end services that MOSIP provides. The test execution work-flow for the module Pre-Registration involving Rest API\u2019s is as depicted below 4.2.2 Pre-requisites for understanding Java API automation Knowledge on Java 8 Knowledge on Rest services Knowledge on maven Good analytical and debugging skill 4.2.3 Procedure to checkout-out the test code from the repository Navigate to git repository. Copy URI Open Git Bash Clone repository(git clone \u201cURI\u201d) 4.2.4 Pre-configuration information prior to test run None 4.2.5 Procedure to Add new test cases into the API test suite From the code repository of the module, the test suites and cases can be located in the folder [ src/main/resources ] Every API tests structure (test suite and test case) are stored in a folder/sub-folder approach. Let us take an example of \u201cCreate_PreRegistration\u201d and Here you can see Create_PreRegistration is the suite name and inside that we have list of test cases. To add new test case we need to create a folder inside test suite folder. You can give folder name same as test case name Every test case name we need to add Create_PreRegistrationRequest.json file In the Create_PreRegistartionRequest.json file, we need to mention all folder name(Test Case Name). When we run any class, then it will pick request body from folder and it will pick expected response. We will take request body, as input and it will give response (Actual Response). For Validation, we are doing json to json comparison. In Pre-Registration module, we have created on class called PreRegistrationLibrary, which is present in io.mosip.util package. In this class, we have created all reusable method, which is used, in Pre-Registration module. E.g.:-To book an appointment first we need to create an application, upload document, and then book appointment. Here for each operation we have created one method. 4.2.6 Procedure to execute or Run the tests on a new environment To run the automation suite of Pre-Registration module you will need an xml file named [ Pre-Registration_TestNG.xml ], which will be available under [ src/main/resources ]. In this xml file we need to add class name which we want to run. 4.2.7 Running a test suite Procedure to execute the [ Reg-automation-service_TestNG.xml ] xml File: 1. Right click the xml file Pre-Registration_TestNG.xml 1. Select \u201cRun as configuration\u201d 1. Run as Maven 1. Select workspace(${workspace_loc:/automationtests}) 1. In Goal Pass environment name,Base URI and type of test case you want to run(smoke or regression) Here, -Denv.user indicates environment name. -Denv.endpoint indicates base URI -Denv.testLevel indicates types of test case we want to run Select or Click the button \u201cRUN\u201d Test Suites execution will commence. Test report will be stored in [ surefire-report ] folder under the base directory/project 4.2.8 Running a single test case Right click on class, which you want to run. Click on run as Click on testing Select class In VM argument pass -Denv.user=qa -Denv.endpoint=\"eg:https://testenvname.mosip.io\" -Denv.testLevel=smokeAndRegression 4.2.9 Analyze the test reports Once run is complete, then refresh project and go to target/surefire folder. Open MOSIP_ModuleLevelAutoRun_TestNGReport.html report. To analyze failure test case check exception message. 4.3 Registration Client Test Automation Suite - User Guide 4.3.1 About the Registration Client Module An important client interface module of MOSIP, which captures the Biometric and Demographic information of the Individual resident. This module also stores supporting information such as proof documents and information about the guardian or introducer as per the configuration set by the Admin. The packet creation is finished in this module in a secure way using sophisticated encryption algorithm and later send to the server for online mode of processing. The registration client test suites comprises of tests related to UI and Java API\u2019s. The test execution module of the Registration client module involving Java API\u2019s is as depicted below 4.3.2 Pre-requisites for understanding Java API automation Knowledge on Java 8 Basic knowledge on Spring services and should know annotations Knowledge on maven Good analytical and debugging skill 4.3.3 Procedure to check out the test code from the repository Instruction to checkout code from GitHub using Eclipse. Open eclipse Go to quick access and search \u201cclone git\u201d Figure 1 Figure 2 A pop up will appear in that enter the URI, Host and Repository path as same as below. Pass your GitHub username and password and click on next. Search the branch name, select it, and then click next. Our latest branch name as link . Browse the directory to pull the code. Now the code will be in eclipse git repository. Import the required project to the workspace. For registration client automation, we want to import kernel and registration projects. 4.3.4 Pre-configuration information prior to test run None 4.3.5 Procedure to Add new test cases into the API test suite From the code repository of the module, the test suites and cases can be located in the folder [ src/test/resources ] Every API tests structure (test suite and test case) are stored in a folder/sub-folder approach. Let us take an example of \u201c Email Notification service \u201d and explain how to add a new test Every test case will have a configuration property file named [ condition.properties ] in its sub-folder as shown below In this condition.properties file, we need to mention the parameter type that needs to be sent to the API. [ valid ] indicates the value passed is a correct/right data and [ invalid ] indicates the data being sent is an incorrect/wrong data.(we can also check the parameter behavior for the empty and space also, for that we can pass the value as space and empty respectively in condition.properties) This information has to be entered for every field/parameter that the API consists. Based on values set inside the condition.properties file test cases will fetch the data from yaml file and then call a data generator code internally which shall add meaningful right or incorrect values as test data into these variables. More information on the Yaml file can be found under appendix 4.3.6 Procedure to execute or Run the tests on a new environment To run the automation suite of Registration Client module you will need an xml file named [ Reg-automation-service_TestNG.xml ], which will be available under [ src/test/resources ]. 4.3.7 Running a test suite Procedure to execute the [ Reg-automation-service_TestNG.xml ] xml File: 1. Right click the xml file Reg-automation-service_TestNG.xml 1. Select \u201cRun as configuration\u201d 1. Under configuration select [TestNG] and pass the VM argument as -Dspring.profiles.active=required environment (which could be either of QA or INT or DEV) -Dmosip.dbpath=DB path *DB path \u2013 this is the local DB path where all the sync happens and other data\u2019s get updated while running the code. The empty DB name is available in /registration/registration-libs/src/main/resources/db/reg . We are copying this empty DB in our project and passing as vm argument while running the code. -Dmosip.registration.db.key=DB key path Sample representation of the VM argument is as below -Dspring.profiles.active=qa -Dmosip.dbpath=reg -Dmosip.registration.db.key=D:\\keys.properties 4. Select or Click the button \u201cRUN\u201d Test Suites execution will commence. 5. Test report will be stored in [test-output] folder under the base directory/project 4.3.8 Appendix 1. Java API Java application programming interface (API) is a list of all classes that are part of the Java development kit (JDK). An application-programming interface (API), in the context of Java, is a collection of prewritten packages, classes, and interfaces with their respective methods, fields and constructors. For more detail, refer to the link 2. Yaml master data file Yaml file is the master data set for testing the API, Sample Mater Data set is as below: 3. How to increase the data coverage inside Yaml file? To increase the data coverage we can add as many as test data\u2019s into the Yaml file 4. Any dependencies of values in the Database None 4.4 Registration Processor Test Automation Suite - User Guide 4.4.1 About The Registration Processor Module Registration Processor processes the data (demographic and biometric) of an Individual for quality and uniqueness and then issues a Unique Identification Number (UIN). The source of data are primarily from MOSIP Registration Client Existing ID system(s) of a country The workflow of testing or running the test suite of the available API\u2019s And Stages is as depicted below 4.4.2 Pre-requisites for understanding Rest API automation Knowledge on Core Java Basic knowledge on Rest assured library Knowledge on Maven Knowledge on TestNg framework Knowledge on Keyword, Data Driven and Hybrid methodology Knowledge on GitHub Good analytical and debugging skill 4.4.3 Procedure to checkout-out the test code from the repository Launch eclipse with new or existing workspace Clone project from link Import the automationtests project into the eclipse. 4.4.4 Procedure to Add new test cases into the API test suite Case1 : For Api Level Testing From the automationtests project, the testdata can be located in the folder [ src/main/resources ] Every API tests structure (model, api name and test case) are stored in a folder/sub-folder approach. Let us take an example of \u201c Sync Api Service \u201d and explain how to add a new test Case2 : For Stage Level Testing 1. From the automationtests project, the testdata can be located in the folder [ src/main/resources ] 1. Every stage can be tested by feeding negative packets to the system and expecting them to fail for the particular stage. Let us take the example of \u201c OSI Validation Stage \u201d A sample property file looks like as follows : The reg proc automation suite will tweak the values in a valid packet and will generate packets for the above attributes sequentially. There is one more file \u201c StageBits.properties \u201d which has a stage string. The string is used to construct the status string for a packet. For eg if stage string is \u201c 111110000 \u201d it means that packet should go through first five stage and should fail for last 4 stages. 4.4.5 Procedure to execute or Run the tests on a new environment To run the automation suite of Reg-Proc, build the project and get the uber jar generated under target. Run the jar using the command line \u201cjava -Denv.user= -Denv.endpoint= -Denv.testLevel= -jar \u201d Example: java -Denv.user=qa -Denv.endpoint=\"eg:https://testenvname.mosip.io\" -Denv.testLevel=smokeandregression -jar automationtests-refactor-0.9.1-jar-with-dependencies.jar Note: env = qa,dev,int | testLevel=smoke,regression,smokeandregression Report will be generated under \u201c< wokspace >/testing-report. 4.4.6 Analyze the test reports Report can be opened in any Web browser (i.e. Internet Explorer) The report will consist of module name, total number of test case executed with status as either pass, skipped and fail and their count. Report will also display API name and corresponding test case names with execution time along with build version and execution time. For detailed analysis, refer logs or default testing-report and for failed test cases, the related cause of failure will be highlighted. 4.5 ID Authentication (IDA) Test Automation Suite - User Guide 4.5.1 About the ID-Authentication MOSIP ID Authentication provides an API based authentication mechanism for entities to validate Individuals. ID Authentication is the primary mode for entities to validate an Individual before providing any service. An example of how this service will work is as depicted below The workflow of testing or running the test suite of the available API\u2019s is as depicted below 4.5.2 Pre-requisites for understanding Rest API automation Knowledge on Core Java Basic knowledge on Rest assured library Knowledge on Maven Knowledge on TestNg framework Knowledge on Keyword, Data Driven and Hybrid methodology Knowledge on GitHub Good analytical and debugging skill 4.5.3 Procedure to checkout-out the test code from the repository Launch eclipse with new or existing workspace Clone project from link Import the automationtests project into the eclipse. 4.5.4 Procedure to Add new test cases into the API test suite From the automationtests project, the testdata can be located in the folder [ src/main/resources ] Every API tests structure (model, api name and test case) are stored in a folder/sub-folder approach. Let us take an example of \u201c Demo-Address Authentication service \u201d and explain how to add a new test Pre-requisites : open runConfiguration.properties file Add the following two lines which represents your test case; one for the folder location and another on the test data as below, the array [X], where \u201cX\u201d represents the number of times this tests shall be repeated with different test data An example: DemographicAuthentication.testDataPath[6]=ida/TestData/Demo/Name/ DemographicAuthentication.testDataFileName[6]=testdata.ida.Demo.Name.mapping.yml If you want to remove a test, kindly comment the relevant line in this file before the execution of TestNG runner class 4. Configuration Setup for creating the request Json file : Please use the TestData keyword defined under in appendix for creating your request.json file. The provided keywords are sufficient for testing the ID Authentication module, however If you ever need you can add an additional attribute to the end of this list Sample structure of the request.JSON file , which is being created at run time using the attributes defined in the TestData, which reads from the Yaml data file: 4.5.5 Procedure to execute or Run the tests on a new environment To run the automation suite of ID-Authentication, build the project and get the uber jar generated under target. Run the jar using the command line \u201cjava -Denv.user= -Denv.endpoint= -Denv.testLevel= -jar \u201d Example: java -Denv.user=qa -Denv.endpoint=\"eg:https://testenvname.mosip.io)\" -Denv.testLevel=smokeandregression -jar automationtests-refactor-0.9.1-jar-with-dependencies.jar Note: env = qa,dev,int | testLevel=smoke,regression,smokeandregression Report will be generated under \u201c /testing-report 4.5.6 Analyze the test reports Report can be opened in any Web browser (i.e. Internet Explorer) The report will consist of module name, total number of test case executed with status as either pass, skipped and fail and their count. Report will also display API name and corresponding test case names with execution time along with build version and execution time. For detailed analysis, refer logs or default testing-report and for failed test cases, the related cause of failure will be highlighted. 4.5.7 Annexure Yaml Test Data Format : The sample structure should be like below: TestData Keyword repository : Keywords KeywordName/Purpose Example $TIMESTAMPZ$ To generate current timestamp with UTC format 2019-06-20T16:18:08.008Z $TIMESTAMP$ To generate current timestamp with timezone format 2019-06-20T16:18:08.008+05:30 $TIMESTAMP$HOUR+24 $TIMESTAMP$HOUR-24 $TIMESTAMP$MINUTE+23 $TIMESTAMP$MINUTW-56 $TIMESTAMP$SECOND+145 $TIMESTAMP$SECOND-123 To generate future or current timestamp $RANDOM:N:10$ To generate random digit for the given number $RANDOM:N:10$ $RANDOM:N:3$ $RANDOM:N:14$ $UIN$ To get random UIN number from uin.property file $UIN$:WITH:Deactivated# To get uin number from uin.property file where value contains Deactivated $VID$ To get random VID from vid.property file where type as perpetual and status as ACTIVE $VID:WITH:Temporary$ $VID:WITH:REVOKE$ To get random VID from vid.property file where value contains Temporary or Revoke $VID:WHERE:UIN:WITH:VALID$ To get the VID from vid.property where uin.property value contains specified keyword after \u201cWITH:\u201d $TestData: indvId_Vid_valid$ $TestData: bio_finger_LeftIndex_subType$ $TestData:bio_face_deviceCode$ To get the random value form the list in the authenticationTestData.yml file. $input.bio-auth-request : AuthReq.transactionID$ To get the already assigned for the files input.filename1: mappingName1: value1 mappingName2: value2 ouput.filename2: mappingName3: $ input.filename: mappingName2$ where mappingName3 has set as value2 $errors:RevokedVID:errorCode$ $errors:InactiveVID:errorCode$ Get error code for the mentioned key \u201cRevokedVID\u201d from the errorCodeMsg.yml file. $errors:InactiveVID:errorMessage$ $errors:RevokedVID:errorMessage$ Get error message for the mentioned key \u201cRevokedVID\u201d from the errorCodeMsg.yml file. $idrepo ~ $input.bio-auth-request : AuthReq.individualId$ ~ DECODEFILE: individualBiometricsValue ~ //BIR/BDBInfo[Type = 'Finger'][Subtype = 'Left IndexFinger']//following::BDB$ $idrepo ~ $input.bio-auth-request : AuthReq.individualId$ ~ DECODEFILE: individualBiometricsValue ~ //BIR/BDBInfo [Type= 'Face']//following::BDB$ To get biometric value for the UIN using cbeff File. It is combination of above listed keyword. Where $idrepo -> keyword mandatory at start. $input.bio-auth-request : AuthReq.individualId$ -> get the value from the previously mentioned field individualBiometricsValue -> Mapping name in UINMapping/mapping.property //BIR/BDBInfo[Type = 'Finger'][Subtype = 'Left IndexFinger']//following::BDB -> cbeff xpath, where in this location biovalue will be saved for Left IndexFinger $idrepo~$input.demo-auth-request: AuthReq.individualId$ ~ valueaddressLine1: langcode: TestData: primary_lang_code$ $idrepo~$input.demo-auth-request: AuthReq.individualId$ ~ valuecity: langcode: TestData: secondary_lang_code$ To get demographic data for the UIN and language. Where $idrepo -> keyword mandatory at start. $input.demo-auth-request: AuthReq.individualId$ -> get the value from the previously mentioned field valueaddressLine1 -> Mapping name in UINMapping/mapping.property TestData: primary_lang_code -> keyword to get language code from the authenticationTestData.yml 4.6 E2E Test Automation Suite - User Guide 4.6.1 About The End To End Test Rig End to end system level Test Rig covers the functionality across the modules starting with Pre-Registration and ending in Registration Processor or IDA. The below diagram depicts the overall design of the end to end suite. 4.6.2 Pre Requisite To Understand The Flow Of E2E Test Rig Knowledge on Core Java Basic knowledge on Rest assured library Knowledge on Maven Knowledge on TestNg framework Knowledge on Keyword, Data Driven and Hybrid methodology Knowledge on GitHub Good analytical and debugging skill 4.6.3 Procedure to checkout-out the test code from the repository Launch eclipse with new or existing workspace Clone project from link Import the e2etestrig project into the eclipse. 4.6.4 Basic Code Structure Of The E2E Rig The E2E rig is a basic parent child maven project It contains 5 child project which are as follows: Pre Registration which will generate list of Pre-IDs Registration Client which will generate list of packets. Registration Processor which will generate UIN for the packets. IDA to perform authentication. E2E to consume the above projects A basic code structure looks as follows: 4.6.4.1 A Code Structure For E2E run The E2E code has the following prerequisite Under src/test/resources we should have an input.json which contains a data to generate the list of preIDs. All the module level suites should be up and running. * A sample pom for e2e looks like the following: The pre Reg Automation has the following pre Requisite : It should have an input.json file which will conation info about the adults and children against whom the preIDs are being generated. The Reg Client Automation has the following pre Requisite: The pre Reg Automation Should have run. A json file with a list of preIds must have been generated. The reg Proc Automation has the following pre Requisite: The reg Client and pre Reg automation should have run. A list of packets must have been generated. The IDA automation has the following pre Requisite : The regProc,reg Client,Pre Reg automation should have run. A list of uin should be present as a property file under src/main/resources which is generated by regProc. 4.6.5 Procedure to execute or Run the tests on a new environment To run the automation suite simply select the \u201cEndToEndRun.java\u201d class under the package \u201cio.mosip.e2e.runner\u201d. Report will be generated under \u201c< wokspace >/testing-report. 4.6.6 Analyze the test reports Report can be opened in any Web browser (i.e. Internet Explorer) The report will consist of total number of test case executed with status as either pass, skipped and fail and their count. Report will also display applicant type and corresponding test case names with execution time along with build version and execution time. For detailed analysis, refer logs or default testing-report and for failed test cases, the related cause of failure will be highlighted. 4.6.7 Limitations for the test rig The rig is designed to run for only 5 packets. The rig should run on a particular version of each module.","title":"Tester Documentation"},{"location":"Tester-Documentation/#1-introduction","text":"","title":"1. Introduction"},{"location":"Tester-Documentation/#11-overview","text":"The MOSIP architecture mainly consists of the following functional blocks/modules Pre-Registration - Web application designed in Angular JS A resident can provide his demographic details in this web application and book an appointment for his future registration at a registration center Registration Client - A Desktop thick client application developed in JavaFX. A resident is registered through the Registration Client software to generate get a unique identification number. The software captures demographic and biometrics information of the residents. It is connected to scanner devices (finger print, iris), camera and printer to capture resident biometrics information Registration Processor - A backend server application developed using SEDA framework It processes the client packets and generates UIN based on de-dup information from ABIS (Automated Biometrics Identification System) IDA (ID Authentication) - A backend authentication server developed using spring family. It authenticates the resident based on registered set of biometric and demographic information Test automation is the key to the success of comprehensive test coverage and test data. However in the context of MOSIP testing, where there are external devices and integration with third party software, test automation cannot be exhaustive and comprehensive test coverage can be achieved by testing driven by manual intervention, along with test automation. In this document we will also talk about utilities for test data generation, tools for test automation and test strategy in general.","title":"1.1 Overview"},{"location":"Tester-Documentation/#12-scope","text":"","title":"1.2 Scope"},{"location":"Tester-Documentation/#121-test-coverage","text":"Each of the modules has the following building blocks which are the testable entities, at the module level System Integration Testing - This involves testing functional workflows across the modules, starting from Pre-Reg and ending in IDA Test Automation - tools, approach, test code configuration management process, regular usage Module Testable Entities Levels of Testing Comments Pre-Registration UI REST APIs UI Functional Testing Individual API testing API level integration testing Registration Client Java APIs UI Functional Testing (with simulators and with devices) Individual API testing API level integration testing Registration Processor Java APIs SEDA vert.x stages Individual API testing Integration workflow testing including the APIs and Vert.x for processing various packet types IDA REST APIs Individual API testing Integration workflow testing Kernel REST APIs Individual API testing Integration workflow testing","title":"1.2.1 Test Coverage"},{"location":"Tester-Documentation/#122-data-coverage","text":"Data utility tools - approach, usage","title":"1.2.2 Data Coverage"},{"location":"Tester-Documentation/#123-test-management-tools","text":"","title":"1.2.3 Test Management Tools"},{"location":"Tester-Documentation/#124-defect-management-lifecycle","text":"","title":"1.2.4 Defect Management &amp; Lifecycle"},{"location":"Tester-Documentation/#2-test-approach","text":"Each module is tested, both manually and through automation software for effective test coverage. A progressively evolving test approach is being adopted in both cases. 1. Manual Testing starts with module level functional coverage followed by --> integration across modules --> End to end workflow testing 1. Automation Testing starts with the fundamental building blocks like APIs, and grows up the stack. * Individual API verification is followed by --> API Integration testing --> integration across modules --> End to end workflow testing","title":"2. Test Approach"},{"location":"Tester-Documentation/#4-test-automation-user-guides","text":"","title":"4. Test Automation User Guides"},{"location":"Tester-Documentation/#41-kernel-test-automation-suite-user-guide","text":"","title":"4.1 Kernel Test Automation Suite - User Guide"},{"location":"Tester-Documentation/#411-about-the-kernel-module","text":"A critical interface module of MOSIP, Kernel is the core package on which MOSIP services are built upon and is a platform, which provides higher-level services and functions that shall be reused by other modules of MOSIP. Kernel provides a foundation to build and run the services by providing several significant necessary technical functions. Kernel makes it easy to build the higher-level services (domain services, batch services and core services) by taking care of fundamental features so that individual services are concerned with specific business functions. Kernel provides an active framework that ensures structure and rules within which the higher-level services operate. Kernel automation works with Restful and Java API\u2019s. The test execution module of the Kernel module involving API\u2019s is as depicted below","title":"4.1.1 About the Kernel Module"},{"location":"Tester-Documentation/#412-pre-requisites-for-understanding-java-api-automation","text":"Knowledge on Java 8 Basic knowledge on Rest assured tool Knowledge on Maven Knowledge on TestNg framework Knowledge on GitHub Good analytical and debugging skill","title":"4.1.2 Pre-requisites for understanding Java API automation"},{"location":"Tester-Documentation/#413-procedure-to-check-out-the-test-code-from-the-repository","text":"Create a workspace in the local system Open git bash in the workspace Enter the command :- git clone link MOSIP project shall be cloned Import the \u201cautomationtests\u201d project into the eclipse.","title":"4.1.3 Procedure to check out the test code from the repository"},{"location":"Tester-Documentation/#414-pre-configuration-information-prior-to-test-run","text":"None","title":"4.1.4 Pre-configuration information prior to test run"},{"location":"Tester-Documentation/#415-procedure-to-add-new-test-cases-into-the-api-test-suite","text":"From the automationtests project, the test suites and cases can be located in the folder [ src/main/resources ] Every API tests structure (model, api name and test case) are stored in a folder/sub-folder approach. Let us take an example of \u201c Email Notification service \u201d and explain how to add a new test Every test case will have 2 json files named [ request.json and response.json ] in its sub-folder as shown below In the request.json file, we need to mention the input that needs to be send to the API and response.json file contains the expected result for that particular input. Based on the test cases, we need to add the test case folders with request and response files. The readTestCases method from TestCaseReader class will read the folder names and give the test case names and readRequestResponseJson method from TestCaseReader class will read the request and response files from the tests.","title":"4.1.5 Procedure to Add new test cases into the API test suite"},{"location":"Tester-Documentation/#416-procedure-to-execute-or-run-the-tests-on-a-new-environment","text":"To run the automation suite of Kernel you will need an xml file named [ testngKernel.xml ], which will be available under [ src/main/resources ]. Add what are the test need to run in that xml file. Add the path of the xml file in pom.xml file under maven surefire plugin.","title":"4.1.6 Procedure to execute or Run the tests on a new environment"},{"location":"Tester-Documentation/#417-running-a-test-suite","text":"Right click project Select \u201cRun as configuration\u201d Under configuration select Maven build and create new maven build Select current project as workspace. Pass the below commands in the Goals:- Command:- clean install - Denv.user = required environment -Denv.endpoint =application url -Denv.testLevel =testLevel Where Required environment- In which environment the suite needs to run. (Ex:- qa, dev, int) TestLevel- Type of tests like (Ex:-smoke, regression, smokeAndRegression) Note: - Here regression means all tests other than smoke tests. 6. Select or Click the button \u201cRUN\u201d 1. Once the execution is completed, Test report will be generated in target/surefire-reports folder with the name MOSIP_ModuleLevelAutoRun_TestNGReport.html .","title":"4.1.7 Running a test suite"},{"location":"Tester-Documentation/#418-analyze-the-test-reports","text":"Open the report in Internet Explorer The report will give the module name, total number of test case execution with pass, skipped and fail count Report will provide the build version and also execution time Report will show API name and corresponding test case names with execution time For failed test cases, it will show the cause of failure","title":"4.1.8 Analyze the test reports"},{"location":"Tester-Documentation/#42-pre-registration-test-automation-suite-user-guide","text":"","title":"4.2 Pre-Registration Test Automation Suite - User Guide"},{"location":"Tester-Documentation/#421-about-the-pre-registration-module","text":"This is the web channel of MOSIP, which facilitates capturing individual information, relevant documents and booking an appointment with a registration center. This helps to reduce registration time and optimize the process. The current web application is highly modular by design and with multi language support. This UI can be customized or modified as per the country's requirements. A country can also build a new web/mobile application on top of the back end services that MOSIP provides. The test execution work-flow for the module Pre-Registration involving Rest API\u2019s is as depicted below","title":"4.2.1 About the Pre-Registration Module"},{"location":"Tester-Documentation/#422-pre-requisites-for-understanding-java-api-automation","text":"Knowledge on Java 8 Knowledge on Rest services Knowledge on maven Good analytical and debugging skill","title":"4.2.2 Pre-requisites for understanding Java API automation"},{"location":"Tester-Documentation/#423-procedure-to-checkout-out-the-test-code-from-the-repository","text":"Navigate to git repository. Copy URI Open Git Bash Clone repository(git clone \u201cURI\u201d)","title":"4.2.3 Procedure to checkout-out the test code from the repository"},{"location":"Tester-Documentation/#424-pre-configuration-information-prior-to-test-run","text":"None","title":"4.2.4 Pre-configuration information prior to test run"},{"location":"Tester-Documentation/#425-procedure-to-add-new-test-cases-into-the-api-test-suite","text":"From the code repository of the module, the test suites and cases can be located in the folder [ src/main/resources ] Every API tests structure (test suite and test case) are stored in a folder/sub-folder approach. Let us take an example of \u201cCreate_PreRegistration\u201d and Here you can see Create_PreRegistration is the suite name and inside that we have list of test cases. To add new test case we need to create a folder inside test suite folder. You can give folder name same as test case name Every test case name we need to add Create_PreRegistrationRequest.json file In the Create_PreRegistartionRequest.json file, we need to mention all folder name(Test Case Name). When we run any class, then it will pick request body from folder and it will pick expected response. We will take request body, as input and it will give response (Actual Response). For Validation, we are doing json to json comparison. In Pre-Registration module, we have created on class called PreRegistrationLibrary, which is present in io.mosip.util package. In this class, we have created all reusable method, which is used, in Pre-Registration module. E.g.:-To book an appointment first we need to create an application, upload document, and then book appointment. Here for each operation we have created one method.","title":"4.2.5 Procedure to Add new test cases into the API test suite"},{"location":"Tester-Documentation/#426-procedure-to-execute-or-run-the-tests-on-a-new-environment","text":"To run the automation suite of Pre-Registration module you will need an xml file named [ Pre-Registration_TestNG.xml ], which will be available under [ src/main/resources ]. In this xml file we need to add class name which we want to run.","title":"4.2.6 Procedure to execute or Run the tests on a new environment"},{"location":"Tester-Documentation/#427-running-a-test-suite","text":"Procedure to execute the [ Reg-automation-service_TestNG.xml ] xml File: 1. Right click the xml file Pre-Registration_TestNG.xml 1. Select \u201cRun as configuration\u201d 1. Run as Maven 1. Select workspace(${workspace_loc:/automationtests}) 1. In Goal Pass environment name,Base URI and type of test case you want to run(smoke or regression) Here, -Denv.user indicates environment name. -Denv.endpoint indicates base URI -Denv.testLevel indicates types of test case we want to run Select or Click the button \u201cRUN\u201d Test Suites execution will commence. Test report will be stored in [ surefire-report ] folder under the base directory/project","title":"4.2.7 Running a test suite"},{"location":"Tester-Documentation/#428-running-a-single-test-case","text":"Right click on class, which you want to run. Click on run as Click on testing Select class In VM argument pass -Denv.user=qa -Denv.endpoint=\"eg:https://testenvname.mosip.io\" -Denv.testLevel=smokeAndRegression","title":"4.2.8 Running a single test case"},{"location":"Tester-Documentation/#429-analyze-the-test-reports","text":"Once run is complete, then refresh project and go to target/surefire folder. Open MOSIP_ModuleLevelAutoRun_TestNGReport.html report. To analyze failure test case check exception message.","title":"4.2.9 Analyze the test reports"},{"location":"Tester-Documentation/#43-registration-client-test-automation-suite-user-guide","text":"","title":"4.3 Registration Client Test Automation Suite - User Guide"},{"location":"Tester-Documentation/#431-about-the-registration-client-module","text":"An important client interface module of MOSIP, which captures the Biometric and Demographic information of the Individual resident. This module also stores supporting information such as proof documents and information about the guardian or introducer as per the configuration set by the Admin. The packet creation is finished in this module in a secure way using sophisticated encryption algorithm and later send to the server for online mode of processing. The registration client test suites comprises of tests related to UI and Java API\u2019s. The test execution module of the Registration client module involving Java API\u2019s is as depicted below","title":"4.3.1 About the Registration Client Module"},{"location":"Tester-Documentation/#432-pre-requisites-for-understanding-java-api-automation","text":"Knowledge on Java 8 Basic knowledge on Spring services and should know annotations Knowledge on maven Good analytical and debugging skill","title":"4.3.2 Pre-requisites for understanding Java API automation"},{"location":"Tester-Documentation/#433-procedure-to-check-out-the-test-code-from-the-repository","text":"Instruction to checkout code from GitHub using Eclipse. Open eclipse Go to quick access and search \u201cclone git\u201d Figure 1 Figure 2 A pop up will appear in that enter the URI, Host and Repository path as same as below. Pass your GitHub username and password and click on next. Search the branch name, select it, and then click next. Our latest branch name as link . Browse the directory to pull the code. Now the code will be in eclipse git repository. Import the required project to the workspace. For registration client automation, we want to import kernel and registration projects.","title":"4.3.3 Procedure to check out the test code from the repository"},{"location":"Tester-Documentation/#434-pre-configuration-information-prior-to-test-run","text":"None","title":"4.3.4 Pre-configuration information prior to test run"},{"location":"Tester-Documentation/#435-procedure-to-add-new-test-cases-into-the-api-test-suite","text":"From the code repository of the module, the test suites and cases can be located in the folder [ src/test/resources ] Every API tests structure (test suite and test case) are stored in a folder/sub-folder approach. Let us take an example of \u201c Email Notification service \u201d and explain how to add a new test Every test case will have a configuration property file named [ condition.properties ] in its sub-folder as shown below In this condition.properties file, we need to mention the parameter type that needs to be sent to the API. [ valid ] indicates the value passed is a correct/right data and [ invalid ] indicates the data being sent is an incorrect/wrong data.(we can also check the parameter behavior for the empty and space also, for that we can pass the value as space and empty respectively in condition.properties) This information has to be entered for every field/parameter that the API consists. Based on values set inside the condition.properties file test cases will fetch the data from yaml file and then call a data generator code internally which shall add meaningful right or incorrect values as test data into these variables. More information on the Yaml file can be found under appendix","title":"4.3.5 Procedure to Add new test cases into the API test suite"},{"location":"Tester-Documentation/#436-procedure-to-execute-or-run-the-tests-on-a-new-environment","text":"To run the automation suite of Registration Client module you will need an xml file named [ Reg-automation-service_TestNG.xml ], which will be available under [ src/test/resources ].","title":"4.3.6 Procedure to execute or Run the tests on a new environment"},{"location":"Tester-Documentation/#437-running-a-test-suite","text":"Procedure to execute the [ Reg-automation-service_TestNG.xml ] xml File: 1. Right click the xml file Reg-automation-service_TestNG.xml 1. Select \u201cRun as configuration\u201d 1. Under configuration select [TestNG] and pass the VM argument as -Dspring.profiles.active=required environment (which could be either of QA or INT or DEV) -Dmosip.dbpath=DB path *DB path \u2013 this is the local DB path where all the sync happens and other data\u2019s get updated while running the code. The empty DB name is available in /registration/registration-libs/src/main/resources/db/reg . We are copying this empty DB in our project and passing as vm argument while running the code. -Dmosip.registration.db.key=DB key path Sample representation of the VM argument is as below -Dspring.profiles.active=qa -Dmosip.dbpath=reg -Dmosip.registration.db.key=D:\\keys.properties 4. Select or Click the button \u201cRUN\u201d Test Suites execution will commence. 5. Test report will be stored in [test-output] folder under the base directory/project","title":"4.3.7 Running a test suite"},{"location":"Tester-Documentation/#438-appendix","text":"1. Java API Java application programming interface (API) is a list of all classes that are part of the Java development kit (JDK). An application-programming interface (API), in the context of Java, is a collection of prewritten packages, classes, and interfaces with their respective methods, fields and constructors. For more detail, refer to the link 2. Yaml master data file Yaml file is the master data set for testing the API, Sample Mater Data set is as below: 3. How to increase the data coverage inside Yaml file? To increase the data coverage we can add as many as test data\u2019s into the Yaml file 4. Any dependencies of values in the Database None","title":"4.3.8 Appendix"},{"location":"Tester-Documentation/#44-registration-processor-test-automation-suite-user-guide","text":"","title":"4.4 Registration Processor Test Automation Suite - User Guide"},{"location":"Tester-Documentation/#441-about-the-registration-processor-module","text":"Registration Processor processes the data (demographic and biometric) of an Individual for quality and uniqueness and then issues a Unique Identification Number (UIN). The source of data are primarily from MOSIP Registration Client Existing ID system(s) of a country The workflow of testing or running the test suite of the available API\u2019s And Stages is as depicted below","title":"4.4.1 About The Registration Processor Module"},{"location":"Tester-Documentation/#442-pre-requisites-for-understanding-rest-api-automation","text":"Knowledge on Core Java Basic knowledge on Rest assured library Knowledge on Maven Knowledge on TestNg framework Knowledge on Keyword, Data Driven and Hybrid methodology Knowledge on GitHub Good analytical and debugging skill","title":"4.4.2 Pre-requisites for understanding Rest API automation"},{"location":"Tester-Documentation/#443-procedure-to-checkout-out-the-test-code-from-the-repository","text":"Launch eclipse with new or existing workspace Clone project from link Import the automationtests project into the eclipse.","title":"4.4.3 Procedure to checkout-out the test code from the repository"},{"location":"Tester-Documentation/#444-procedure-to-add-new-test-cases-into-the-api-test-suite","text":"Case1 : For Api Level Testing From the automationtests project, the testdata can be located in the folder [ src/main/resources ] Every API tests structure (model, api name and test case) are stored in a folder/sub-folder approach. Let us take an example of \u201c Sync Api Service \u201d and explain how to add a new test Case2 : For Stage Level Testing 1. From the automationtests project, the testdata can be located in the folder [ src/main/resources ] 1. Every stage can be tested by feeding negative packets to the system and expecting them to fail for the particular stage. Let us take the example of \u201c OSI Validation Stage \u201d A sample property file looks like as follows : The reg proc automation suite will tweak the values in a valid packet and will generate packets for the above attributes sequentially. There is one more file \u201c StageBits.properties \u201d which has a stage string. The string is used to construct the status string for a packet. For eg if stage string is \u201c 111110000 \u201d it means that packet should go through first five stage and should fail for last 4 stages.","title":"4.4.4 Procedure to Add new test cases into the API test suite"},{"location":"Tester-Documentation/#445-procedure-to-execute-or-run-the-tests-on-a-new-environment","text":"To run the automation suite of Reg-Proc, build the project and get the uber jar generated under target. Run the jar using the command line \u201cjava -Denv.user= -Denv.endpoint= -Denv.testLevel= -jar \u201d Example: java -Denv.user=qa -Denv.endpoint=\"eg:https://testenvname.mosip.io\" -Denv.testLevel=smokeandregression -jar automationtests-refactor-0.9.1-jar-with-dependencies.jar Note: env = qa,dev,int | testLevel=smoke,regression,smokeandregression Report will be generated under \u201c< wokspace >/testing-report.","title":"4.4.5 Procedure to execute or Run the tests on a new environment"},{"location":"Tester-Documentation/#446-analyze-the-test-reports","text":"Report can be opened in any Web browser (i.e. Internet Explorer) The report will consist of module name, total number of test case executed with status as either pass, skipped and fail and their count. Report will also display API name and corresponding test case names with execution time along with build version and execution time. For detailed analysis, refer logs or default testing-report and for failed test cases, the related cause of failure will be highlighted.","title":"4.4.6 Analyze the test reports"},{"location":"Tester-Documentation/#45-id-authentication-ida-test-automation-suite-user-guide","text":"","title":"4.5 ID Authentication (IDA) Test Automation Suite - User Guide"},{"location":"Tester-Documentation/#451-about-the-id-authentication","text":"MOSIP ID Authentication provides an API based authentication mechanism for entities to validate Individuals. ID Authentication is the primary mode for entities to validate an Individual before providing any service. An example of how this service will work is as depicted below The workflow of testing or running the test suite of the available API\u2019s is as depicted below","title":"4.5.1 About the ID-Authentication"},{"location":"Tester-Documentation/#452-pre-requisites-for-understanding-rest-api-automation","text":"Knowledge on Core Java Basic knowledge on Rest assured library Knowledge on Maven Knowledge on TestNg framework Knowledge on Keyword, Data Driven and Hybrid methodology Knowledge on GitHub Good analytical and debugging skill","title":"4.5.2 Pre-requisites for understanding Rest API automation"},{"location":"Tester-Documentation/#453-procedure-to-checkout-out-the-test-code-from-the-repository","text":"Launch eclipse with new or existing workspace Clone project from link Import the automationtests project into the eclipse.","title":"4.5.3 Procedure to checkout-out the test code from the repository"},{"location":"Tester-Documentation/#454-procedure-to-add-new-test-cases-into-the-api-test-suite","text":"From the automationtests project, the testdata can be located in the folder [ src/main/resources ] Every API tests structure (model, api name and test case) are stored in a folder/sub-folder approach. Let us take an example of \u201c Demo-Address Authentication service \u201d and explain how to add a new test Pre-requisites : open runConfiguration.properties file Add the following two lines which represents your test case; one for the folder location and another on the test data as below, the array [X], where \u201cX\u201d represents the number of times this tests shall be repeated with different test data An example: DemographicAuthentication.testDataPath[6]=ida/TestData/Demo/Name/ DemographicAuthentication.testDataFileName[6]=testdata.ida.Demo.Name.mapping.yml If you want to remove a test, kindly comment the relevant line in this file before the execution of TestNG runner class 4. Configuration Setup for creating the request Json file : Please use the TestData keyword defined under in appendix for creating your request.json file. The provided keywords are sufficient for testing the ID Authentication module, however If you ever need you can add an additional attribute to the end of this list Sample structure of the request.JSON file , which is being created at run time using the attributes defined in the TestData, which reads from the Yaml data file:","title":"4.5.4 Procedure to Add new test cases into the API test suite"},{"location":"Tester-Documentation/#455-procedure-to-execute-or-run-the-tests-on-a-new-environment","text":"To run the automation suite of ID-Authentication, build the project and get the uber jar generated under target. Run the jar using the command line \u201cjava -Denv.user= -Denv.endpoint= -Denv.testLevel= -jar \u201d Example: java -Denv.user=qa -Denv.endpoint=\"eg:https://testenvname.mosip.io)\" -Denv.testLevel=smokeandregression -jar automationtests-refactor-0.9.1-jar-with-dependencies.jar Note: env = qa,dev,int | testLevel=smoke,regression,smokeandregression Report will be generated under \u201c /testing-report","title":"4.5.5 Procedure to execute or Run the tests on a new environment"},{"location":"Tester-Documentation/#456-analyze-the-test-reports","text":"Report can be opened in any Web browser (i.e. Internet Explorer) The report will consist of module name, total number of test case executed with status as either pass, skipped and fail and their count. Report will also display API name and corresponding test case names with execution time along with build version and execution time. For detailed analysis, refer logs or default testing-report and for failed test cases, the related cause of failure will be highlighted.","title":"4.5.6 Analyze the test reports"},{"location":"Tester-Documentation/#457-annexure","text":"Yaml Test Data Format : The sample structure should be like below: TestData Keyword repository : Keywords KeywordName/Purpose Example $TIMESTAMPZ$ To generate current timestamp with UTC format 2019-06-20T16:18:08.008Z $TIMESTAMP$ To generate current timestamp with timezone format 2019-06-20T16:18:08.008+05:30 $TIMESTAMP$HOUR+24 $TIMESTAMP$HOUR-24 $TIMESTAMP$MINUTE+23 $TIMESTAMP$MINUTW-56 $TIMESTAMP$SECOND+145 $TIMESTAMP$SECOND-123 To generate future or current timestamp $RANDOM:N:10$ To generate random digit for the given number $RANDOM:N:10$ $RANDOM:N:3$ $RANDOM:N:14$ $UIN$ To get random UIN number from uin.property file $UIN$:WITH:Deactivated# To get uin number from uin.property file where value contains Deactivated $VID$ To get random VID from vid.property file where type as perpetual and status as ACTIVE $VID:WITH:Temporary$ $VID:WITH:REVOKE$ To get random VID from vid.property file where value contains Temporary or Revoke $VID:WHERE:UIN:WITH:VALID$ To get the VID from vid.property where uin.property value contains specified keyword after \u201cWITH:\u201d $TestData: indvId_Vid_valid$ $TestData: bio_finger_LeftIndex_subType$ $TestData:bio_face_deviceCode$ To get the random value form the list in the authenticationTestData.yml file. $input.bio-auth-request : AuthReq.transactionID$ To get the already assigned for the files input.filename1: mappingName1: value1 mappingName2: value2 ouput.filename2: mappingName3: $ input.filename: mappingName2$ where mappingName3 has set as value2 $errors:RevokedVID:errorCode$ $errors:InactiveVID:errorCode$ Get error code for the mentioned key \u201cRevokedVID\u201d from the errorCodeMsg.yml file. $errors:InactiveVID:errorMessage$ $errors:RevokedVID:errorMessage$ Get error message for the mentioned key \u201cRevokedVID\u201d from the errorCodeMsg.yml file. $idrepo ~ $input.bio-auth-request : AuthReq.individualId$ ~ DECODEFILE: individualBiometricsValue ~ //BIR/BDBInfo[Type = 'Finger'][Subtype = 'Left IndexFinger']//following::BDB$ $idrepo ~ $input.bio-auth-request : AuthReq.individualId$ ~ DECODEFILE: individualBiometricsValue ~ //BIR/BDBInfo [Type= 'Face']//following::BDB$ To get biometric value for the UIN using cbeff File. It is combination of above listed keyword. Where $idrepo -> keyword mandatory at start. $input.bio-auth-request : AuthReq.individualId$ -> get the value from the previously mentioned field individualBiometricsValue -> Mapping name in UINMapping/mapping.property //BIR/BDBInfo[Type = 'Finger'][Subtype = 'Left IndexFinger']//following::BDB -> cbeff xpath, where in this location biovalue will be saved for Left IndexFinger $idrepo~$input.demo-auth-request: AuthReq.individualId$ ~ valueaddressLine1: langcode: TestData: primary_lang_code$ $idrepo~$input.demo-auth-request: AuthReq.individualId$ ~ valuecity: langcode: TestData: secondary_lang_code$ To get demographic data for the UIN and language. Where $idrepo -> keyword mandatory at start. $input.demo-auth-request: AuthReq.individualId$ -> get the value from the previously mentioned field valueaddressLine1 -> Mapping name in UINMapping/mapping.property TestData: primary_lang_code -> keyword to get language code from the authenticationTestData.yml","title":"4.5.7 Annexure"},{"location":"Tester-Documentation/#46-e2e-test-automation-suite-user-guide","text":"","title":"4.6 E2E Test Automation Suite - User Guide"},{"location":"Tester-Documentation/#461-about-the-end-to-end-test-rig","text":"End to end system level Test Rig covers the functionality across the modules starting with Pre-Registration and ending in Registration Processor or IDA. The below diagram depicts the overall design of the end to end suite.","title":"4.6.1 About The End To End Test Rig"},{"location":"Tester-Documentation/#462-pre-requisite-to-understand-the-flow-of-e2e-test-rig","text":"Knowledge on Core Java Basic knowledge on Rest assured library Knowledge on Maven Knowledge on TestNg framework Knowledge on Keyword, Data Driven and Hybrid methodology Knowledge on GitHub Good analytical and debugging skill","title":"4.6.2 Pre Requisite To Understand The Flow Of E2E Test Rig"},{"location":"Tester-Documentation/#463-procedure-to-checkout-out-the-test-code-from-the-repository","text":"Launch eclipse with new or existing workspace Clone project from link Import the e2etestrig project into the eclipse.","title":"4.6.3 Procedure to checkout-out the test code from the repository"},{"location":"Tester-Documentation/#464-basic-code-structure-of-the-e2e-rig","text":"The E2E rig is a basic parent child maven project It contains 5 child project which are as follows: Pre Registration which will generate list of Pre-IDs Registration Client which will generate list of packets. Registration Processor which will generate UIN for the packets. IDA to perform authentication. E2E to consume the above projects A basic code structure looks as follows:","title":"4.6.4 Basic Code Structure Of The E2E Rig"},{"location":"Tester-Documentation/#4641-a-code-structure-for-e2e-run","text":"The E2E code has the following prerequisite Under src/test/resources we should have an input.json which contains a data to generate the list of preIDs. All the module level suites should be up and running. * A sample pom for e2e looks like the following: The pre Reg Automation has the following pre Requisite : It should have an input.json file which will conation info about the adults and children against whom the preIDs are being generated. The Reg Client Automation has the following pre Requisite: The pre Reg Automation Should have run. A json file with a list of preIds must have been generated. The reg Proc Automation has the following pre Requisite: The reg Client and pre Reg automation should have run. A list of packets must have been generated. The IDA automation has the following pre Requisite : The regProc,reg Client,Pre Reg automation should have run. A list of uin should be present as a property file under src/main/resources which is generated by regProc.","title":"4.6.4.1 A Code Structure For E2E run"},{"location":"Tester-Documentation/#465-procedure-to-execute-or-run-the-tests-on-a-new-environment","text":"To run the automation suite simply select the \u201cEndToEndRun.java\u201d class under the package \u201cio.mosip.e2e.runner\u201d. Report will be generated under \u201c< wokspace >/testing-report.","title":"4.6.5 Procedure to execute or Run the tests on a new environment"},{"location":"Tester-Documentation/#466-analyze-the-test-reports","text":"Report can be opened in any Web browser (i.e. Internet Explorer) The report will consist of total number of test case executed with status as either pass, skipped and fail and their count. Report will also display applicant type and corresponding test case names with execution time along with build version and execution time. For detailed analysis, refer logs or default testing-report and for failed test cases, the related cause of failure will be highlighted.","title":"4.6.6 Analyze the test reports"},{"location":"Tester-Documentation/#467-limitations-for-the-test-rig","text":"The rig is designed to run for only 5 packets. The rig should run on a particular version of each module.","title":"4.6.7 Limitations for the test rig"}]}